{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdjG4-XsE0I"
      },
      "source": [
        "# **SaprotHub: Making Protein Modeling Accessible to All Biologists**\n",
        "\n",
        "<a href=\"https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3\"><img src=\"https://img.shields.io/badge/Paper-bioRxiv-green\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/SaProtHub\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-red?label=SaprotHub\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://github.com/westlake-repl/SaprotHub\"><img src=\"https://img.shields.io/badge/Github-black?logo=github\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://theopmc.github.io/\"><img src=\"https://img.shields.io/badge/Website-OPMC-yellow\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://cbirt.net/no-coding-required-saprothub-brings-protein-modeling-to-every-biologist/\" alt=\"blog\"><img src=\"https://img.shields.io/badge/Blog-Medium-purple\" /></a>\n",
        "<a href=\"https://x.com/sokrypton/status/1795525127653986415\"><img src=\"https://img.shields.io/badge/Twitter-blue?logo=twitter\" style=\"max-width: 100%;\"></a>\n",
        "\n",
        "\n",
        "This is **ColabSaprot**, the Colab version of [SaProt](https://github.com/westlake-repl/SaProt), a pre-trained protein language model designed for various downstream protein tasks.\n",
        "\n",
        "**ColabSaprot** is a platform where **Protein Language Models(PLMs)** are more accessible and user-friendly for biologists, enabling effortless model training and sharing within the scientific community.\n",
        "\n",
        "We've established the **SaprotHub**([website](https://huggingface.co/SaProtHub), [paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3)) for storing and sharing models and datasets, where you can explore extensive collections for specific protein prediction tasks.\n",
        "\n",
        "We hope ColabSaprot and SaprotHub can contribute to advancing biological research, fostering collaboration, and accelerating discoveries in the field. You can access [our paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3) for further details.\n",
        "\n",
        "Check these videos ([training](https://www.youtube.com/watch?v=r42z1hvYKfw), [predicting](https://www.youtube.com/watch?v=N5VMBwM_ukQ)) to see how to use  ColabSaprot.\n",
        "\n",
        "Joining [**OPMC**](https://theopmc.github.io/) as an author of SaprotHub.\n",
        "\n",
        "ColabSaprot supports hundreds of [protein prediction tasks](https://github.com/westlake-repl/SaProtHub/blob/main/task_list.md).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLb_im9sJWw"
      },
      "source": [
        "## ColabSaprot\n",
        "\n",
        "| Function                             | Tutorial                                                     | Video                                                        |\n",
        "| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| <a href=\"#train\">Train your model</a>                     | [How to train your model](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model) | - [YouTube](https://www.youtube.com/watch?v=r42z1hvYKfw)<br />- [Bilibili](https://www.bilibili.com/video/BV1HDhHeTEmH/?spm_id_from=333.337.search-card.all.click&vd_source=a418185fadee73ac65d8fab69eee0b52) |\n",
        "| <a href=\"#prediction\">Classification/Regression Prediction</a> | [How to use model for classification/regression prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.1:-Classification-Regression-Prediction) | - [YouTube](https://www.youtube.com/watch?v=N5VMBwM_ukQ)      |\n",
        "| <a href=\"#mutational_effect_prediction\">Mutational Effect Prediction</a>         | [How to use model for mutational effect prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.2:-Mutational-Effect-Prediction) | -                                                            |\n",
        "| <a href=\"#inverse_folding_prediction\">Inverse Folding Prediction</a>           | [How to use model for inverse folding prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.3:-Inverse-Folding-Prediction) | -                                                            |\n",
        "| <a href=\"#upload_model\">Contribute to SaprotHub</a>              | [How to contribute to SaprotHub](https://github.com/westlake-repl/SaprotHub/wiki/0.4:-Contribute-to-SaprotHub) | -                                                            |\n",
        "\n",
        "<br>\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQ6vaQTjYO3"
      },
      "source": [
        "## SaprotHub\n",
        "\n",
        "Find awesome models and datasets for specific protein task on SaprotHub!\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SaProtHub.png?raw=true\" height=\"350\" width=\"600px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dw1U1uBI7d"
      },
      "source": [
        "# **1: Installation**\n",
        "\n",
        "## ‚ö†Ô∏èSWITCH YOUR RUNTIME TYPE TO GPU\n",
        "Before installing SaProt, please **<font color=red>SWITCH YOUR RUNTIME TYPE TO GPU!!!</font>**\n",
        "\n",
        "> üìçPlease check this [page](https://github.com/westlake-repl/SaprotHub/wiki/1.1:-Switch-your-runtime-type-to-GPU) to learn **how to switch your runtime type to GPU**.\n",
        "\n",
        "## ‚ö†Ô∏èMaximum Runtime and Idle Timeout\n",
        "\n",
        "To ensure your program finishes properly, please avoid letting your computer go to **<font color=red>sleep</font>** or remain **<font color=red>idle</font>** for long periods.\n",
        "\n",
        "Please be aware of **<font color=red>the maximum runtime</font>**, as your program may be automatically terminated when this limit is reached.\n",
        "\n",
        "| Plan            | Maximum Runtime | Idle Timeout | Additional Features                        |\n",
        "|-----------------|------------------|--------------|--------------------------------------------|\n",
        "| **Free**        | 12 hours         | Yes          | -                                          |\n",
        "| **Colab Pro**   | Based on availability and usage patterns | Yes          | Increased compute availability             |\n",
        "| **Pay As You Go**| Based on availability and usage patterns | Yes          | Increased compute availability             |\n",
        "| **Colab Pro+**  | Up to 24 hours   | No           | Background execution, continuous code execution |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tgvb8ibwBI7d"
      },
      "outputs": [],
      "source": [
        "#@title **1.1: ‚ñ∂Ô∏è Click the run button to install SaProt**\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "# Check whether the server is local or from google cloud\n",
        "root_dir = os.getcwd()\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "try:\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "  import saprot\n",
        "  print(\"SaProt is installed successfully!\")\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "\n",
        "except ImportError:\n",
        "  print(\"Installing SaProt...\")\n",
        "  os.system(f\"rm -rf {root_dir}/SaprotHub\")\n",
        "  # !rm -rf /content/SaprotHub/\n",
        "\n",
        "  !git clone https://github.com/westlake-repl/SaprotHub.git\n",
        "\n",
        "  # !pip install /content/SaprotHub/saprot-0.4.7-py3-none-any.whl\n",
        "  os.system(f\"pip install -r {root_dir}/SaprotHub/requirements.txt\")\n",
        "  # !pip install -r /content/SaprotHub/requirements.txt\n",
        "\n",
        "  os.system(f\"pip install {root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/LMDB\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/bin\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/output\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/datasets\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/token_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/structures\")\n",
        "  # !mkdir -p /content/SaprotHub/LMDB\n",
        "  # !mkdir -p /content/SaprotHub/bin\n",
        "  # !mkdir -p /content/SaprotHub/output\n",
        "  # !mkdir -p /content/SaprotHub/datasets\n",
        "  # !mkdir -p /content/SaprotHub/adapters/classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/token_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/structures\n",
        "\n",
        "  # !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  # os.system(\n",
        "  #   f\"wget 'https://drive.usercontent.google.com/download?id=1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re&export=download&authuser=0' -O {root_dir}/SaprotHub/bin/foldseek\"\n",
        "  # )\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "  # !chmod +x /content/SaprotHub/bin/foldseek\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "\n",
        "  # !mv /content/SaprotHub/ColabSaprotSetup/foldseek /content/SaprotHub/bin/\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import lmdb\n",
        "import base64\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import yaml\n",
        "import argparse\n",
        "import pprint\n",
        "import subprocess\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from loguru import logger\n",
        "from easydict import EasyDict\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import snapshot_download\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, EsmForProteinFolding, EsmTokenizer\n",
        "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from saprot.data.parse import get_chain_ids\n",
        "from saprot.scripts.training import my_load_model\n",
        "from safetensors import safe_open\n",
        "\n",
        "DATASET_HOME = Path(f'{root_dir}/SaprotHub/datasets')\n",
        "ADAPTER_HOME = Path(f'{root_dir}/SaprotHub/adapters')\n",
        "STRUCTURE_HOME = Path(f\"{root_dir}/SaprotHub/structures\")\n",
        "LMDB_HOME = Path(f'{root_dir}/SaprotHub/LMDB')\n",
        "OUTPUT_HOME = Path(f'{root_dir}/SaprotHub/output')\n",
        "UPLOAD_FILE_HOME = Path(f'{root_dir}/SaprotHub/upload_files')\n",
        "FOLDSEEK_PATH = Path(f\"{root_dir}/SaprotHub/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"SaprotHub Dataset\",\n",
        "                  \"A pair of AA Sequences\",\n",
        "                  \"A pair of SA Sequences\",\n",
        "                  \"A pair of UniProt IDs\",\n",
        "                  \"A pair of PDB/CIF Structures\",\n",
        "                  \"Multiple pairs of AA Sequences\",\n",
        "                  \"Multiple pairs of SA Sequences\",\n",
        "                  \"Multiple pairs of UniProt IDs\",\n",
        "                  \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_single = [\n",
        "    \"Single AA Sequence\",\n",
        "    \"Single SA Sequence\",\n",
        "    \"Single UniProt ID\",\n",
        "    \"Single PDB/CIF Structure\",\n",
        "    \"A pair of AA Sequences\",\n",
        "    \"A pair of SA Sequences\",\n",
        "    \"A pair of UniProt IDs\",\n",
        "    \"A pair of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_multiple = [\n",
        "    \"Multiple AA Sequences\",\n",
        "    \"Multiple SA Sequences\",\n",
        "    \"Multiple UniProt IDs\",\n",
        "    \"Multiple PDB/CIF Structures\",\n",
        "    \"Multiple pairs of AA Sequences\",\n",
        "    \"Multiple pairs of SA Sequences\",\n",
        "    \"Multiple pairs of UniProt IDs\",\n",
        "    \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Protein-level Classification\": \"classification\",\n",
        "  \"Residue-level Classification\" : \"token_classification\",\n",
        "  \"Protein-level Regression\" : \"regression\",\n",
        "  \"Protein-protein Classification\": \"pair_classification\",\n",
        "  \"Protein-protein Regression\": \"pair_regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_model\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_dataset\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_dataset\",\n",
        "}\n",
        "training_data_type_dict = {\n",
        "  \"Single AA Sequence\": \"AA\",\n",
        "  \"Single SA Sequence\": \"SA\",\n",
        "  \"Single UniProt ID\": \"SA\",\n",
        "  \"Single PDB/CIF Structure\": \"SA\",\n",
        "  \"Multiple AA Sequences\": \"AA\",\n",
        "  \"Multiple SA Sequences\": \"SA\",\n",
        "  \"Multiple UniProt IDs\": \"SA\",\n",
        "  \"Multiple PDB/CIF Structures\": \"SA\",\n",
        "  \"SaprotHub Dataset\": \"SA\",\n",
        "  \"A pair of AA Sequences\": \"AA\",\n",
        "  \"A pair of SA Sequences\": \"SA\",\n",
        "  \"A pair of UniProt IDs\": \"SA\",\n",
        "  \"A pair of PDB/CIF Structures\": \"SA\",\n",
        "  \"Multiple pairs of AA Sequences\": \"AA\",\n",
        "  \"Multiple pairs of SA Sequences\": \"SA\",\n",
        "  \"Multiple pairs of UniProt IDs\": \"SA\",\n",
        "  \"Multiple pairs of PDB/CIF Structures\": \"SA\",\n",
        "}\n",
        "\n",
        "\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### adapters #######################################\n",
        "################################################################################\n",
        "def get_adapters_list(task_type=None):\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    if task_type:\n",
        "      for file_path in (ADAPTER_HOME / task_type).glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.relative_to(ADAPTER_HOME / task_type).parent)\n",
        "    else:\n",
        "      for file_path in ADAPTER_HOME.glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.relative_to(ADAPTER_HOME).parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    # options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    options=adapters_list,\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "  display(dropdown)\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def adapters_selectmultiple(adapters_list):\n",
        "  selectmulitiple = ipywidgets.SelectMultiple(\n",
        "  # options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "  options=adapters_list,\n",
        "  value=[],\n",
        "  #rows=10,\n",
        "  placeholder='Select multiple models',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(selectmulitiple)\n",
        "\n",
        "  return selectmulitiple\n",
        "\n",
        "def adapters_textmultiple(adapters_list):\n",
        "  textmultiple = ipywidgets.Text(\n",
        "  value=None,\n",
        "  placeholder='Enter multiple SaprotHub Model IDs, separated by commas.',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(textmultiple)\n",
        "\n",
        "  return textmultiple\n",
        "\n",
        "\n",
        "def select_adapter_from(task_type, use_model_from):\n",
        "  adapters_list = get_adapters_list(task_type)\n",
        "\n",
        "  if use_model_from == 'Trained by yourself on ColabSaprot':\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_dropdown(adapters_list)\n",
        "\n",
        "  elif use_model_from == 'Shared by peers on SaprotHub':\n",
        "    print(Fore.BLUE+\"SaprotHub Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Saved in your local computer\":\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    if task_type:\n",
        "      adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    else:\n",
        "      adapter_upload_path = ADAPTER_HOME / \"Local\"\n",
        "\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "\n",
        "    # # 4. move to correct folder\n",
        "    # num_labels, task_type = get_num_labels_and_task_type_by_adapter(adapter_path)\n",
        "    # shutil.move(adapter_path, ADAPTER_HOME / task_type)\n",
        "\n",
        "    return EasyDict({\"value\":  f\"Local/{adapter_zip_path.stem}\"})\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ColabSaprot\":\n",
        "    # 1. select the list of adapters\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Multiple values can be selected with \\\"shift\\\" and/or \\\"ctrl\\\" (or \\\"command\\\") pressed and mouse clicks or arrow keys.\"+Style.RESET_ALL)\n",
        "    return adapters_selectmultiple(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "    # 1. enter the list of adapters\n",
        "    print(Fore.BLUE+f\"SaprotHub Model IDs, separated by commas ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_textmultiple(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "def read_csv_dataset(uploaded_csv_path):\n",
        "  df = pd.read_csv(uploaded_csv_path)\n",
        "  df.columns = df.columns.str.lower()\n",
        "  return df\n",
        "\n",
        "def check_column_label_and_stage(csv_dataset_path):\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "  assert {'label', 'stage'}.issubset(df.columns), f\"Make sure your CSV dataset includes both `label` and `stage` columns!\\nCurrent columns: {df.columns}\"\n",
        "  column_values = set(df['stage'].unique())\n",
        "  assert all(value in column_values for value in ['train', 'valid', 'test']), f\"Ensure your dataset includes samples for all three stages: `train`, `valid` and `test`.\\nCurrent columns: {df.columns}\"\n",
        "\n",
        "def get_data_type(csv_dataset_path):\n",
        "  # AA, SA, Pair AA, Pair SA\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "  df = df.rename(columns={\n",
        "      \"protein_1\": \"sequence_1\",\n",
        "      \"protein_2\": \"sequence_2\",\n",
        "      \"protein\": \"sequence\"})\n",
        "\n",
        "  # AA, SA\n",
        "  if 'sequence' in df.columns:\n",
        "    second_token = df.loc[0, 'sequence'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  # Pair AA, Pair SA\n",
        "  elif 'sequence_1' in df.columns and 'sequence_2' in df.columns:\n",
        "    second_token = df.loc[0, 'sequence_1'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple pairs of AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple pairs of SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  else:\n",
        "      raise RuntimeError(f\"The data type of the dataset({csv_dataset_path}) should be one of the following types: Multiple AA Sequences, Multiple SA Sequences, Multiple pairs of AA Sequences, Multiple pairs of SA Sequences\")\n",
        "\n",
        "def check_task_type_and_data_type(original_task_type, data_type):\n",
        "  if \"Protein-protein\" in original_task_type:\n",
        "    assert data_type == \"SaprotHub Dataset\" or \"pair\" in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please use Pair Sequence Datset for {original_task_type} task!\"\n",
        "  else:\n",
        "    assert \"pair\" not in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please avoid using the Pair Sequence Dataset({data_type}) for the {original_task_type} task!\"\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "  print(Fore.BLUE+\"Dataset: \"+Style.RESET_ALL, end='')\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == 'Single PDB/CIF Structure':\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type = ipywidgets.Dropdown(\n",
        "      value=\"AF2\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type)\n",
        "\n",
        "    input_chain = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain here',\n",
        "      disabled=False)\n",
        "    input_chain.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "    display(input_chain)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    return pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "  # 4-7 & 13-16. Multiple Sequences\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type in ['Multiple PDB/CIF Structures', 'Multiple pairs of PDB/CIF Structures']:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Copy and paste the SaprotHub Dataset ID here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "  # 9-11. A pair of seq\n",
        "  elif data_type in [\"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\"]:\n",
        "    print()\n",
        "\n",
        "    seq_type = data_type[len(\"A pair of \"):-1]\n",
        "\n",
        "    input_seq1 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 1 here',\n",
        "      disabled=False)\n",
        "    input_seq1.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 1:\"+Style.RESET_ALL)\n",
        "    display(input_seq1)\n",
        "\n",
        "    input_seq2 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 2 here',\n",
        "      disabled=False)\n",
        "    input_seq2.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 2:\"+Style.RESET_ALL)\n",
        "    display(input_seq2)\n",
        "\n",
        "    return (input_seq1, input_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  elif data_type == 'A pair of PDB/CIF Structures':\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type1 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The first structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type1)\n",
        "\n",
        "    input_chain1 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the first structure here',\n",
        "      disabled=False)\n",
        "    input_chain1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the first structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain1)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path1 = upload_file(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    dropdown_type2 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The second structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type2)\n",
        "\n",
        "    input_chain2 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the second structure here',\n",
        "      disabled=False)\n",
        "    input_chain2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the second structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain2)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path2 = upload_file(STRUCTURE_HOME)\n",
        "    return (pdb_file_path1.stem, dropdown_type1, input_chain1, pdb_file_path2.stem, dropdown_type2, input_chain2)\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # Multiple sequences\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  if data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=DATASET_HOME / REPO_ID)\n",
        "    csv_dataset_path = DATASET_HOME / REPO_ID / 'dataset.csv'\n",
        "    assert csv_dataset_path.exists(), f\"Can't find {csv_dataset_path}\"\n",
        "    protein_df = read_csv_dataset(csv_dataset_path)\n",
        "\n",
        "    data_type = get_data_type(csv_dataset_path)\n",
        "\n",
        "    return get_SA_sequence_by_data_type(data_type, csv_dataset_path)\n",
        "\n",
        "    # # AA, SA\n",
        "    # if data_type == \"Multiple AA Sequences\":\n",
        "    #   for index, value in protein_df['sequence'].items():\n",
        "    #     sa_seq = ''\n",
        "    #     for aa in value:\n",
        "    #       sa_seq += aa + '#'\n",
        "    #     protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "    # # Pair AA, Pair SA\n",
        "    # elif data_type in [\"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\"]:\n",
        "    #   for i in ['1', '2']:\n",
        "    #     if data_type == \"Multiple pairs of AA Sequences\":\n",
        "    #       for index, value in protein_df[f'sequence_{i}'].items():\n",
        "    #         sa_seq = ''\n",
        "    #         for aa in value:\n",
        "    #           sa_seq += aa + '#'\n",
        "    #         protein_df.at[index, f'sequence_{i}'] = sa_seq\n",
        "\n",
        "    #     protein_df[f'name_{i}'] = f'name_{i}'\n",
        "    #     protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "    # protein_df.to_csv(csv_dataset_path, index=None)\n",
        "\n",
        "    # return csv_dataset_path\n",
        "\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "    protein_df = read_csv_dataset(uploaded_csv_path)\n",
        "    protein_df = protein_df.rename(columns={\n",
        "      \"protein_1\": \"sequence_1\",\n",
        "      \"protein_2\": \"sequence_2\",\n",
        "      \"protein\": \"sequence\"})\n",
        "\n",
        "    if 'pair' in data_type:\n",
        "      assert {'sequence_1', 'sequence_2'}.issubset(protein_df.columns), f\"The CSV dataset ({uploaded_csv_path}) must contain `sequence_1` and `sequence_2` columns. \\n Current columns:{protein_df.columns}\"\n",
        "    else:\n",
        "      assert 'sequence' in protein_df.columns, f\"The CSV Dataset({uploaded_csv_path}) must contain a `sequence` column. \\n Current columns:{protein_df.columns}\"\n",
        "\n",
        "    # 4. Multiple AA Sequences\n",
        "    if data_type == 'Multiple AA Sequences':\n",
        "      for index, value in protein_df['sequence'].items():\n",
        "        sa_seq = ''\n",
        "        for aa in value:\n",
        "          sa_seq += aa + '#'\n",
        "        protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 5. Multiple SA Sequences\n",
        "    elif data_type == 'Multiple SA Sequences':\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 6. Multiple UniProt IDs\n",
        "    elif data_type == 'Multiple UniProt IDs':\n",
        "      protein_list = protein_df.loc[:, 'sequence'].tolist()\n",
        "      uniprot2pdb(protein_list)\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 7. Multiple PDB/CIF Structures\n",
        "    elif data_type == 'Multiple PDB/CIF Structures':\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "      protein_list = []\n",
        "      for row_tuple in protein_df.itertuples(index=False):\n",
        "        assert row_tuple.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "        protein_list.append(row_tuple)\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 13. Pair Multiple AA Sequences\n",
        "    elif data_type == \"Multiple pairs of AA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        for index, value in protein_df[f'sequence_{i}'].items():\n",
        "          sa_seq = ''\n",
        "          for aa in value:\n",
        "            sa_seq += aa + '#'\n",
        "          protein_df.at[index, f'sequence_{i}'] = sa_seq\n",
        "\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 14. Pair Multiple SA Sequences\n",
        "    elif data_type == \"Multiple pairs of SA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 15. Pair Multiple UniProt IDs\n",
        "    elif data_type == \"Multiple pairs of UniProt IDs\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = protein_df.loc[:, f'sequence_{i}'].tolist()\n",
        "        uniprot2pdb(protein_list)\n",
        "        protein_df[f'name_{i}'] = protein_list\n",
        "        protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        protein_df[f'sequence_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    elif data_type ==  \"Multiple pairs of PDB/CIF Structures\":\n",
        "      # columns: sequence_1, sequence_2, type_1, type_2, chain_1, chain_2, label, stage\n",
        "\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = []\n",
        "        for index, row in protein_df.iterrows():\n",
        "          assert row[f\"type_{i}\"] in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "          row_tuple = (row[f\"sequence_{i}\"], row[f\"type_{i}\"], row[f\"chain_{i}\"])\n",
        "          protein_list.append(row_tuple)\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        # add name column, del type column\n",
        "        protein_df[f'name_{i}'] = protein_df[f'sequence_{i}'].apply(lambda x: x.split('.')[0])\n",
        "        protein_df.drop(f\"type_{i}\", axis=1, inplace=True)\n",
        "        protein_df[f'sequence_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "\n",
        "      # columns: name_1, name_2, chain_1, chain_2, sequence_1, sequence_2, label, stage\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "  else:\n",
        "    # 0. Single AA Sequence\n",
        "    if data_type == 'Single AA Sequence':\n",
        "      input_seq = raw_data\n",
        "      aa_seq = input_seq.value\n",
        "\n",
        "      sa_seq = ''\n",
        "      for aa in aa_seq:\n",
        "          sa_seq += aa + '#'\n",
        "      return sa_seq\n",
        "\n",
        "    # 1. Single SA Sequence\n",
        "    elif data_type == 'Single SA Sequence':\n",
        "      input_seq = raw_data\n",
        "      sa_seq = input_seq.value\n",
        "\n",
        "      return sa_seq\n",
        "\n",
        "    # 2. Single UniProt ID\n",
        "    elif data_type == 'Single UniProt ID':\n",
        "      input_seq = raw_data\n",
        "      uniprot_id = input_seq.value\n",
        "\n",
        "\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\")]\n",
        "      uniprot2pdb([protein_list[0][0]])\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 3. Single PDB/CIF Structure\n",
        "    elif data_type == 'Single PDB/CIF Structure':\n",
        "      uniprot_id = raw_data[0]\n",
        "      struc_type = raw_data[1].value\n",
        "      chain = raw_data[2].value\n",
        "\n",
        "      protein_list = [(uniprot_id, struc_type, chain)]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      assert len(seqs)>0, \"Unable to convert to SA sequence. Please check the `type`, `chain`, and `.pdb/.cif file`.\"\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 9. Pair Single AA Sequences\n",
        "    elif data_type == \"A pair of AA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 10. Pair Single SA Sequences\n",
        "    elif data_type ==  \"A pair of SA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 11. Pair Single UniProt IDs\n",
        "    elif data_type ==  \"A pair of UniProt IDs\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 12. Pair Single PDB/CIF Structure\n",
        "    elif data_type == \"A pair of PDB/CIF Structures\":\n",
        "      uniprot_id1 = raw_data[0]\n",
        "      struc_type1 = raw_data[1].value\n",
        "      chain1 = raw_data[2].value\n",
        "\n",
        "      protein_list1 = [(uniprot_id1, struc_type1, chain1)]\n",
        "      mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs1 = mprs1.run()\n",
        "      sa_seq1 = seqs1[0].split('\\t')[1]\n",
        "\n",
        "      uniprot_id2 = raw_data[3]\n",
        "      struc_type2 = raw_data[4].value\n",
        "      chain2 = raw_data[5].value\n",
        "\n",
        "      protein_list2 = [(uniprot_id2, struc_type2, chain2)]\n",
        "      mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs2 = mprs2.run()\n",
        "      sa_seq2 = seqs2[0].split('\\t')[1]\n",
        "      return sa_seq1, sa_seq2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "\n",
        "  # print(\"=\"*100)\n",
        "  # print(row_tuple)\n",
        "  # print(\"=\"*100)\n",
        "  uniprot_id = row_tuple[0].split('.')[0]     #\n",
        "  struc_type = row_tuple[1]                   # PDB or AF2\n",
        "  chain = row_tuple[2]\n",
        "\n",
        "  if struc_type==\"AF2\":\n",
        "    plddt_mask= True\n",
        "    chain = 'A'\n",
        "  else:\n",
        "    plddt_mask= False\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "    if Path(pdb_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    elif Path(cif_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    else:\n",
        "      raise BaseException(f\"The {uniprot_id}.pdb/{uniprot_id}.cif file doesn't exists!\")\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "          \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "          \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "          \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "          \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "def convert_outputs_to_pdb(outputs):\n",
        "\tfinal_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
        "\toutputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
        "\tfinal_atom_positions = final_atom_positions.cpu().numpy()\n",
        "\tfinal_atom_mask = outputs[\"atom37_atom_exists\"]\n",
        "\tpdbs = []\n",
        "\toutputs[\"plddt\"] *= 100\n",
        "\n",
        "\tfor i in range(outputs[\"aatype\"].shape[0]):\n",
        "\t\taa = outputs[\"aatype\"][i]\n",
        "\t\tpred_pos = final_atom_positions[i]\n",
        "\t\tmask = final_atom_mask[i]\n",
        "\t\tresid = outputs[\"residue_index\"][i] + 1\n",
        "\t\tpred = OFProtein(\n",
        "\t\t    aatype=aa,\n",
        "\t\t    atom_positions=pred_pos,\n",
        "\t\t    atom_mask=mask,\n",
        "\t\t    residue_index=resid,\n",
        "\t\t    b_factors=outputs[\"plddt\"][i],\n",
        "\t\t    chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
        "\t\t)\n",
        "\t\tpdbs.append(to_pdb(pred))\n",
        "\treturn pdbs\n",
        "\n",
        "\n",
        "# This function is copied from ColabFold!\n",
        "def show_pdb(path, show_sidechains=False, show_mainchains=False, color=\"lddt\"):\n",
        "  file_type = str(path).split(\".\")[-1]\n",
        "  if file_type == \"cif\":\n",
        "    file_type == \"mmcif\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(path,'r').read(),file_type)\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(get_chain_ids(path))\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###############   Download file to local computer   ##################\n",
        "################################################################################\n",
        "def file_download(path: str):\n",
        "  with open(path, \"rb\") as r:\n",
        "    res = r.read()\n",
        "\n",
        "  #FILE\n",
        "  filename = os.path.basename(path)\n",
        "  b64 = base64.b64encode(res)\n",
        "  payload = b64.decode()\n",
        "\n",
        "  #BUTTONS\n",
        "  html_buttons = '''<html>\n",
        "  <head>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "  </head>\n",
        "  <body>\n",
        "  <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "  <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "  </a>\n",
        "  </body>\n",
        "  </html>\n",
        "  '''\n",
        "\n",
        "  html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "  display(HTML(html_button))\n",
        "\n",
        "  # Automatically download file if the server is from google cloud.\n",
        "  if root_dir == \"/content\":\n",
        "    files.download(path)\n",
        "\n",
        "################################################################################\n",
        "############################ MODEL INFO #######################################\n",
        "################################################################################\n",
        "def get_base_model(adapter_path):\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    adapter_config_dict = json.load(f)\n",
        "    base_model = adapter_config_dict['base_model_name_or_path']\n",
        "    if 'SaProt_650M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    elif 'SaProt_35M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    else:\n",
        "      raise RuntimeError(\"Please ensure the base model is \\\"SaProt_650M_AF2\\\" or \\\"SaProt_35M_AF2\\\"\")\n",
        "  return base_model\n",
        "\n",
        "def check_training_data_type(adapter_path, data_type):\n",
        "  metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "  if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      required_training_data_type = metadata['training_data_type']\n",
        "  else:\n",
        "    required_training_data_type = \"SA\"\n",
        "\n",
        "  if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with AA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) includes structural information, which will not be used for predictions.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "  elif (required_training_data_type == \"SA\") and (\"AA\" in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with SA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) does not include structural information, which may lead to weak prediction performance.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"If you only have the amino acid sequence, we strongly recommend using AF2 to predict the structure and generate a PDB file before prediction.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "\n",
        "  return required_training_data_type\n",
        "\n",
        "def mask_struc_token(sequence):\n",
        "    return ''.join('#' if i % 2 == 1 and char.islower() else char for i, char in enumerate(sequence))\n",
        "\n",
        "def get_num_labels_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "        file_path = adapter_path / 'adapter_model.safetensors'\n",
        "        with safe_open(file_path, framework=\"pt\") as f:\n",
        "          if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "          elif 'base_model.model.classifier.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "          else:\n",
        "              raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path)\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in f.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    return num_labels\n",
        "\n",
        "def get_num_labels_and_task_type_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    task_type = None\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "      file_path = adapter_path / 'adapter_model.safetensors'\n",
        "      with safe_open(file_path, framework=\"pt\") as f:\n",
        "        if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "          tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "        elif 'base_model.model.classifier.bias' in f.keys():\n",
        "          task_type = 'token_classification'\n",
        "          tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "        else:\n",
        "          raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path)\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in f.keys():\n",
        "        task_type = 'token_classification'\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    if task_type != 'token_classification':\n",
        "      if num_labels > 1:\n",
        "        task_type = 'classification'\n",
        "      elif num_labels == 1:\n",
        "        task_type = 'regression'\n",
        "\n",
        "    return num_labels, task_type\n",
        "\n",
        "################################################################################\n",
        "############################ INFO ##############################################\n",
        "################################################################################\n",
        "clear_output(wait=True)\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uxag_RSBI7e"
      },
      "source": [
        "# **2: Train and Share your Protein Model** <a name=\"train\"></a>\n",
        "\n",
        "You can **train** a model based on pre-trained SaProt, or **continually train** a fine-tuned model in SaprotHub.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- ## Training Dataset\n",
        "\n",
        "For the training dataset, **two additional columns** are required in the CSV file: `label` and `stage`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_training.png\n",
        "?raw=true\" height=\"200\" width=\"400px\" align=\"center\">\n",
        "\n",
        "### Column `label`\n",
        "\n",
        "The content of column `label` depends on your **task type**:\n",
        "\n",
        "| Task Type                         | Content in the Column                          |\n",
        "|-----------------------------------|------------------------------------------------|\n",
        "| Classification tasks              | Category index starting from zero              |\n",
        "| Amino Acid Classification tasks   | A list of category indices for each amino acid |\n",
        "| Regression tasks                  | Numerical values                               |\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/label_format.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "<br>\n",
        "\n",
        "\n",
        "### Column `stage`\n",
        "\n",
        "The column `stage` indicate whether the sample is used for training, validation, or testing. Ensure your dataset includes samples for all three stages. The values are: `train`, `valid`, `test`.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Note:**\n",
        "\n",
        "1. **Examples are available** at /content/SaprotHub/upload_files (if you connect to your local server, then the path is /SaprotHub/upload_files). Download to review their format, and then upload them for a trial.\n",
        "\n",
        "2.  <a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "3. <a href=\"#fa2csv\">Here</a> you can **convert your .fa/.fasta file to a .csv file**, which corresponds to the data format for Multiple AA Sequences.\n",
        "\n",
        "4. <a href=\"#split_dataset\">Here</a> you can **randomly split your .csv dataset**, which means to add a `stage` column, where the ratio of `train`:`valid`:`test` is 8:1:1.\n",
        "\n",
        "4. The maximum input length of the model is 1024, and protein sequences exceeding this length will only retain the first 1024 amino acids. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8po43iIU7NcG"
      },
      "source": [
        "## **2.1: Train your Model** <a name=\"train\"></a>\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model)\n",
        "\n",
        "\n",
        "‚ö†Ô∏èIf you want to **interrupt** the training, **do not** click the run button again. Please refer to [here](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model#interrupt-training-to-avoid-overfitting).\n",
        "\n",
        "Example datasets are available in at /content/SaprotHub/upload_files/example_csv_dataset and [Github Repository](https://github.com/westlake-repl/SaprotHub/tree/main/upload_files/example_csv_dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vqdmLslQBI7e"
      },
      "outputs": [],
      "source": [
        "##@title **2.1: Train your Model** <a name=\"train\"></a>\n",
        "\n",
        "################################################################################\n",
        "############################# ADVANCED CONFIG ##################################\n",
        "################################################################################\n",
        "\n",
        "# training config\n",
        "GPU_batch_size = 0\n",
        "accumulate_grad_batches = 0\n",
        "num_workers = 2\n",
        "seed = 20000812\n",
        "\n",
        "# lora config\n",
        "r = 8\n",
        "lora_dropout = 0.0\n",
        "lora_alpha = 16\n",
        "\n",
        "# dataset config\n",
        "val_check_interval=0.5\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "\n",
        "mask_struc_ratio=None\n",
        "\n",
        "################################################################################\n",
        "################################## MARKDOWN #################################\n",
        "################################################################################\n",
        "##@markdown ‚ö†Ô∏èIf you want to **interrupt** the training, **do not** click the run button again. Please refer to [here](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model#interrupt-training-to-avoid-overfitting).\n",
        "\n",
        "##@markdown Example datasets are available in at /content/SaprotHub/upload_files/example_csv_dataset and [Github Repository](https://github.com/westlake-repl/SaprotHub/tree/main/upload_files/example_csv_dataset).\n",
        "\n",
        "##@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "  raise BaseException(\"Please refer to Section 1.1 to switch your Runtime to a GPU!\")\n",
        "\n",
        "################################################################################\n",
        "################################## TASK CONFIG #################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "task_name = \"demo\" # @param {type:\"string\"}\n",
        "task_type = \"Protein-level Classification\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "original_task_type = task_type\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'Enter the number of category in your training dataset here:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "#################################### MODEL CONFIG #####################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "base_model = \"Official pretrained SaProt (35M)\" # @param [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]\n",
        "\n",
        "# continue learning\n",
        "if base_model in [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]:\n",
        "  continue_learning = True\n",
        "  adapter_combobox = select_adapter_from(task_type, use_model_from=base_model)\n",
        "else:\n",
        "  continue_learning = False\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "\n",
        "data_type = \"SaprotHub Dataset\" # @param [\"SaprotHub Dataset\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "check_task_type_and_data_type(original_task_type, data_type)\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 4. Training\n",
        "\n",
        "batch_size = \"Adaptive\" # @param [\"Adaptive\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]\n",
        "max_epochs = 1 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-3 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# CONFIG #######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN ####################################\n",
        "################################################################################\n",
        "\n",
        "def train(button):\n",
        "  global base_model\n",
        "  global GPU_batch_size\n",
        "  global accumulate_grad_batches\n",
        "\n",
        "  button.disabled = True\n",
        "  button.description = 'Training...'\n",
        "  button.button_style = ''\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIRM ####################################\n",
        "################################################################################\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  check_column_label_and_stage(csv_dataset_path)\n",
        "  from saprot.utils.construct_lmdb import construct_lmdb\n",
        "  construct_lmdb(csv_dataset_path, LMDB_HOME, task_name, task_type)\n",
        "  lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "################################################################################\n",
        "################################### MODEL CONFIRM ####################################\n",
        "################################################################################\n",
        "\n",
        "  # base_model\n",
        "  if continue_learning:\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "    print(f\"Training on an existing model: {adapter_path}\")\n",
        "\n",
        "    if base_model == \"Shared by peers on SaprotHub\":\n",
        "      if not adapter_path.exists():\n",
        "        snapshot_download(repo_id=adapter_combobox.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    adapter_config_path = Path(adapter_path) / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "      adapter_config = json.load(f)\n",
        "      base_model = adapter_config['base_model_name_or_path']\n",
        "\n",
        "  elif base_model == \"Official pretrained SaProt (35M)\":\n",
        "    base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "\n",
        "  elif base_model == \"Official pretrained SaProt (650M)\":\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "  # model size and model name\n",
        "  if base_model == \"westlake-repl/SaProt_650M_AF2\":\n",
        "    model_size = \"650M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "  elif base_model == \"westlake-repl/SaProt_35M_AF2\":\n",
        "    model_size = \"35M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "\n",
        "  config.setting.run_mode = \"train\"\n",
        "  config.setting.seed = seed\n",
        "\n",
        "################################################################################\n",
        "################################# MODEL ########################################\n",
        "################################################################################\n",
        "\n",
        "  if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "  config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / \"Local\" / model_name)\n",
        "\n",
        "  if task_type in [\"regression\", \"pair_regression\"]:\n",
        "    config.model.kwargs.extra_config = {}\n",
        "    config.model.kwargs.extra_config.attention_probs_dropout_prob=0\n",
        "    config.model.kwargs.extra_config.hidden_dropout_prob=0\n",
        "\n",
        "  config.model.kwargs.lora_kwargs = EasyDict({\n",
        "    \"is_trainable\": True,\n",
        "    \"num_lora\": 1,\n",
        "    \"r\": r,\n",
        "    \"lora_dropout\": lora_dropout,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"config_list\": []})\n",
        "  if continue_learning:\n",
        "    config.model.kwargs.lora_kwargs.config_list.append({\"lora_config_path\": adapter_path})\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "  config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "\n",
        "  config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "  config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "  config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "  # num_workers\n",
        "  config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "  # mask_struc\n",
        "  # config.dataset.kwargs.mask_struc_ratio= mask_struc_ratio\n",
        "\n",
        "  ################################################################################\n",
        "  ######################## batch size ############################################\n",
        "  ################################################################################\n",
        "  def get_accumulate_grad_samples(num_samples):\n",
        "      if num_samples > 3200:\n",
        "          return 64\n",
        "      elif 1600 < num_samples <= 3200:\n",
        "          return 32\n",
        "      elif 800 < num_samples <= 1600:\n",
        "          return 16\n",
        "      elif 400 < num_samples <= 800:\n",
        "          return 8\n",
        "      elif 200 < num_samples <= 400:\n",
        "          return 4\n",
        "      elif 100 < num_samples <= 200:\n",
        "          return 2\n",
        "      else:\n",
        "          return 1\n",
        "\n",
        "  # advanced config\n",
        "  if (GPU_batch_size > 0) and (accumulate_grad_batches > 0):\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  elif (GPU_batch_size == 0) and (accumulate_grad_batches == 0):\n",
        "\n",
        "    # batch_size\n",
        "    if base_model == \"westlake-repl/SaProt_650M_AF2\" and root_dir == \"/content\":\n",
        "      GPU_batch_size = 1\n",
        "    else:\n",
        "      GPU_batch_size_dict = {\n",
        "        \"Tesla T4\": 2,\n",
        "        \"NVIDIA L4\": 2,\n",
        "        \"NVIDIA A100-SXM4-40GB\": 4,\n",
        "        }\n",
        "      GPU_name = torch.cuda.get_device_name(0)\n",
        "      GPU_batch_size = GPU_batch_size_dict[GPU_name] if GPU_name in GPU_batch_size_dict else 2\n",
        "\n",
        "      if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "        GPU_batch_size = int(max(GPU_batch_size / 2, 1))\n",
        "\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "\n",
        "    # accumulate_grad_batches\n",
        "    if batch_size == \"Adaptive\":\n",
        "\n",
        "      env = lmdb.open(config.dataset.train_lmdb, readonly=True)\n",
        "\n",
        "      with env.begin() as txn:\n",
        "        stat = txn.stat()\n",
        "        num_samples = stat['entries']\n",
        "\n",
        "      accumulate_grad_samples = get_accumulate_grad_samples(num_samples)\n",
        "\n",
        "    else:\n",
        "      accumulate_grad_samples = int(batch_size)\n",
        "\n",
        "    accumulate_grad_batches = max(int(accumulate_grad_samples / GPU_batch_size), 1)\n",
        "\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  else:\n",
        "    raise BaseException(f\"Please make sure `GPU_batch_size`({GPU_batch_size}) and `accumulate_grad_batches`({accumulate_grad_batches}) are both greater than zero!\")\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## TRAINER #########################################\n",
        "  ################################################################################\n",
        "\n",
        "  config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  # epoch\n",
        "  config.Trainer.max_epochs = max_epochs\n",
        "  # test only: load the existing model\n",
        "  if config.Trainer.max_epochs == 0 and continue_learning:\n",
        "    config.model.save_path = config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']\n",
        "\n",
        "  # learning rate\n",
        "  config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "  # trainer\n",
        "  config.Trainer.limit_train_batches=limit_train_batches\n",
        "  config.Trainer.limit_val_batches=limit_val_batches\n",
        "  config.Trainer.limit_test_batches=limit_test_batches\n",
        "  config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "  # strategy\n",
        "  strategy = {\n",
        "      # - deepspeed\n",
        "      # 'class': 'DeepSpeedStrategy',\n",
        "      # 'stage': 2\n",
        "\n",
        "      # - None\n",
        "      # 'class': None,\n",
        "\n",
        "      # - DP\n",
        "      # 'class': 'DataParallelStrategy',\n",
        "\n",
        "      # - DDP\n",
        "      # 'class': 'DDPStrategy',\n",
        "      # 'find_unused_parameter': True\n",
        "  }\n",
        "  config.Trainer.strategy = strategy\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Run the task ####################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f\"Training task type: {task_type}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "  if continue_learning:\n",
        "    print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']}\"+Style.RESET_ALL)\n",
        "  print('='*100)\n",
        "  pprint.pprint(config)\n",
        "  print('='*100)\n",
        "\n",
        "  from saprot.scripts.training import finetune\n",
        "  finetune(config)\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Save the adapter ################################\n",
        "  ################################################################################\n",
        "\n",
        "  def add_training_data_type_to_config(metadata_path, training_data_type):\n",
        "    if metadata_path.exists() is False:\n",
        "      config_data = {\n",
        "          'training_data_type': training_data_type\n",
        "          }\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "    else:\n",
        "      with open(metadata_path, 'r') as file:\n",
        "          config_data = json.load(file)\n",
        "\n",
        "      config_data['training_data_type'] = training_data_type\n",
        "\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "  metadata_path = Path(config.model.save_path) / \"metadata.json\"\n",
        "  training_data_type = training_data_type_dict[data_type]\n",
        "  add_training_data_type_to_config(metadata_path, training_data_type)\n",
        "\n",
        "  print(Fore.BLUE)\n",
        "  print(f\"Model is saved to \\\"{config.model.save_path}\\\" on Colab Server\")\n",
        "  print(Style.RESET_ALL)\n",
        "\n",
        "\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{model_name}.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\" \"metadata.json\"\n",
        "  # !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\" \"metadata.json\"\n",
        "  print(\"Click to download the model to your local computer\")\n",
        "  if adapter_zip.exists():\n",
        "    # files.download(adapter_zip)\n",
        "    file_download(adapter_zip)\n",
        "\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ############################### Modify README ##################################\n",
        "  ################################################################################\n",
        "  name = model_name\n",
        "  description = '<slot name=\\'description\\'>'\n",
        "  label_meanings = '<slot name=\\'label_meanings\\'>'\n",
        "\n",
        "  with open(f'{config.model.save_path}/adapter_config.json', 'r') as f:\n",
        "    lora_config = json.load(f)\n",
        "\n",
        "  markdown = f'''\n",
        "---\n",
        "\n",
        "base_model: {base_model} \\n\n",
        "library_name: peft\n",
        "\n",
        "---\n",
        "\\n\n",
        "\n",
        "# Model Card for {name}\n",
        "{description}\n",
        "\n",
        "## Task type\n",
        "{original_task_type}\n",
        "\n",
        "## Model input type\n",
        "{training_data_type_dict[data_type]} Sequence\n",
        "\n",
        "## Label meanings\n",
        "{label_meanings}\n",
        "\n",
        "## LoRA config\n",
        "\n",
        "- **r:** {lora_config['r']}\n",
        "- **lora_dropout:** {lora_config['lora_dropout']}\n",
        "- **lora_alpha:** {lora_config['lora_alpha']}\n",
        "- **target_modules:** {lora_config['target_modules']}\n",
        "- **modules_to_save:** {lora_config['modules_to_save']}\n",
        "\n",
        "## Training config\n",
        "\n",
        "- **optimizer:**\n",
        "  - **class:** AdamW\n",
        "  - **betas:** (0.9, 0.98)\n",
        "  - **weight_decay:** 0.01\n",
        "- **learning rate:** {config.model.lr_scheduler_kwargs.init_lr}\n",
        "- **epoch:** {config.Trainer.max_epochs}\n",
        "- **batch size:** {config.dataset.dataloader_kwargs.batch_size * config.Trainer.accumulate_grad_batches}\n",
        "- **precision:** 16-mixed \\n\n",
        "'''\n",
        "\n",
        "  # Write the markdown output to a file\n",
        "  with open(f\"{config.model.save_path}/README.md\", \"w\") as file:\n",
        "    file.write(markdown)\n",
        "\n",
        "\n",
        "button_train = ipywidgets.Button(\n",
        "    description='Start Training',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_train.on_click(train)\n",
        "button_train.layout.width = '300px'\n",
        "display(button_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUP-Iz2Z5pu0"
      },
      "source": [
        "## **2.2: Upload your model (Optional)** <a name=\"upload_model\"></a>\n",
        "\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.2:-Upload-your-model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UnKX1BTZBI7f"
      },
      "outputs": [],
      "source": [
        "#@title **2.2.1: Login Huggingface**\n",
        "#@markdown Click the run button to login Huggingface\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6XlluTsPBI7m"
      },
      "outputs": [],
      "source": [
        "##@title **2.3: Upload your Model (Optional)**\n",
        "#@title **2.2.2: Upload your Model**\n",
        "\n",
        "\n",
        "# #@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "\n",
        "################################################################################\n",
        "########################## Metadata  ###########################################\n",
        "################################################################################\n",
        "name = \"demo_cls\" # @param {type:\"string\"}\n",
        "description = \"This model is used for a demo classification task\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# #@markdown > 0: Nucleus <br>\n",
        "# #@markdown > 1: Cytoplasm <br>\n",
        "# #@markdown > 2: Extracellular <br>\n",
        "# #@markdown > ... <br>\n",
        "# #@markdown > 9: Peroxisome <br>\n",
        "\n",
        "label_meanings = \"A, B\" #@param {type:\"string\"}\n",
        "\n",
        "################################################################################\n",
        "########################### Move Files  ########################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "if name == \"\":\n",
        "  name = model_name\n",
        "repo_name = user['name'] + '/' + name\n",
        "local_dir = Path(\"/content/SaprotHub/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "################################################################################\n",
        "########################## Modify README  ######################################\n",
        "################################################################################\n",
        "import json\n",
        "\n",
        "md_path = local_dir / \"README.md\"\n",
        "\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    label_meanings_md = ''\n",
        "    for index, label in enumerate(label_meanings.split(', ')):\n",
        "      label_meanings_md += f'''\n",
        "{index}: {label.strip()}\n",
        "'''\n",
        "    label_meanings = label_meanings_md\n",
        "\n",
        "replace_data = {\n",
        "    '<slot name=\\'description\\'>': description,\n",
        "    '<slot name=\\'label_meanings\\'>': label_meanings\n",
        "}\n",
        "\n",
        "with open(md_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "for key, value in replace_data.items():\n",
        "    if value != \"\":\n",
        "        content = content.replace(key, value)\n",
        "\n",
        "with open(md_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1JgmrsBI7m"
      },
      "source": [
        "# **3: Use SaProt to Predict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4359yf2P5DAM"
      },
      "source": [
        "## **3.1: Classification&Regression Prediction** <a name=\"prediction\"></a>\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.1:-Classification-Regression-Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h8qHRJtIQxU4"
      },
      "outputs": [],
      "source": [
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "################################################################################\n",
        "################################# TASK #########################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "\n",
        "task_type = \"Protein-level Classification\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "original_task_type = task_type\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'The number of categories in your classification task:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################## MODEL #######################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "use_model_from = \"Trained by yourself on ColabSaprot\" # @param [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\", \"Multi-models on SaprotHub\"]\n",
        "if use_model_from == \"Multi-models on SaprotHub\":\n",
        "  multi_lora = True\n",
        "else:\n",
        "  multi_lora = False\n",
        "\n",
        "adapter_input = select_adapter_from(task_type, use_model_from)\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################ DATASET #######################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\", \"A pair of PDB/CIF Structures\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "check_task_type_and_data_type(original_task_type, data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if (data_type in data_type_list_multiple) else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "################################################################################\n",
        "##################################### PREDICT ###################################\n",
        "################################################################################\n",
        "def predict(button):\n",
        "  button.disabled = True\n",
        "  button.description = 'Predicting...'\n",
        "  button.button_style = ''\n",
        "\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# MODEL ###################################\n",
        "  ##############################################################################\n",
        "  if multi_lora:\n",
        "    if use_model_from == \"Multi-models on ColabSaprot\":\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / lora_config_path}) for lora_config_path in list(adapter_input.value)]\n",
        "    elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "      #1. get adapter_list\n",
        "      repo_id_list = adapter_input.value.replace(\" \", \"\").split(',')\n",
        "      #2. download adapters\n",
        "      for repo_id in repo_id_list:\n",
        "        snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / repo_id)\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "    assert len(config_list) > 0, \"Please select your models from the dropdown menu on the output of 3.1!\"\n",
        "    base_model = get_base_model(ADAPTER_HOME / task_type / config_list[0].lora_config_path)\n",
        "\n",
        "    required_training_data_type_list = []\n",
        "    for lora_config in config_list:\n",
        "      required_training_data_type_list.append(check_training_data_type(lora_config.lora_config_path, data_type))\n",
        "    assert len(set(required_training_data_type_list)) == 1, f\"Error: The input data types of these models are not identical: {required_training_data_type_list}\"\n",
        "    required_training_data_type = required_training_data_type_list[0]\n",
        "\n",
        "    lora_kwargs = EasyDict({\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": len(config_list),\n",
        "      \"config_list\": config_list\n",
        "    })\n",
        "\n",
        "  else:\n",
        "    if use_model_from == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / adapter_input.value)\n",
        "\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_input.value\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    required_training_data_type = check_training_data_type(adapter_path, data_type)\n",
        "    lora_kwargs = {\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# DATASET ###################################\n",
        "  ##############################################################################\n",
        "  if data_type in data_type_list_multiple:\n",
        "    csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    df = read_csv_dataset(csv_dataset_path)\n",
        "  else:\n",
        "    single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "      df = pd.DataFrame({\n",
        "          'sequence_1': [single_sa_seq[0]],\n",
        "          'sequence_2': [single_sa_seq[1]]\n",
        "      })\n",
        "    else:\n",
        "      df = pd.DataFrame({\n",
        "          'sequence': [single_sa_seq]\n",
        "      })\n",
        "\n",
        "  if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "    if 'sequence' in df.columns:\n",
        "      df['sequence'] = df['sequence'].apply(mask_struc_token)\n",
        "    elif 'sequence_1' in df.columns and 'sequence_2' in df.columns:\n",
        "      df['sequence_1'] = df['sequence_1'].apply(mask_struc_token)\n",
        "      df['sequence_2'] = df['sequence_2'].apply(mask_struc_token)\n",
        "\n",
        "  ################################################################################\n",
        "  ##################################### CONFIG ###################################\n",
        "  ################################################################################\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "  # task\n",
        "  if task_type in [ \"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### LOAD MODEL ##################################\n",
        "  ################################################################################\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFO #######################################\n",
        "  ################################################################################\n",
        "  # clear_output(wait=True)\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  print(Fore.BLUE+f\"Task Type: {original_task_type}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f\"Model ({use_model_from}):\"+Style.RESET_ALL)\n",
        "  if multi_lora:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter:\"+Style.RESET_ALL)\n",
        "    for lora_config in lora_kwargs.config_list:\n",
        "      print(Fore.BLUE+f\"    {lora_config.lora_config_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter: {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f'Dataset ({data_type}):' +Style.RESET_ALL)\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"  CSV Dataset Path: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    if \"A pair of\" in data_type:\n",
        "      print(Fore.BLUE+f\"  Sequence 1: {single_sa_seq[0]}\"+Style.RESET_ALL)\n",
        "      print(Fore.BLUE+f\"  Sequence 2: {single_sa_seq[1]}\"+Style.RESET_ALL)\n",
        "    else:\n",
        "      print(Fore.BLUE+f\"  Sequence: {single_sa_seq}\"+Style.RESET_ALL)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFERENCE ##################################\n",
        "  ################################################################################\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+\"Prediction Result:\"+Style.RESET_ALL)\n",
        "\n",
        "  outputs_list=[]\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "      input_1 = tokenizer(row['sequence_1'], return_tensors=\"pt\")\n",
        "      input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "      input_2 = tokenizer(row['sequence_2'], return_tensors=\"pt\")\n",
        "      input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "      with torch.no_grad(): outputs = model(input_1, input_2)\n",
        "      outputs_list.append(outputs)\n",
        "  else:\n",
        "    for index in tqdm(range(len(df))):\n",
        "      seq = df['sequence'].iloc[index]\n",
        "      inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      with torch.no_grad(): outputs = model(inputs)\n",
        "      outputs_list.append(outputs)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### RESULT ##################################\n",
        "  ################################################################################\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "\n",
        "  if task_type == \"pair_classification\":\n",
        "    softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "    print()\n",
        "    for index, output in enumerate(softmax_output_list):\n",
        "      print(f\"For Sequence Pair {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "      df.loc[index, 'result'] = output.index(max(output))\n",
        "      df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"pair_regression\":\n",
        "    print()\n",
        "    for index, output in enumerate(outputs_list):\n",
        "      print(f\"For Sequence Pair {index}, Value {output.cpu().item()}\")\n",
        "    df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"classification\":\n",
        "    print()\n",
        "    softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "    for index, output in enumerate(softmax_output_list):\n",
        "      print(f\"For Sequence {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "      df.loc[index, 'result'] = output.index(max(output))\n",
        "      df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"regression\":\n",
        "    print()\n",
        "    for index, output in enumerate(outputs_list):\n",
        "      print(f\"For Sequence {index}, Value {output.item()}\")\n",
        "    df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"token_classification\":\n",
        "    seq_prob_df_list = []\n",
        "    softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "    # print(\"The probability of each category:\")\n",
        "    for seq_index, seq in enumerate(softmax_output_list):\n",
        "      seq_prob_df = pd.DataFrame(seq)[1:-1]\n",
        "      # print('='*100)\n",
        "      # print(f'Sequence {seq_index + 1}:')\n",
        "      # print(seq_prob_df.to_string())\n",
        "      seq_prob_df['seq_index'] = seq_index\n",
        "      seq_prob_df['aa_index'] = seq_prob_df.index\n",
        "      seq_prob_df['sequence'] = df.loc[seq_index, 'sequence']\n",
        "      seq_prob_df_list.append(seq_prob_df)\n",
        "    combined_df = pd.concat(seq_prob_df_list, ignore_index=False)\n",
        "    combined_df.to_csv(output_file, index=True)\n",
        "\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f\"The prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "  file_download(output_file)\n",
        "\n",
        "################################################################################\n",
        "#################################### BUTTON #################################\n",
        "################################################################################\n",
        "button_predict = ipywidgets.Button(\n",
        "    description='Make Prediction',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_predict.on_click(predict)\n",
        "# button_predict.layout.width = '500px'\n",
        "display(button_predict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPtlNrDl5Sru"
      },
      "source": [
        "## **3.2: Zero-shot Mutational Effect Prediction** <a name=\"mutational_effect_prediction\"></a>\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.2:-Mutational-Effect-Prediction)\n",
        "\n",
        "\n",
        "The model takes the \"wild type sequence\" and [\"mutation information\"](https://github.com/westlake-repl/SaprotHub/wiki/3.2:-Mutational-Effect-Prediction#mutation-information) as **input** and **outputs** a \"score\".\n",
        "A **positive score** means the mutation is **better** than the wild type from evolution perspective.\n",
        "\n",
        "<font color=\"red\">Our model is pre-trained based on protein structures and performs best when provided with structural data.\n",
        "\n",
        "**For this task, if you only have the protein AA sequence, we strongly recommend using AF2 to predict its structure and then using the SA sequence as input.**\n",
        "\n",
        "<a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uxD_KOF1BI7n"
      },
      "outputs": [],
      "source": [
        "\n",
        "mutation_task = \"Single-site saturation mutagenesis\" #@param [\"Single-site or Multi-site mutagenesis\", \"Single-site saturation mutagenesis\"]\n",
        "\n",
        "# data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "data_type = \"Single SA Sequence\" # @param [\"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list_multiple else \"Single Sequence\"\n",
        "\n",
        "if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "  if mode == \"Single Sequence\":\n",
        "    input_mut = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder='Enter Single Mutation Information here',\n",
        "      # description='SA Sequence:',\n",
        "      disabled=False)\n",
        "    print(Fore.BLUE+\"Mutation:\"+Style.RESET_ALL)\n",
        "    input_mut.layout.width = '500px'\n",
        "    display(input_mut)\n",
        "\n",
        "def mutational_effect_predict(button):\n",
        "  button.disabled = True\n",
        "  button.description = 'Clicked'\n",
        "  button.button_style = ''\n",
        "\n",
        "\n",
        "  #@title 3.2.2: Get your Result\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# DATASET ###################################\n",
        "  ################################################################################\n",
        "  if mode == \"Single Sequence\":\n",
        "    seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  else:\n",
        "    dataset_csv_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# Task Info ####################################\n",
        "  ################################################################################\n",
        "  base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "  # clear_output(wait=True)\n",
        "\n",
        "  print(Fore.BLUE)\n",
        "  print(f\"Mutation task: {mutation_task}\")\n",
        "  print(f\"Mode: {mode}\")\n",
        "  print(f\"Model: {base_model}\")\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"Dataset: {dataset_csv_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"Dataset: {seq}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Style.RESET_ALL)\n",
        "\n",
        "  print(f\"Predicting...\")\n",
        "  timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# load model ###################################\n",
        "  ################################################################################\n",
        "\n",
        "  from saprot.model.saprot.saprot_foldseek_mutation_model import SaprotFoldseekMutationModel\n",
        "\n",
        "  config = {\n",
        "      \"foldseek_path\": None,\n",
        "      \"config_path\": base_model,\n",
        "      \"load_pretrained\": True,\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    zero_shot_model\n",
        "  except Exception:\n",
        "    zero_shot_model = SaprotFoldseekMutationModel(**config)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    zero_shot_model.to(device)\n",
        "\n",
        "  ################################################################################\n",
        "  ########################### Single Sequence ####################################\n",
        "  ################################################################################\n",
        "  if mode == \"Single Sequence\":\n",
        "\n",
        "    if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "      mut = input_mut.value\n",
        "      # validate mut\n",
        "      aa_seq = seq[0::2]\n",
        "      for m in mut.split(':'):\n",
        "        ori_aa = m[0]\n",
        "        pos = int(m[1:-1])\n",
        "        mut_aa = m[-1]\n",
        "        assert aa_seq[pos-1] == ori_aa, f\"The provided mutation information contains an error ({m}): the original amino acid at position {pos} ({ori_aa}) does not match your sequence ({aa_seq[pos-1]}).\"\n",
        "\n",
        "      score = zero_shot_model.predict_mut(seq, mut)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "      print(f\"The score of mutation {mut} is {Fore.BLUE}{score}{Style.RESET_ALL}\")\n",
        "\n",
        "    if mutation_task==\"Single-site saturation mutagenesis\":\n",
        "      timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "      output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "      mut_dicts = []\n",
        "      for pos in tqdm(range(1, int(len(seq) / 2)+1), total=int(len(seq) / 2)+1, leave=False, desc=f\"Predicting\"):\n",
        "        mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "        mut_dicts.append(mut_dict)\n",
        "\n",
        "      mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "      df = pd.DataFrame(mut_list)\n",
        "      df.to_csv(output_path, index=None)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "      # files.download(output_path)\n",
        "      file_download(output_path)\n",
        "      print(f\"\\n{Fore.BLUE}The result has been saved to {output_path} and your local computer.{Style.RESET_ALL}\")\n",
        "\n",
        "  ################################################################################\n",
        "  ########################### Multiple Sequences #################################\n",
        "  ################################################################################\n",
        "  if mode == \"Multiple Sequences\":\n",
        "\n",
        "    dataset_df = read_csv_dataset(dataset_csv_path)\n",
        "    results = []\n",
        "\n",
        "    if mutation_task==\"Single-site or Multi-site mutagenesis\":\n",
        "      for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "       seq = row['sequence']\n",
        "       mut_info = row['mutation']\n",
        "       results.append(zero_shot_model.predict_mut(seq, mut_info).cpu().item())\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "      # result_df = pd.DataFrame()\n",
        "      # result_df['sequence'] = dataset_df['sequence']\n",
        "      # result_df['mutation'] = dataset_df['mutation']\n",
        "      dataset_df['score'] = results\n",
        "\n",
        "      output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}.csv\"\n",
        "      dataset_df.to_csv(output_path, index=None)\n",
        "      file_download(output_path)\n",
        "      print(f\"{Fore.BLUE}The result has been saved to {output_path} and your local computer {Style.RESET_ALL}\")\n",
        "\n",
        "    else:\n",
        "      for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "        seq = row['sequence']\n",
        "        mut_dicts = []\n",
        "        for pos in range(1, int(len(seq) / 2)+1):\n",
        "          mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "          mut_dicts.append(mut_dict)\n",
        "        mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "        result_df = pd.DataFrame(mut_list)\n",
        "        results.append(result_df)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "      zip_files = []\n",
        "      for i in range(len(results)):\n",
        "        output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}_Sequence{i+1}.csv\"\n",
        "        results[i].to_csv(output_path, index=None)\n",
        "        zip_files.append(output_path)\n",
        "\n",
        "      # zip and download zip to local computer\n",
        "      zip_path = OUTPUT_HOME / f\"{timestamp}_{Path(dataset_csv_path).stem}.zip\"\n",
        "      with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "          for file in zip_files:\n",
        "              zipf.write(file, os.path.basename(file))\n",
        "      # files.download(zip_path)\n",
        "      print(f\"{Fore.BLUE}The result has been saved to {zip_path} and your local computer{Style.RESET_ALL}\")\n",
        "      file_download(zip_path)\n",
        "\n",
        "button_mutational_effect_predict = ipywidgets.Button(\n",
        "  description='Mutational Effect Predict',\n",
        "  disabled=False,\n",
        "  button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Apply',\n",
        "  # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "button_mutational_effect_predict.on_click(mutational_effect_predict)\n",
        "button_mutational_effect_predict.layout.width = '300px'\n",
        "display(button_mutational_effect_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlQdqTcBI7n"
      },
      "source": [
        "## **3.3: Inverse Folding Prediction** <a name=\"inverse_folding_prediction\"></a>\n",
        "\n",
        "<!-- Predict the amino acid sequence from protein backbone structure.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The protein backbone structure should be provided in .pdb/.cif file format. -->\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.3:-Inverse-Folding-Prediction)\n",
        "\n",
        "\n",
        "<!-- Predict the residue sequence of a structure-aware sequence with masked amino acids (which could be all masked or partially masked).\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Enter a **SA sequence with masked amino acids** into the `sa_seq` input box.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example,\n",
        "**input** is a SA Sequence with masked amino acids:\n",
        "\n",
        "`#d#v#v#v#p#p#p#p#a#p#a#q#k#k#k#k#w`\n",
        "\n",
        "and the **output** predicted by model is an AA Sequence:\n",
        "\n",
        "`MEELGLPDLPPGGVVVV`.\n",
        "\n",
        "<br> -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DT7M_DU2BI7n"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.1: Upload .pdb/.cif structure file**\n",
        "\n",
        "#@markdown After clicking the run button, an upload button will appear for you to upload your .pdb/.cif structure file.\n",
        "\n",
        "#@markdown <font face=\"Consolas\" size=2 color='gray'>NoteÔºösince you may not know the AA type, you can simply populate your .pdb/.cif file with any random AA. If you want to predit partial positions given some accurate AA information in other positions, just input the accurate AA in these positions and any random AA in unknown positions.</fonte>\n",
        "\n",
        "#@markdown After uploading is finished, the .pdb/.cif structure will be transformed into the corresponding AA Sequence and Structure (3Di) Sequence.\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"\n",
        "# raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "def get_structure_file():\n",
        "  print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "  dropdown_type = ipywidgets.Dropdown(\n",
        "    value=\"PDB\",\n",
        "    options=[\"PDB\", \"AF2\"],\n",
        "    disabled=False)\n",
        "  dropdown_type.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "  display(dropdown_type)\n",
        "\n",
        "  input_chain = ipywidgets.Text(\n",
        "    value=\"A\",\n",
        "    placeholder=f'Enter the name of chain here',\n",
        "    disabled=False)\n",
        "  input_chain.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "  display(input_chain)\n",
        "\n",
        "  print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "  pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "  return pdb_file_path, pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "\n",
        "backbone_path, stem, dropdown_type, input_chain = get_structure_file()\n",
        "raw_data = (stem, dropdown_type, input_chain)\n",
        "\n",
        "sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "aa_seq = sa_seq[0::2]\n",
        "struc_seq = sa_seq[1::2]\n",
        "\n",
        "# masked_sa_seq = ''\n",
        "# for s in sa_seq[1::2]:\n",
        "#   masked_sa_seq += '#' + s\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "print()\n",
        "\n",
        "input_aa_seq = ipywidgets.Text(\n",
        "      value=aa_seq,\n",
        "      placeholder='Enter Amino Acid Sequence here',\n",
        "      disabled=False)\n",
        "print(Fore.BLUE+\"Amino Acid Sequence:\"+Style.RESET_ALL)\n",
        "input_aa_seq.layout.width = '500px'\n",
        "display(input_aa_seq)\n",
        "\n",
        "input_struc_seq = ipywidgets.Text(\n",
        "  value=struc_seq,\n",
        "  placeholder='Enter Structure Sequence here',\n",
        "  disabled=False)\n",
        "print(Fore.BLUE+\"Structure Sequence:\"+Style.RESET_ALL)\n",
        "input_struc_seq.layout.width = '500px'\n",
        "display(input_struc_seq)\n",
        "\n",
        "# print(Fore.RED+\"If you want to mask all amino acids and make prediction, simply clear the 'Amino Acid Sequence' box.\")\n",
        "\n",
        "backbone_name = os.path.basename(backbone_path)\n",
        "show_pdb(backbone_path, color=\"chain\").show()\n",
        "print(f\"Backbone visualization of {backbone_name} ({len(struc_seq)} amino acids)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "baiH-BrBl2Ge"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.2: Predict Amino Acid Sequence**\n",
        "\n",
        "#@markdown You can **mask partial or all amino acids** in the AA sequence with '#' at certain positions, allowing the model to make predictions for those masked amino acids.\n",
        "\n",
        "#@markdown <font color=\"red\">If you want to **mask all amino acids** and make prediction, simply clear the 'masked_aa_seq' box.</font>\n",
        "\n",
        "#@markdown | Original AA Sequence | Masked AA Sequence | Description                                |\n",
        "#@markdown | -------------------- | ------------------ | ------------------------------------------ |\n",
        "#@markdown | MEETMKLATMEDTVEYCL   | ME#T#KL#TMEDTVEYCL | Predict the 3rd, 5th, and 8th amino acids. |\n",
        "#@markdown | MEETMKLATMEDTVEYCL   |                    | Predict all amino acids.                   |\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "# #@markdown Click the run button to get the predicted Amino Acid Sequence\n",
        "masked_aa_seq = \"\" # @param {type:\"string\", placeholder:\"mask the amino acids with `#` and then paste the sequence here\"}\n",
        "method = \"multinomial\" # @param [\"argmax\", \"multinomial\"]\n",
        "num_samples = 10 # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown - `method` refers to the prediction method. It could be either \"argmax\" or \"multinomial\".\n",
        "#@markdown   - `argmax` selects the amino acid with the highest probability.\n",
        "#@markdown   - `multinomial` samples an amino acid from the multinomial distribution.\n",
        "\n",
        "\n",
        "#@markdown - `num_samples` refers to the number of output amino acid sequences.\n",
        "\n",
        "save_name = \"predicted_seq\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### Dataset ########################################\n",
        "################################################################################\n",
        "\n",
        "# masked_aa_seq = input_aa_seq.value\n",
        "if masked_aa_seq.strip() == \"\":\n",
        "  masked_aa_seq = \"#\" * len(input_struc_seq.value)\n",
        "\n",
        "masked_struc_seq = input_struc_seq.value\n",
        "\n",
        "# assert len(masked_aa_seq) == len(masked_struc_seq), f\"Please make sure that the amino acid sequence ({len(masked_aa_seq)}) and the structure sequence ({len(masked_struc_seq)}) have the same length.\"\n",
        "# masked_sa_seq = ''.join(a + b for a, b in zip(masked_aa_seq, masked_struc_seq))\n",
        "\n",
        "\n",
        "# if num_samples == 1:\n",
        "#   method = \"argmax\"\n",
        "# elif num_samples > 1:\n",
        "#   method = \"multinomial\"\n",
        "# else:\n",
        "#   raise BaseException(\"\\\"num_samples\\\" should be an integer greater than or equal to 1.\")\n",
        "\n",
        "################################################################################\n",
        "############################### Model ##########################################\n",
        "################################################################################\n",
        "# base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "base_model = \"westlake-repl/SaProt_650M_AF2_inverse_folding\"\n",
        "\n",
        "config = {\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "from saprot.model.saprot.saprot_if_model import SaProtIFModel\n",
        "try:\n",
        "  saprot_if_model\n",
        "except Exception:\n",
        "  saprot_if_model = SaProtIFModel(**config)\n",
        "  tokenizer = saprot_if_model.tokenizer\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  saprot_if_model.to(device)\n",
        "\n",
        "################################################################################\n",
        "############################### Predict ########################################\n",
        "################################################################################\n",
        "\n",
        "pred_aa_seqs = saprot_if_model.predict(masked_aa_seq, masked_struc_seq, method=method, num_samples=num_samples)\n",
        "\n",
        "clear_output(wait=True)\n",
        "print('='*100)\n",
        "print(Fore.BLUE+\"Outputs:\"+Style.RESET_ALL)\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.fasta\"\n",
        "with open(save_path, \"w\") as w:\n",
        "  for i, aa_seq in enumerate(pred_aa_seqs):\n",
        "    print(aa_seq)\n",
        "    w.write(f\">predicted_seq_{i}\\n{aa_seq}\\n\\n\")\n",
        "\n",
        "file_download(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idDDKW2pl2Gf"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.3: Predict the structure of generated sequence**\n",
        "\n",
        "#@markdown <font color=\"red\">**Warning: Please make sure you have enough RAM to run this cell. (Do not use Colab's T4 or local server with less than 64G RAM).**\n",
        "#@markdown\n",
        "#@markdown Otherwise, it will cause an out-of-memory error, and you will have to restart the notebook. We recommend you connect to a runtime\n",
        "#@markdown with more RAM to run the cell properly.</font>\n",
        "\n",
        "#@markdown Click the run button to predict the structure of generated sequence using ESMFold\n",
        "\n",
        "protein_sequence = \"QQVGSQLDLQEESVEYQIFPTQTHQNDTKNVKERLESILERINSIFIPYSQDYVWQEKELSFMISLGLQQGRPHLMGSTHFGDNIDDEWFLVNLLKQLSQVFPQLTAKISDSDGEFLLIEAADLMPKWLIPEGIENRVFVYNGNLMIIPFLLGRYSLIHYQLSKPSLDQAIDLLRNFPEETRASRDQQKLIHNRINGIIKSFLAGTHKAYCYIPRPIATLLKRKPSLVSHAVETFYYRDPIDVKNCRNMEDFTNAERLRVDVRFTRVLYAQLVSQSFNPPKQMGIEAPDPEDKEFKRELLGMKLTCGFAMMAANLLPSTVDPSLNGWAYLEQFKRFRENVEKGNATAKISEPDDQLELISAVRKFLRYIVEDHIDASILKSLLVVELHRQKQMLPESEEAIRKIKKTLLERWNPGWQMSEEYREKTVGQVENGGDSSCEALKSDSKRADLADLDMGRVQDLSRFIDKESRPLERSKISDLQPEVVMGMEQEEDAAAAVSKVYKGGPYLVPIADLKERPEAVHPKATQVVQGELLLISAEDQESKTSNRRVRFGQHGQSQDQAAPMLVGCDRMTALDSIVPEKEEDKVKKGLGYIHLEKSTNSLLTHAIYKIQGSVSHVAARLADRGIDVTSDNVPIKPQTMEEG\" # @param {type:\"string\"}\n",
        "save_name = \"predicted_structure\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Visualization settings\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### LOAD ESMFOLD ################################\n",
        "################################################################################\n",
        "try:\n",
        "  esmfold\n",
        "except Exception:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold.esm = esmfold.esm.half()\n",
        "  esmfold.trunk.set_chunk_size(64)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  esmfold.to(device)\n",
        "\n",
        "################################################################################\n",
        "################################## PREDICT ###################################\n",
        "################################################################################\n",
        "tokenized_input = tokenizer(\n",
        "    [protein_sequence],\n",
        "    return_tensors=\"pt\",\n",
        "    add_special_tokens=False,\n",
        "    )['input_ids']\n",
        "\n",
        "tokenized_input = tokenized_input.to(esmfold.device)\n",
        "with torch.no_grad():\n",
        "  output = esmfold(tokenized_input)\n",
        "\n",
        "################################################################################\n",
        "#################################### SAVE ####################################\n",
        "################################################################################\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.pdb\"\n",
        "pdb = convert_outputs_to_pdb(output)\n",
        "with open(save_path, \"w\") as f:\n",
        "  f.write(\"\".join(pdb))\n",
        "\n",
        "################################################################################\n",
        "################################# VISUALIZE ##################################\n",
        "################################################################################\n",
        "show_pdb(save_path, show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\":\n",
        "  plot_plddt_legend().show()\n",
        "\n",
        "print(\"Predicted structure\")\n",
        "file_download(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PecOgaEEZw4C"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.4: Align proteins using TMalign**\n",
        "\n",
        "#@markdown You can find the **uploaded proteins** from /content/SaprotHub/structures (if you connect to your local server, then the path is /SaprotHub/structures).\n",
        "\n",
        "#@markdown You can find the **predicted proteins** from /content/SaprotHub/output (if you connect to your local server, then the path is /SaprotHub/output).\n",
        "\n",
        "#@markdown Right click the **pdb file** to copy the path and then paste it into the box:\n",
        "pdb_path_1 = \"/SaprotHub/structures/1qsf.pdb\" # @param {type:\"string\"}\n",
        "pdb_path_2 = \"/SaprotHub/output/predicted_structure.pdb\" # @param {type:\"string\"}\n",
        "\n",
        "pdb_path_1 = f\"{root_dir}{pdb_path_1}\"\n",
        "pdb_path_2 = f\"{root_dir}{pdb_path_2}\"\n",
        "\n",
        "assert os.path.exists(pdb_path_1) and os.path.exists(pdb_path_2), \"Input proteins do not exist!\"\n",
        "\n",
        "cmd = f\"{root_dir}/SaprotHub/bin/TMalign {pdb_path_1} {pdb_path_2}\"\n",
        "print(os.popen(cmd).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8i9p_Ff4232"
      },
      "source": [
        "## **3.4 Extract Protein Embedding**\n",
        "\n",
        "The shape of extracted embedding is `[N, D]`, where `N` is the number of sequences and `D` is the hidden dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aA7QozqDh8Nj"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "########################## MODEL ###############################################\n",
        "################################################################################\n",
        "use_model_from = \"Official pretrained SaProt (35M)\" # @param [\"Official pretrained SaProt (35M)\",\"Official pretrained SaProt (650M)\",\"Trained by yourself on ColabSaprot\",\"Shared by peers on SaprotHub\",\"Saved in your local computer\"]\n",
        "if use_model_from == \"Multi-models on SaprotHub\":\n",
        "  multi_lora = True\n",
        "else:\n",
        "  multi_lora = False\n",
        "\n",
        "adapter_input = select_adapter_from(None, use_model_from)\n",
        "\n",
        "################################################################################\n",
        "########################## DATASET #############################################\n",
        "################################################################################\n",
        "data_type = \"Multiple SA Sequences\" # @param [\"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "# check_task_type_and_data_type(original_task_type, data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if (data_type in data_type_list_multiple) else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "################################################################################\n",
        "########################## EXTRACT #############################################\n",
        "################################################################################\n",
        "def extract(button):\n",
        "  button.disabled = True\n",
        "  button.description = 'Extracting...'\n",
        "  button.button_style = ''\n",
        "\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# MODEL ###################################\n",
        "  ##############################################################################\n",
        "  if multi_lora:\n",
        "    if use_model_from == \"Multi-models on ColabSaprot\":\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / lora_config_path}) for lora_config_path in list(adapter_input.value)]\n",
        "    elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "      #1. get adapter_list\n",
        "      repo_id_list = adapter_input.value.replace(\" \", \"\").split(',')\n",
        "      #2. download adapters\n",
        "      for repo_id in repo_id_list:\n",
        "        snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / repo_id)\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "    assert len(config_list) > 0, \"Please select your models from the dropdown menu on the output of 3.1!\"\n",
        "    base_model = get_base_model(ADAPTER_HOME / task_type / config_list[0].lora_config_path)\n",
        "\n",
        "    required_training_data_type_list = []\n",
        "    for lora_config in config_list:\n",
        "      required_training_data_type_list.append(check_training_data_type(lora_config.lora_config_path, data_type))\n",
        "    assert len(set(required_training_data_type_list)) == 1, f\"Error: The input data types of these models are not identical: {required_training_data_type_list}\"\n",
        "    required_training_data_type = required_training_data_type_list[0]\n",
        "\n",
        "    lora_kwargs = EasyDict({\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": len(config_list),\n",
        "      \"config_list\": config_list\n",
        "    })\n",
        "\n",
        "  elif use_model_from == \"Official pretrained SaProt (35M)\":\n",
        "    base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    lora_kwargs = None\n",
        "\n",
        "  elif use_model_from == \"Official pretrained SaProt (650M)\":\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    lora_kwargs = None\n",
        "\n",
        "  else:\n",
        "    adapter_path = ADAPTER_HOME / adapter_input.value\n",
        "\n",
        "    if use_model_from == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    required_training_data_type = check_training_data_type(adapter_path, data_type)\n",
        "    lora_kwargs = {\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# DATASET ###################################\n",
        "  ##############################################################################\n",
        "  if data_type in data_type_list_multiple:\n",
        "    csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    df = read_csv_dataset(csv_dataset_path)\n",
        "  else:\n",
        "    single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    # if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    #   df = pd.DataFrame({\n",
        "    #       'sequence_1': [single_sa_seq[0]],\n",
        "    #       'sequence_2': [single_sa_seq[1]]\n",
        "    #   })\n",
        "    # else:\n",
        "    df = pd.DataFrame({\n",
        "        'sequence': [single_sa_seq]\n",
        "    })\n",
        "\n",
        "  # if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "  #   if 'sequence' in df.columns:\n",
        "  #     df['sequence'] = df['sequence'].apply(mask_struc_token)\n",
        "  #   elif 'sequence_1' in df.columns and 'sequence_2' in df.columns:\n",
        "  #     df['sequence_1'] = df['sequence_1'].apply(mask_struc_token)\n",
        "  #     df['sequence_2'] = df['sequence_2'].apply(mask_struc_token)\n",
        "\n",
        "  ################################################################################\n",
        "  ##################################### CONFIG ###################################\n",
        "  ################################################################################\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "\n",
        "  # task\n",
        "  if use_model_from in [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\"]:\n",
        "    num_labels, task_type = 1, 'classification'\n",
        "  else:\n",
        "    num_labels, task_type = get_num_labels_and_task_type_by_adapter(lora_kwargs[\"config_list\"][0][\"lora_config_path\"])\n",
        "\n",
        "  config.model.kwargs.num_labels = num_labels\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### LOAD MODEL ##################################\n",
        "  ################################################################################\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFO #######################################\n",
        "  ################################################################################\n",
        "  # clear_output(wait=True)\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  # print(Fore.BLUE+f\"Task Type: {original_task_type}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f\"Model ({use_model_from}):\"+Style.RESET_ALL)\n",
        "  if use_model_from in [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\"]:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "  elif multi_lora:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter:\"+Style.RESET_ALL)\n",
        "    for lora_config in lora_kwargs.config_list:\n",
        "      print(Fore.BLUE+f\"    {lora_config.lora_config_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter: {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f'Dataset ({data_type}):' +Style.RESET_ALL)\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"  CSV Dataset Path: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    # if \"A pair of\" in data_type:\n",
        "    #   print(Fore.BLUE+f\"  Sequence 1: {single_sa_seq[0]}\"+Style.RESET_ALL)\n",
        "    #   print(Fore.BLUE+f\"  Sequence 2: {single_sa_seq[1]}\"+Style.RESET_ALL)\n",
        "    # else:\n",
        "    print(Fore.BLUE+f\"  Sequence: {single_sa_seq}\"+Style.RESET_ALL)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFERENCE ##################################\n",
        "  ################################################################################\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+\"Predicting...\"+Style.RESET_ALL)\n",
        "\n",
        "  seqs = df['sequence']\n",
        "  embedding_list = []\n",
        "  with torch.no_grad():\n",
        "    for seq in tqdm(seqs, total=len(seqs)):\n",
        "      embedding = model.get_hidden_states_from_seqs([seq], reduction='mean')\n",
        "      embedding_list.append(embedding[0])\n",
        "  embeddings = torch.stack(embedding_list)\n",
        "  # print(embeddings.shape)\n",
        "\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+\"Prediction Result:\"+Style.RESET_ALL)\n",
        "\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  embeddings_path = OUTPUT_HOME / f'embeddings_{timestamp}.pt'\n",
        "  torch.save(embeddings, embeddings_path)\n",
        "  print(Fore.BLUE+f\"The extracted embeddings is saved to {embeddings_path}.\"+Style.RESET_ALL)\n",
        "  file_download(embeddings_path)\n",
        "\n",
        "################################################################################\n",
        "#################################### BUTTON #################################\n",
        "################################################################################\n",
        "button_extract = ipywidgets.Button(\n",
        "    description='Extract Protein Embeddings',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_extract.on_click(extract)\n",
        "button_extract.layout.width = '500px'\n",
        "display(button_extract)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIIoTQgJBI7o"
      },
      "source": [
        "# **4: (Optional) Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BgsBSLcmBI7o"
      },
      "outputs": [],
      "source": [
        "# @title **4.1: Get Structure-Aware Sequence** <a name=\"get_sa\"></a>\n",
        "\n",
        "#@markdown AA Sequence, UniProt ID, PDB/CIF file -> SA Sequence\n",
        "\n",
        "################################################################################\n",
        "################################ input #########################################\n",
        "################################################################################\n",
        "\n",
        "data_type = \"Multiple PDB/CIF Structures\"  # @param [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "################################################################################\n",
        "############################### output #########################################\n",
        "################################################################################\n",
        "\n",
        "if data_type in [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\"]:\n",
        "    def apply(button):\n",
        "        button.disabled = True\n",
        "        button.description = 'Clicked'\n",
        "        button.button_style = ''\n",
        "        sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "        print(\"=\"*100)\n",
        "        print(f\"Amino Acid Sequence: {sa_seq[0::2]}\")\n",
        "        print(f\"Structure Sequence: {sa_seq[1::2]}\")\n",
        "        print(\"=\"*100)\n",
        "        print(\"Please note that structure tokens with a plDDT score lower than 70% are denoted as \\\"#\\\"\")\n",
        "        print(Fore.BLUE + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + Style.RESET_ALL)\n",
        "        print(sa_seq)\n",
        "\n",
        "    button_apply = ipywidgets.Button(\n",
        "        description='Apply',\n",
        "        disabled=False,\n",
        "        button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Apply',\n",
        "        icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "    button_apply.on_click(apply)\n",
        "    button_apply.layout.width = '500px'\n",
        "    display(button_apply)\n",
        "else:\n",
        "    csv_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    print(Fore.BLUE + \"\\n\\nThe Structure-Aware Sequences are saved in a .csv file here:\" + Style.RESET_ALL)\n",
        "    print(csv_dataset)\n",
        "    file_download(csv_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDkm_OeABI7o"
      },
      "outputs": [],
      "source": [
        "#@title **4.2: Convert `.fa/.fasta` file to `.csv` file in the data format of \"Multiple AA Sequences\"**\n",
        "\n",
        "\n",
        "#@markdown `.fa/.fasta` -> Multiple AA Sequences `.csv` <a name=\"fa2csv\"></a>\n",
        "\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "aa_seq_dict = { 'sequence': [],\n",
        "                # \"label\": [],\n",
        "                # \"stage\":[]\n",
        "                }\n",
        "\n",
        "fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "assert Path(fa_file_path).name.split('.')[1] in ['fa', 'fasta'], \"Please upload a .fa or .fasta file.\"\n",
        "with fa_file_path.open(\"r\") as fa:\n",
        "  for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "      aa_seq_dict['sequence'].append(str(record.seq))\n",
        "\n",
        "fa_df = pd.DataFrame(aa_seq_dict)\n",
        "print(fa_df[5:])\n",
        "\n",
        "csv_file_path = UPLOAD_FILE_HOME / f'{fa_file_path.stem}.csv'\n",
        "fa_df.to_csv(csv_file_path, index=None)\n",
        "# files.download(csv_file_path)\n",
        "file_download(csv_file_path)\n",
        "\n",
        "################################################################################\n",
        "############################ .fa 2 .csv and split ##############################\n",
        "################################################################################\n",
        "\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# split = ['train', 'valid', 'test']\n",
        "\n",
        "# aa_seq_dict = { 'sequence': [],\n",
        "#                 \"label\": [],\n",
        "#                 \"stage\":[]}\n",
        "\n",
        "\n",
        "\n",
        "# if automatically_split_dataset:\n",
        "\n",
        "#   fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   label = fa_file_path.stem\n",
        "\n",
        "#   with fa_file_path.open(\"r\") as fa:\n",
        "#       for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "#           aa_seq_dict['sequence'].append(str(record.seq))\n",
        "#           aa_seq_dict[\"label\"].append(label)\n",
        "#   weights = [0.8, 0.1, 0.1]\n",
        "#   aa_seq_dict[\"stage\"] = np.random.choice(split, size=len(aa_seq_dict['sequence']), p=weights).tolist()\n",
        "\n",
        "# else:\n",
        "#   for i in range(3):\n",
        "#     print(Fore.BLUE+f\"Please upload a .fa file as your {split[i]} dataset\")\n",
        "#     fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#     label = fa_file_path.stem\n",
        "\n",
        "#     with fa_file_path.open(\"r\") as fa:\n",
        "#         for record in tqdm(SeqIO.parse(fa, 'fasta')):\n",
        "#             aa_seq_dict['sequence'].append(str(record.seq))\n",
        "#             aa_seq_dict[\"label\"].append(label)\n",
        "#             aa_seq_dict[\"stage\"].append(split[i])\n",
        "\n",
        "#     print()\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "# fa_df = pd.DataFrame(aa_seq_dict)\n",
        "# timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "# fa_df.to_csv(f'/content/SaprotHub/upload_files/{timestamp}.csv', index=None)\n",
        "# files.download(f'/content/SaprotHub/upload_files/{timestamp}.csv')\n",
        "# print(fa_df[5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xtadHW9vBI7o"
      },
      "outputs": [],
      "source": [
        "#@title **4.3: Dataset Split** <a name=\"split_dataset\"></a>\n",
        "\n",
        "#@markdown Randomly split your .csv dataset by adding a `stage` column, assigning the values `train`, `valid`, and `test` according to the specified split ratio (default is `0.8:0.1:0.1`). Ensure that the sum of the ratios equals 1 and that each ratio is greater than 0.\n",
        "\n",
        "#@markdown Your .csv dataset should contain `sequence` and `label` columns, making it suitable for training after splitting. For the specific format, please refer to [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model#dataset-format).\n",
        "\n",
        "# @markdown Please click the run button to upload your .csv dataset\n",
        "\n",
        "split = ['train', 'valid', 'test']\n",
        "# split_ratio = [0.8, 0.1, 0.1]\n",
        "split_ratio = 0.8, 0.1, 0.1 # @param {\"type\":\"raw\",\"placeholder\":\"0.8, 0.1, 0.1\"}\n",
        "\n",
        "if any(w == 0 or w == 1 for w in split_ratio):\n",
        "    raise ValueError(\"One or more proportions for train, valid, and test are either 0 or 1. Please ensure all values are between 0 and 1.\")\n",
        "elif sum(split_ratio) != 1:\n",
        "    raise ValueError(\"The sum of the proportions for train, valid, and test is not equal to 1. Please check the values.\")\n",
        "else:\n",
        "    print(f\"The split ratio for train, valid, and test is {split_ratio[0]}:{split_ratio[1]}:{split_ratio[2]}.\")\n",
        "\n",
        "print('='*100)\n",
        "print(\"Upload your .csv dataset:\")\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = read_csv_dataset(csv_dataset_path)\n",
        "\n",
        "while ('stage' not in dataset_df.columns) or (dataset_df[\"stage\"].nunique()<3):\n",
        "  dataset_df[\"stage\"] = np.random.choice(split, size=len(dataset_df), p=split_ratio).tolist()\n",
        "\n",
        "dataset_df.to_csv(csv_dataset_path, index=None)\n",
        "file_download(csv_dataset_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}