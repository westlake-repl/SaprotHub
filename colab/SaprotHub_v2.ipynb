{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdjG4-XsE0I"
      },
      "source": [
        "# **SaprotHub: Making Protein Modeling Accessible to All Biologists**\n",
        "\n",
        "<a href=\"https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3\"><img src=\"https://img.shields.io/badge/Paper-bioRxiv-green\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/SaProtHub\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-red?label=SaprotHub\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://github.com/westlake-repl/SaprotHub\"><img src=\"https://img.shields.io/badge/Github-black?logo=github\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://theopmc.github.io/\"><img src=\"https://img.shields.io/badge/Website-OPMC-yellow\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://cbirt.net/no-coding-required-saprothub-brings-protein-modeling-to-every-biologist/\" alt=\"blog\"><img src=\"https://img.shields.io/badge/Blog-Medium-purple\" /></a>\n",
        "<a href=\"https://x.com/sokrypton/status/1795525127653986415\"><img src=\"https://img.shields.io/badge/Twitter-blue?logo=twitter\" style=\"max-width: 100%;\"></a>\n",
        "\n",
        "\n",
        "This is **ColabSaprot**, the Colab version of [SaProt](https://github.com/westlake-repl/SaProt), a pre-trained protein language model designed for various downstream protein tasks.\n",
        "\n",
        "**ColabSaprot** is a platform where **Protein Language Models(PLMs)** are more accessible and user-friendly for biologists, enabling effortless model training and sharing within the scientific community.\n",
        "\n",
        "We've established the **SaprotHub**([website](https://huggingface.co/SaProtHub), [paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3)) for storing and sharing models and datasets, where you can explore extensive collections for specific protein prediction tasks.\n",
        "\n",
        "We hope ColabSaprot and SaprotHub can contribute to advancing biological research, fostering collaboration, and accelerating discoveries in the field. You can access [our paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3) for further details.\n",
        "\n",
        "Check these videos ([training](https://www.youtube.com/watch?v=r42z1hvYKfw), [predicting](https://www.youtube.com/watch?v=N5VMBwM_ukQ)) to see how to use  ColabSaprot.\n",
        "\n",
        "Joining [**OPMC**](https://theopmc.github.io/) as an author of SaprotHub.\n",
        "\n",
        "ColabSaprot supports hundreds of [protein prediction tasks](https://github.com/westlake-repl/SaProtHub/blob/main/task_list.md).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLb_im9sJWw"
      },
      "source": [
        "## ColabSaprot\n",
        "\n",
        "| Function                             | Tutorial                                                     | Video                                                        |\n",
        "| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| <a href=\"#train\">Train your model</a>                     | [How to train your model](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model) | - [YouTube](https://www.youtube.com/watch?v=r42z1hvYKfw)<br />- [Bilibili](https://www.bilibili.com/video/BV1HDhHeTEmH/?spm_id_from=333.337.search-card.all.click&vd_source=a418185fadee73ac65d8fab69eee0b52) |\n",
        "| <a href=\"#prediction\">Classification/Regression Prediction</a> | [How to use model for classification/regression prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.1:-Classification-Regression-Prediction) | - [YouTube](https://www.youtube.com/watch?v=N5VMBwM_ukQ)      |\n",
        "| <a href=\"#mutational_effect_prediction\">Mutational Effect Prediction</a>         | [How to use model for mutational effect prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.2:-Mutational-Effect-Prediction) | -                                                            |\n",
        "| <a href=\"#inverse_folding_prediction\">Inverse Folding Prediction</a>           | [How to use model for inverse folding prediction](https://github.com/westlake-repl/SaprotHub/wiki/3.3:-Inverse-Folding-Prediction) | -                                                            |\n",
        "| <a href=\"#upload_model\">Contribute to SaprotHub</a>              | [How to contribute to SaprotHub](https://github.com/westlake-repl/SaprotHub/wiki/0.4:-Contribute-to-SaprotHub) | -                                                            |\n",
        "\n",
        "<br>\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQ6vaQTjYO3"
      },
      "source": [
        "## SaprotHub\n",
        "\n",
        "Find awesome models and datasets for specific protein task on SaprotHub!\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SaProtHub.png?raw=true\" height=\"350\" width=\"600px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dw1U1uBI7d"
      },
      "source": [
        "# **1: Installation**\n",
        "\n",
        "‚ö†Ô∏è**IMPORTANT**‚ö†Ô∏è\n",
        "\n",
        "Before installing SaProt, please **<font color=red>SWITCH YOUR RUNTIME TYPE TO GPU!!!</font>**\n",
        "\n",
        "> üìçPlease check this [page](https://github.com/westlake-repl/SaprotHub/wiki/1.1:-Switch-your-runtime-type-to-GPU) to learn **how to switch your runtime type to GPU**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tgvb8ibwBI7d"
      },
      "outputs": [],
      "source": [
        "#@title **1.1: ‚ñ∂Ô∏è Click the run button to install SaProt**\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "# Check whether the server is local or from google cloud\n",
        "root_dir = os.getcwd()\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "try:\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "  import saprot\n",
        "  print(\"SaProt is installed successfully!\")\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "\n",
        "except ImportError:\n",
        "  print(\"Installing SaProt...\")\n",
        "  os.system(f\"rm -rf {root_dir}/SaprotHub\")\n",
        "  # !rm -rf /content/SaprotHub/\n",
        "\n",
        "  !git clone https://github.com/westlake-repl/SaprotHub.git\n",
        "\n",
        "  # !pip install /content/SaprotHub/saprot-0.4.7-py3-none-any.whl\n",
        "  os.system(f\"pip install -r {root_dir}/SaprotHub/requirements.txt\")\n",
        "  # !pip install -r /content/SaprotHub/requirements.txt\n",
        "\n",
        "  os.system(f\"pip install {root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/LMDB\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/bin\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/output\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/datasets\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/token_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/structures\")\n",
        "  # !mkdir -p /content/SaprotHub/LMDB\n",
        "  # !mkdir -p /content/SaprotHub/bin\n",
        "  # !mkdir -p /content/SaprotHub/output\n",
        "  # !mkdir -p /content/SaprotHub/datasets\n",
        "  # !mkdir -p /content/SaprotHub/adapters/classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/token_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/structures\n",
        "\n",
        "  # !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  # os.system(\n",
        "  #   f\"wget 'https://drive.usercontent.google.com/download?id=1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re&export=download&authuser=0' -O {root_dir}/SaprotHub/bin/foldseek\"\n",
        "  # )\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "  # !chmod +x /content/SaprotHub/bin/foldseek\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "\n",
        "  # !mv /content/SaprotHub/ColabSaprotSetup/foldseek /content/SaprotHub/bin/\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import lmdb\n",
        "import base64\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import yaml\n",
        "import argparse\n",
        "import pprint\n",
        "import subprocess\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from loguru import logger\n",
        "from easydict import EasyDict\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from huggingface_hub import snapshot_download\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from transformers import AutoTokenizer, EsmForProteinFolding\n",
        "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "from saprot.data.parse import get_chain_ids\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DATASET_HOME = Path(f'{root_dir}/SaprotHub/datasets')\n",
        "ADAPTER_HOME = Path(f'{root_dir}/SaprotHub/adapters')\n",
        "STRUCTURE_HOME = Path(f\"{root_dir}/SaprotHub/structures\")\n",
        "LMDB_HOME = Path(f'{root_dir}/SaprotHub/LMDB')\n",
        "OUTPUT_HOME = Path(f'{root_dir}/SaprotHub/output')\n",
        "UPLOAD_FILE_HOME = Path(f'{root_dir}/SaprotHub/upload_files')\n",
        "FOLDSEEK_PATH = Path(f\"{root_dir}/SaprotHub/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"SaprotHub Dataset\",\n",
        "                  \"A pair of AA Sequences\",\n",
        "                  \"A pair of SA Sequences\",\n",
        "                  \"A pair of UniProt IDs\",\n",
        "                  \"A pair of PDB/CIF Structures\",\n",
        "                  \"Multiple pairs of AA Sequences\",\n",
        "                  \"Multiple pairs of SA Sequences\",\n",
        "                  \"Multiple pairs of UniProt IDs\",\n",
        "                  \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Protein-level Classification\": \"classification\",\n",
        "  \"Residue-level Classification\" : \"token_classification\",\n",
        "  \"Protein-level Regression\" : \"regression\",\n",
        "  \"Protein-protein Classification\": \"pair_classification\",\n",
        "  \"Protein-protein Regression\": \"pair_regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_model\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_dataset\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_dataset\",\n",
        "}\n",
        "training_data_type_dict = {\n",
        "  \"Single AA Sequence\": \"AA\",\n",
        "  \"Single SA Sequence\": \"SA\",\n",
        "  \"Single UniProt ID\": \"SA\",\n",
        "  \"Single PDB/CIF Structure\": \"SA\",\n",
        "  \"Multiple AA Sequences\": \"AA\",\n",
        "  \"Multiple SA Sequences\": \"SA\",\n",
        "  \"Multiple UniProt IDs\": \"SA\",\n",
        "  \"Multiple PDB/CIF Structures\": \"SA\",\n",
        "  \"SaprotHub Dataset\": \"SA\",\n",
        "  \"A pair of AA Sequences\": \"AA\",\n",
        "  \"A pair of SA Sequences\": \"SA\",\n",
        "  \"A pair of UniProt IDs\": \"SA\",\n",
        "  \"A pair of PDB/CIF Structures\": \"SA\",\n",
        "  \"Multiple pairs of AA Sequences\": \"AA\",\n",
        "  \"Multiple pairs of SA Sequences\": \"SA\",\n",
        "  \"Multiple pairs of UniProt IDs\": \"SA\",\n",
        "  \"Multiple pairs of PDB/CIF Structures\": \"SA\",\n",
        "}\n",
        "\n",
        "\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### adapters #######################################\n",
        "################################################################################\n",
        "def get_adapters_list(task_type=None):\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    if task_type:\n",
        "      for file_path in (ADAPTER_HOME / task_type).glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "    else:\n",
        "      for file_path in ADAPTER_HOME.glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "\n",
        "def show_adapters_info(adapters_list):\n",
        "  grid = widgets.Grid(len(adapters_list)+1, 2, header_row=True, header_column=True)\n",
        "\n",
        "  with grid.output_to(0, 0):\n",
        "    print(\"ID\")\n",
        "\n",
        "  with grid.output_to(0, 1):\n",
        "    print(\"Local Model\")\n",
        "\n",
        "  # with grid.output_to(0, 2):\n",
        "  #   print(\"Adapter Path\")\n",
        "\n",
        "  for i in range(len(adapters_list)):\n",
        "    with grid.output_to(i+1, 0):\n",
        "      print(i)\n",
        "    with grid.output_to(i+1, 1):\n",
        "      print(adapters_list[i].stem)\n",
        "    # with grid.output_to(i+1, 2):\n",
        "    #   print(adapters_list[i])\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "  display(dropdown)\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def select_adapter():\n",
        "  adapters_list = get_adapters_list()\n",
        "  print(Fore.BLUE+\"Existing Models:\"+Style.RESET_ALL)\n",
        "  # print(\"=\"*100)\n",
        "  # show_adapters_info(adapters_list)\n",
        "  # print(\"=\"*100)\n",
        "  return adapters_combobox(adapters_list)\n",
        "\n",
        "def adapters_selectmultiple(adapters_list):\n",
        "  selectmulitiple = ipywidgets.SelectMultiple(\n",
        "  options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "  value=[],\n",
        "  #rows=10,\n",
        "  placeholder='Select multiple models',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(selectmulitiple)\n",
        "\n",
        "  return selectmulitiple\n",
        "\n",
        "def adapters_textmultiple(adapters_list):\n",
        "  textmultiple = ipywidgets.Text(\n",
        "  value=None,\n",
        "  placeholder='Enter multiple SaprotHub Model IDs, separated by commas.',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(textmultiple)\n",
        "\n",
        "  return textmultiple\n",
        "\n",
        "\n",
        "def select_adapter_from(task_type, use_model_from):\n",
        "  adapters_list = get_adapters_list(task_type)\n",
        "\n",
        "  if use_model_from == 'Trained by yourself on ColabSaprot':\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_dropdown(adapters_list)\n",
        "\n",
        "  elif use_model_from == 'Shared by peers on SaprotHub':\n",
        "    print(Fore.BLUE+\"SaprotHub Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Saved in your local computer\":\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "\n",
        "    return EasyDict({\"value\":  f\"Local/{adapter_zip_path.stem}\"})\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ColabSaprot\":\n",
        "    # 1. select the list of adapters\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Multiple values can be selected with \\\"shift\\\" and/or \\\"ctrl\\\" (or \\\"command\\\") pressed and mouse clicks or arrow keys.\"+Style.RESET_ALL)\n",
        "    return adapters_selectmultiple(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "    # 1. enter the list of adapters\n",
        "    print(Fore.BLUE+f\"SaprotHub Model IDs, separated by commas ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_textmultiple(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  import shutil\n",
        "  import os\n",
        "  from pathlib import Path\n",
        "  import sys\n",
        "\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "  print(Fore.BLUE+\"Dataset: \"+Style.RESET_ALL, end='')\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == data_type_list[3]:\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type = ipywidgets.Dropdown(\n",
        "      value=\"AF2\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type)\n",
        "\n",
        "    input_chain = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain here',\n",
        "      disabled=False)\n",
        "    input_chain.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "    display(input_chain)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    return pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "  # 4-7 & 13-16. Multiple Sequences\n",
        "  elif data_type in data_type_list[4:8] or data_type in data_type_list[13:17]:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type == data_type_list[7] or data_type == data_type_list[16]:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == data_type_list[8]:\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Copy and paste the SaprotHub Dataset ID here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "  # 9-11. A pair of seq\n",
        "  elif data_type in [\"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\"]:\n",
        "    print()\n",
        "\n",
        "    seq_type = data_type[len(\"A pair of \"):-1]\n",
        "\n",
        "    input_seq1 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 1 here',\n",
        "      disabled=False)\n",
        "    input_seq1.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 1:\"+Style.RESET_ALL)\n",
        "    display(input_seq1)\n",
        "\n",
        "    input_seq2 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 2 here',\n",
        "      disabled=False)\n",
        "    input_seq2.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 2:\"+Style.RESET_ALL)\n",
        "    display(input_seq2)\n",
        "\n",
        "    return (input_seq1, input_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  elif data_type == data_type_list[12]:\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type1 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The first structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type1)\n",
        "\n",
        "    input_chain1 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the first structure here',\n",
        "      disabled=False)\n",
        "    input_chain1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the first structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain1)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path1 = upload_file(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    dropdown_type2 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The second structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type2)\n",
        "\n",
        "    input_chain2 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the second structure here',\n",
        "      disabled=False)\n",
        "    input_chain2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the second structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain2)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path2 = upload_file(STRUCTURE_HOME)\n",
        "    return (pdb_file_path1.stem, dropdown_type1, input_chain1, pdb_file_path2.stem, dropdown_type2, input_chain2)\n",
        "\n",
        "\n",
        "  # elif data_type == \"Multiple pairs of PDB/CIF Structures\":\n",
        "  #   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "  #   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "  #   print(\"=\"*100)\n",
        "\n",
        "  #   if data_type == data_type_list[7]:\n",
        "  #     # upload and unzip PDB files\n",
        "  #     print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "  #     pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #     if pdb_zip_path.suffix != \".zip\":\n",
        "  #       logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "  #       raise RuntimeError(\"The data type does not match.\")\n",
        "  #     print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "  #     print(\"=\"*100)\n",
        "\n",
        "  #     import zipfile\n",
        "  #     with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "  #       zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "  #   return uploaded_csv_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # 0. Single AA Sequence\n",
        "  if data_type == data_type_list[0]:\n",
        "    input_seq = raw_data\n",
        "    aa_seq = input_seq.value\n",
        "\n",
        "    sa_seq = ''\n",
        "    for aa in aa_seq:\n",
        "        sa_seq += aa + '#'\n",
        "    return sa_seq\n",
        "\n",
        "  # 1. Single SA Sequence\n",
        "  if data_type == data_type_list[1]:\n",
        "    input_seq = raw_data\n",
        "    sa_seq = input_seq.value\n",
        "\n",
        "    return sa_seq\n",
        "\n",
        "  # 2. Single UniProt ID\n",
        "  if data_type == data_type_list[2]:\n",
        "    input_seq = raw_data\n",
        "    uniprot_id = input_seq.value\n",
        "\n",
        "\n",
        "    protein_list = [(uniprot_id, \"AF2\", \"A\")]\n",
        "    uniprot2pdb([protein_list[0][0]])\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  if data_type == data_type_list[3]:\n",
        "    uniprot_id = raw_data[0]\n",
        "    struc_type = raw_data[1].value\n",
        "    chain = raw_data[2].value\n",
        "\n",
        "    protein_list = [(uniprot_id, struc_type, chain)]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # Multiple sequences\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "  if data_type in data_type_list[4:8] or data_type in data_type_list[13:17]:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "\n",
        "  # 4. Multiple AA Sequences\n",
        "  if data_type == data_type_list[4]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    for index, value in protein_df['sequence'].items():\n",
        "      sa_seq = ''\n",
        "      for aa in value:\n",
        "        sa_seq += aa + '#'\n",
        "      protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 5. Multiple SA Sequences\n",
        "  if data_type == data_type_list[5]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 6. Multiple UniProt IDs\n",
        "  if data_type == data_type_list[6]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_list = protein_df.iloc[:, 0].tolist()\n",
        "    uniprot2pdb(protein_list)\n",
        "    protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 7. Multiple PDB/CIF Structures\n",
        "  if data_type == data_type_list[7]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    # protein_list = [(uniprot_id, type, chain), ...]\n",
        "    # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "    # uniprot2pdb(protein_list)\n",
        "    protein_list = []\n",
        "    for row_tuple in protein_df.itertuples(index=False):\n",
        "      assert row_tuple.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "      protein_list.append(row_tuple)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == data_type_list[8]:\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=LMDB_HOME/REPO_ID)\n",
        "\n",
        "    return LMDB_HOME/REPO_ID\n",
        "\n",
        "  # 9. Pair Single AA Sequences\n",
        "  elif data_type == \"A pair of AA Sequences\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[0], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[0], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 10. Pair Single SA Sequences\n",
        "  elif data_type ==  \"A pair of SA Sequences\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[1], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[1], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 11. Pair Single UniProt IDs\n",
        "  elif data_type ==  \"A pair of UniProt IDs\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[2], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[2], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  if data_type == \"A pair of PDB/CIF Structures\":\n",
        "    uniprot_id1 = raw_data[0]\n",
        "    struc_type1 = raw_data[1].value\n",
        "    chain1 = raw_data[2].value\n",
        "\n",
        "    protein_list1 = [(uniprot_id1, struc_type1, chain1)]\n",
        "    mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs1 = mprs1.run()\n",
        "    sa_seq1 = seqs1[0].split('\\t')[1]\n",
        "\n",
        "    uniprot_id2 = raw_data[3]\n",
        "    struc_type2 = raw_data[4].value\n",
        "    chain2 = raw_data[5].value\n",
        "\n",
        "    protein_list2 = [(uniprot_id2, struc_type2, chain2)]\n",
        "    mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs2 = mprs2.run()\n",
        "    sa_seq2 = seqs2[0].split('\\t')[1]\n",
        "    return sa_seq1, sa_seq2\n",
        "\n",
        "  # # Pair raw_data = upload_files/xxx.csv\n",
        "  # if data_type in data_type_list[12:16]:\n",
        "  #   uploaded_csv_path = raw_data\n",
        "  #   csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "\n",
        "  # 13. Pair Multiple AA Sequences\n",
        "  if data_type == \"Multiple pairs of AA Sequences\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    for index, value in protein_df['sequence_1'].items():\n",
        "      sa_seq1 = ''\n",
        "      for aa in value:\n",
        "        sa_seq1 += aa + '#'\n",
        "      protein_df.at[index, 'sequence_1'] = sa_seq1\n",
        "\n",
        "    protein_df['name_1'] = 'name_1'\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "    for index, value in protein_df['sequence_2'].items():\n",
        "      sa_seq2 = ''\n",
        "      for aa in value:\n",
        "        sa_seq2 += aa + '#'\n",
        "      protein_df.at[index, 'sequence_2'] = sa_seq2\n",
        "\n",
        "    protein_df['name_2'] = 'name_2'\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 14. Pair Multiple SA Sequences\n",
        "  if data_type == \"Multiple pairs of SA Sequences\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "\n",
        "    protein_df['name_1'] = 'name_1'\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "\n",
        "    protein_df['name_2'] = 'name_2'\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 15. Pair Multiple UniProt IDs\n",
        "  if data_type == \"Multiple pairs of UniProt IDs\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_list1 = protein_df.loc[:, 'sequence_1'].tolist()\n",
        "    uniprot2pdb(protein_list1)\n",
        "    protein_df['name_1'] = protein_list1\n",
        "    protein_list1 = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list1]\n",
        "    mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs1 = mprs1.run()\n",
        "\n",
        "    protein_df['sequence_1'] = [output.split(\"\\t\")[1] for output in outputs1]\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "    protein_list2 = protein_df.loc[:, 'sequence_2'].tolist()\n",
        "    uniprot2pdb(protein_list2)\n",
        "    protein_df['name_2'] = protein_list2\n",
        "    protein_list2 = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list2]\n",
        "    mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs2 = mprs2.run()\n",
        "\n",
        "    protein_df['sequence_2'] = [output.split(\"\\t\")[1] for output in outputs2]\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "\n",
        "  # # 13-16. Pair Multiple Sequences\n",
        "  # elif data_type in data_type_list[12:16]:\n",
        "  #   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "  #   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "  #   print(\"=\"*100)\n",
        "\n",
        "  elif data_type ==  \"Multiple pairs of PDB/CIF Structures\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    # columns: sequence_1, sequence_2, type_1, type_2, chain_1, chain_2, label, stage\n",
        "\n",
        "    # protein_list = [(uniprot_id, type, chain), ...]\n",
        "    # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "    # uniprot2pdb(protein_list)\n",
        "\n",
        "    for i in range(1, 3):\n",
        "      protein_list = []\n",
        "      for index, row in protein_df.iterrows():\n",
        "        assert row[f\"type_{i}\"] in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "        row_tuple = (row[f\"seq_{i}\"], row[f\"type_{i}\"], row[f\"chain_{i}\"])\n",
        "        protein_list.append(row_tuple)\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      # add name column, del type column\n",
        "      protein_df[f'name_{i}'] = protein_df[f'seq_{i}'].apply(lambda x: x.split('.')[0])\n",
        "      protein_df.drop(f\"type_{i}\", axis=1, inplace=True)\n",
        "      print(outputs)\n",
        "      protein_df[f'seq_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "\n",
        "    # columns: name_1, name_2, chain_1, chain_2, sequence_1, sequence_2, label, stage\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "\n",
        "'''\n",
        "  elif data_type == \"A pair of AA Sequences\",\n",
        "  elif data_type == \"A pair of SA Sequences\",\n",
        "  elif data_type == \"A pair of UniProt IDs\",\n",
        "  elif data_type == \"A pair of PDB/CIF Structures\",\n",
        "  elif data_type == \"Multiple pairs of AA Sequences\",\n",
        "  elif data_type == \"Multiple pairs of SA Sequences\",\n",
        "  elif data_type == \"Multiple pairs of UniProt IDs\",\n",
        "  elif data_type == \"Multiple pairs of PDB/CIF Structures\",\n",
        "'''\n",
        "\n",
        "\n",
        "# # return a SA Sequence or a csv dataset path\n",
        "# def get_raw_dataset(data_type, raw_data):\n",
        "#   if data_type in data_type_list[:3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "#   elif data_type == data_type_list[3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[4:8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "#   return raw_dataset\n",
        "\n",
        "# def upload_dataset(data_type):\n",
        "#   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "#   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "#   print(\"=\"*100)\n",
        "\n",
        "#   # selected_csv_dataset = DATASET_HOME / f\"[DATASET]{Path(uploaded_csv_path).stem}.csv\"\n",
        "#   # get_SASequence_by_data_type(data_type, uploaded_csv_path, selected_csv_dataset)\n",
        "#   # get_SA_sequence_by_data_type(data_type, uploaded_csv_path)\n",
        "#   # print()\n",
        "#   # print(\"=\"*100)\n",
        "#   # print(Fore.BLUE+\"Successfully upload your dataset!\"+Style.RESET_ALL)\n",
        "\n",
        "#   return uploaded_csv_path\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "\n",
        "  # print(\"=\"*100)\n",
        "  # print(row_tuple)\n",
        "  # print(\"=\"*100)\n",
        "  uniprot_id = row_tuple[0].split('.')[0]     #\n",
        "  struc_type = row_tuple[1]                   # PDB or AF2\n",
        "  chain = row_tuple[2]\n",
        "\n",
        "  if struc_type==\"AF2\":\n",
        "    plddt_mask= True\n",
        "    chain = 'A'\n",
        "  else:\n",
        "    plddt_mask= False\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "    if Path(pdb_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    elif Path(cif_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    else:\n",
        "      raise BaseException(f\"The {uniprot_id}.pdb/{uniprot_id}.cif file doesn't exists!\")\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "          \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "          \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "          \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "          \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "def convert_outputs_to_pdb(outputs):\n",
        "\tfinal_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
        "\toutputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
        "\tfinal_atom_positions = final_atom_positions.cpu().numpy()\n",
        "\tfinal_atom_mask = outputs[\"atom37_atom_exists\"]\n",
        "\tpdbs = []\n",
        "\toutputs[\"plddt\"] *= 100\n",
        "\n",
        "\tfor i in range(outputs[\"aatype\"].shape[0]):\n",
        "\t\taa = outputs[\"aatype\"][i]\n",
        "\t\tpred_pos = final_atom_positions[i]\n",
        "\t\tmask = final_atom_mask[i]\n",
        "\t\tresid = outputs[\"residue_index\"][i] + 1\n",
        "\t\tpred = OFProtein(\n",
        "\t\t    aatype=aa,\n",
        "\t\t    atom_positions=pred_pos,\n",
        "\t\t    atom_mask=mask,\n",
        "\t\t    residue_index=resid,\n",
        "\t\t    b_factors=outputs[\"plddt\"][i],\n",
        "\t\t    chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
        "\t\t)\n",
        "\t\tpdbs.append(to_pdb(pred))\n",
        "\treturn pdbs\n",
        "\n",
        "\n",
        "# This function is copied from ColabFold!\n",
        "def show_pdb(path, show_sidechains=False, show_mainchains=False, color=\"lddt\"):\n",
        "  file_type = str(path).split(\".\")[-1]\n",
        "  if file_type == \"cif\":\n",
        "    file_type == \"mmcif\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(path,'r').read(),file_type)\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(get_chain_ids(path))\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###############   Download file to local computer   ##################\n",
        "################################################################################\n",
        "def file_download(path: str):\n",
        "  with open(path, \"rb\") as r:\n",
        "    res = r.read()\n",
        "\n",
        "  #FILE\n",
        "  filename = os.path.basename(path)\n",
        "  b64 = base64.b64encode(res)\n",
        "  payload = b64.decode()\n",
        "\n",
        "  #BUTTONS\n",
        "  html_buttons = '''<html>\n",
        "  <head>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "  </head>\n",
        "  <body>\n",
        "  <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "  <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "  </a>\n",
        "  </body>\n",
        "  </html>\n",
        "  '''\n",
        "\n",
        "  html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "  display(HTML(html_button))\n",
        "\n",
        "  # Automatically download file if the server is from google cloud.\n",
        "  if root_dir == \"/content\":\n",
        "    files.download(path)\n",
        "\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uxag_RSBI7e"
      },
      "source": [
        "# **2: Train and Share your Protein Model** <a name=\"train\"></a>\n",
        "\n",
        "You can **train** a model based on pre-trained SaProt, or **continually train** a fine-tuned model in SaprotHub.\n",
        "\n",
        "\n",
        "\n",
        "<!-- ## Training Dataset\n",
        "\n",
        "For the training dataset, **two additional columns** are required in the CSV file: `label` and `stage`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_training.png\n",
        "?raw=true\" height=\"200\" width=\"400px\" align=\"center\">\n",
        "\n",
        "### Column `label`\n",
        "\n",
        "The content of column `label` depends on your **task type**:\n",
        "\n",
        "| Task Type                         | Content in the Column                          |\n",
        "|-----------------------------------|------------------------------------------------|\n",
        "| Classification tasks              | Category index starting from zero              |\n",
        "| Amino Acid Classification tasks   | A list of category indices for each amino acid |\n",
        "| Regression tasks                  | Numerical values                               |\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/label_format.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "<br>\n",
        "\n",
        "\n",
        "### Column `stage`\n",
        "\n",
        "The column `stage` indicate whether the sample is used for training, validation, or testing. Ensure your dataset includes samples for all three stages. The values are: `train`, `valid`, `test`.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Note:**\n",
        "\n",
        "1. **Examples are available** at /content/SaprotHub/upload_files (if you connect to your local server, then the path is /SaprotHub/upload_files). Download to review their format, and then upload them for a trial.\n",
        "\n",
        "2.  <a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "3. <a href=\"#fa2csv\">Here</a> you can **convert your .fa/.fasta file to a .csv file**, which corresponds to the data format for Multiple AA Sequences.\n",
        "\n",
        "4. <a href=\"#split_dataset\">Here</a> you can **randomly split your .csv dataset**, which means to add a `stage` column, where the ratio of `train`:`valid`:`test` is 8:1:1.\n",
        "\n",
        "4. The maximum input length of the model is 1024, and protein sequences exceeding this length will only retain the first 1024 amino acids. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vqdmLslQBI7e"
      },
      "outputs": [],
      "source": [
        "#@title **2.1: Train your Model** <a name=\"train\"></a>\n",
        "\n",
        "################################################################################\n",
        "############################# ADVANCED CONFIG ##################################\n",
        "################################################################################\n",
        "\n",
        "# training config\n",
        "GPU_batch_size = 0\n",
        "accumulate_grad_batches = 0\n",
        "num_workers = 2\n",
        "seed = 20000812\n",
        "\n",
        "# lora config\n",
        "r = 8\n",
        "lora_dropout = 0.0\n",
        "lora_alpha = 16\n",
        "\n",
        "# dataset config\n",
        "val_check_interval=0.5\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "\n",
        "mask_struc_ratio=None\n",
        "\n",
        "################################################################################\n",
        "################################## MARKDOWN #################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.1:-Train-your-model)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "  raise BaseException(\"Please refer to Section 1.1 to switch your Runtime to a GPU!\")\n",
        "\n",
        "################################################################################\n",
        "################################## TASK CONFIG #################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "task_name = \"demo\" # @param {type:\"string\"}\n",
        "task_type = \"Protein-level Regression\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "original_task_type = task_type\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'Enter the number of category in your training dataset here:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "#################################### MODEL CONFIG #####################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "base_model = \"Official pretrained SaProt (35M)\" # @param [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]\n",
        "\n",
        "# continue learning\n",
        "if base_model in [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]:\n",
        "  continue_learning = True\n",
        "  adapter_combobox = select_adapter_from(task_type, use_model_from=base_model)\n",
        "else:\n",
        "  continue_learning = False\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "\n",
        "data_type = \"Multiple SA Sequences\" # @param [\"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"SaprotHub Dataset\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 4. Training\n",
        "\n",
        "batch_size = \"Adaptive\" # @param [\"Adaptive\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]\n",
        "max_epochs = 1 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-3 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# CONFIG #######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN ####################################\n",
        "################################################################################\n",
        "\n",
        "def train(button):\n",
        "  global base_model\n",
        "  global GPU_batch_size\n",
        "  global accumulate_grad_batches\n",
        "\n",
        "  button.disabled = True\n",
        "  button.description = 'Training...'\n",
        "  button.button_style = ''\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIRM ####################################\n",
        "################################################################################\n",
        "\n",
        "  if data_type == data_type_list[8]:\n",
        "    lmdb_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  else:\n",
        "    csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    from saprot.utils.construct_lmdb import construct_lmdb\n",
        "    construct_lmdb(csv_dataset_path, LMDB_HOME, task_name, task_type)\n",
        "\n",
        "    lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "################################################################################\n",
        "################################### MODEL CONFIRM ####################################\n",
        "################################################################################\n",
        "\n",
        "  # base_model\n",
        "  if continue_learning:\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "    print(f\"Training on an existing model: {adapter_path}\")\n",
        "\n",
        "    if base_model == \"Shared by peers on SaprotHub\":\n",
        "      if not adapter_path.exists():\n",
        "        snapshot_download(repo_id=adapter_combobox.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    adapter_config_path = Path(adapter_path) / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "      adapter_config = json.load(f)\n",
        "      base_model = adapter_config['base_model_name_or_path']\n",
        "\n",
        "  elif base_model == \"Official pretrained SaProt (35M)\":\n",
        "    base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "\n",
        "  elif base_model == \"Official pretrained SaProt (650M)\":\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "  # model size and model name\n",
        "  if base_model == \"westlake-repl/SaProt_650M_AF2\":\n",
        "    model_size = \"650M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "  elif base_model == \"westlake-repl/SaProt_35M_AF2\":\n",
        "    model_size = \"35M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "\n",
        "  config.setting.run_mode = \"train\"\n",
        "  config.setting.seed = seed\n",
        "\n",
        "################################################################################\n",
        "################################# MODEL ########################################\n",
        "################################################################################\n",
        "\n",
        "  if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "  config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / \"Local\" / model_name)\n",
        "\n",
        "  if task_type in [\"regression\", \"pair_regression\", \"pair_classification\"]:\n",
        "    config.model.kwargs.extra_config = {}\n",
        "    config.model.kwargs.extra_config.attention_probs_dropout_prob=0\n",
        "    config.model.kwargs.extra_config.hidden_dropout_prob=0\n",
        "\n",
        "  config.model.kwargs.lora_kwargs = EasyDict({\n",
        "    \"is_trainable\": True,\n",
        "    \"num_lora\": 1,\n",
        "    \"r\": r,\n",
        "    \"lora_dropout\": lora_dropout,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"config_list\": []})\n",
        "  if continue_learning:\n",
        "    config.model.kwargs.lora_kwargs.config_list.append({\"lora_config_path\": adapter_path})\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "  config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "\n",
        "  config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "  config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "  config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "  # num_workers\n",
        "  config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "  # mask_struc\n",
        "  # config.dataset.kwargs.mask_struc_ratio= mask_struc_ratio\n",
        "\n",
        "  ################################################################################\n",
        "  ######################## batch size ############################################\n",
        "  ################################################################################\n",
        "  def get_accumulate_grad_samples(num_samples):\n",
        "      if num_samples > 3200:\n",
        "          return 64\n",
        "      elif 1600 < num_samples <= 3200:\n",
        "          return 32\n",
        "      elif 800 < num_samples <= 1600:\n",
        "          return 16\n",
        "      elif 400 < num_samples <= 800:\n",
        "          return 8\n",
        "      elif 200 < num_samples <= 400:\n",
        "          return 4\n",
        "      elif 100 < num_samples <= 200:\n",
        "          return 2\n",
        "      else:\n",
        "          return 1\n",
        "\n",
        "  # advanced config\n",
        "  if (GPU_batch_size > 0) and (accumulate_grad_batches > 0):\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  elif (GPU_batch_size == 0) and (accumulate_grad_batches == 0):\n",
        "\n",
        "    # batch_size\n",
        "    if base_model == \"westlake-repl/SaProt_650M_AF2\" and root_dir == \"/content\":\n",
        "      GPU_batch_size = 1\n",
        "    else:\n",
        "      GPU_batch_size_dict = {\n",
        "        \"Tesla T4\": 2,\n",
        "        \"NVIDIA L4\": 2,\n",
        "        \"NVIDIA A100-SXM4-40GB\": 4,\n",
        "        }\n",
        "      GPU_name = torch.cuda.get_device_name(0)\n",
        "      GPU_batch_size = GPU_batch_size_dict[GPU_name] if GPU_name in GPU_batch_size_dict else 2\n",
        "\n",
        "      if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "        GPU_batch_size = int(max(GPU_batch_size / 2, 1))\n",
        "\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "\n",
        "    # accumulate_grad_batches\n",
        "    if batch_size == \"Adaptive\":\n",
        "\n",
        "      env = lmdb.open(config.dataset.train_lmdb, readonly=True)\n",
        "\n",
        "      with env.begin() as txn:\n",
        "        stat = txn.stat()\n",
        "        num_samples = stat['entries']\n",
        "\n",
        "      accumulate_grad_samples = get_accumulate_grad_samples(num_samples)\n",
        "\n",
        "    else:\n",
        "      accumulate_grad_samples = int(batch_size)\n",
        "\n",
        "    accumulate_grad_batches = max(int(accumulate_grad_samples / GPU_batch_size), 1)\n",
        "\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  else:\n",
        "    raise BaseException(f\"Please make sure `GPU_batch_size`({GPU_batch_size}) and `accumulate_grad_batches`({accumulate_grad_batches}) are both greater than zero!\")\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## TRAINER #########################################\n",
        "  ################################################################################\n",
        "\n",
        "  config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  # epoch\n",
        "  config.Trainer.max_epochs = max_epochs\n",
        "  # test only: load the existing model\n",
        "  if config.Trainer.max_epochs == 0:\n",
        "    config.model.save_path = config.model.kwargs.lora_kwargs.config_list[0].lora_config_path\n",
        "\n",
        "  # learning rate\n",
        "  config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "  # trainer\n",
        "  config.Trainer.limit_train_batches=limit_train_batches\n",
        "  config.Trainer.limit_val_batches=limit_val_batches\n",
        "  config.Trainer.limit_test_batches=limit_test_batches\n",
        "  config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "  # strategy\n",
        "  strategy = {\n",
        "      # - deepspeed\n",
        "      # 'class': 'DeepSpeedStrategy',\n",
        "      # 'stage': 2\n",
        "\n",
        "      # - None\n",
        "      # 'class': None,\n",
        "\n",
        "      # - DP\n",
        "      # 'class': 'DataParallelStrategy',\n",
        "\n",
        "      # - DDP\n",
        "      # 'class': 'DDPStrategy',\n",
        "      # 'find_unused_parameter': True\n",
        "  }\n",
        "  config.Trainer.strategy = strategy\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Run the task ####################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f\"Training task type: {task_type}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "  if continue_learning:\n",
        "    print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']}\"+Style.RESET_ALL)\n",
        "  print('='*100)\n",
        "  pprint.pprint(config)\n",
        "  print('='*100)\n",
        "\n",
        "  from saprot.scripts.training import finetune\n",
        "  finetune(config)\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Save the adapter ################################\n",
        "  ################################################################################\n",
        "\n",
        "  def add_training_data_type_to_config(metadata_path, training_data_type):\n",
        "    if metadata_path.exists() is False:\n",
        "      config_data = {\n",
        "          'training_data_type': training_data_type\n",
        "          }\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "    else:\n",
        "      with open(metadata_path, 'r') as file:\n",
        "          config_data = json.load(file)\n",
        "\n",
        "      config_data['training_data_type'] = training_data_type\n",
        "\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "  metadata_path = Path(config.model.save_path) / \"metadata.json\"\n",
        "  training_data_type = training_data_type_dict[data_type]\n",
        "  add_training_data_type_to_config(metadata_path, training_data_type)\n",
        "\n",
        "  print(Fore.BLUE)\n",
        "  print(f\"Model is saved to \\\"{config.model.save_path}\\\" on Colab Server\")\n",
        "  print(Style.RESET_ALL)\n",
        "\n",
        "\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{model_name}.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\" \"metadata.json\"\n",
        "  # !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\" \"metadata.json\"\n",
        "  print(\"Click to download the model to your local computer\")\n",
        "  if adapter_zip.exists():\n",
        "    # files.download(adapter_zip)\n",
        "    file_download(adapter_zip)\n",
        "\n",
        "  \n",
        "  \n",
        "  ################################################################################\n",
        "  ############################### Modify README ##################################\n",
        "  ################################################################################\n",
        "  name = model_name\n",
        "  description = '<slot name=\\'description\\'>'\n",
        "  label_meanings = '<slot name=\\'label_meanings\\'>'\n",
        "\n",
        "  with open(f'{config.model.save_path}/adapter_config.json', 'r') as f:\n",
        "    lora_config = json.load(f)\n",
        "\n",
        "  markdown = f'''\n",
        "---\n",
        "\n",
        "base_model: {base_model} \\n\n",
        "library_name: peft\n",
        "\n",
        "---\n",
        "\\n\n",
        "\n",
        "# Model Card for {name} \n",
        "{description}\n",
        "\n",
        "## Task type\n",
        "{original_task_type}\n",
        "\n",
        "## Model input type\n",
        "{training_data_type_dict[data_type]} Sequence\n",
        "\n",
        "## Label meanings\n",
        "{label_meanings}\n",
        "\n",
        "## LoRA config\n",
        "\n",
        "- **r:** {lora_config['r']}\n",
        "- **lora_dropout:** {lora_config['lora_dropout']}\n",
        "- **lora_alpha:** {lora_config['lora_alpha']}\n",
        "- **target_modules:** {lora_config['target_modules']}\n",
        "- **modules_to_save:** {lora_config['modules_to_save']}\n",
        "\n",
        "## Training config\n",
        "\n",
        "- **optimizer:**\n",
        "  - **class:** AdamW\n",
        "  - **betas:** (0.9, 0.98)\n",
        "  - **weight_decay:** 0.01\n",
        "- **learning rate:** {config.model.lr_scheduler_kwargs.init_lr}\n",
        "- **epoch:** {config.Trainer.max_epochs}\n",
        "- **batch size:** {config.dataset.dataloader_kwargs.batch_size * config.Trainer.accumulate_grad_batches}\n",
        "- **precision:** 16-mixed \\n\n",
        "'''\n",
        "\n",
        "  # Write the markdown output to a file\n",
        "  with open(f\"{config.model.save_path}/README.md\", \"w\") as file:\n",
        "    file.write(markdown)\n",
        "\n",
        "\n",
        "button_train = ipywidgets.Button(\n",
        "    description='Start Training',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_train.on_click(train)\n",
        "button_train.layout.width = '300px'\n",
        "display(button_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UnKX1BTZBI7f"
      },
      "outputs": [],
      "source": [
        "#@title **2.2: Login HuggingFace to Upload your model (Optional)** <a name=\"upload_model\"></a>\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6XlluTsPBI7m"
      },
      "outputs": [],
      "source": [
        "#@title **2.3: Upload your Model (Optional)**\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/2.2:-Upload-your-model)\n",
        "\n",
        "# #@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "\n",
        "################################################################################\n",
        "########################## Metadata  ###########################################\n",
        "################################################################################\n",
        "name = \"demo_cls\" # @param {type:\"string\"}\n",
        "description = \"This model is used for a demo classification task\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# #@markdown > 0: Nucleus <br>\n",
        "# #@markdown > 1: Cytoplasm <br>\n",
        "# #@markdown > 2: Extracellular <br>\n",
        "# #@markdown > ... <br>\n",
        "# #@markdown > 9: Peroxisome <br>\n",
        "\n",
        "label_meanings = \"A, B\" #@param {type:\"string\"}\n",
        "\n",
        "################################################################################\n",
        "########################### Move Files  ########################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "if name == \"\":\n",
        "  name = model_name\n",
        "repo_name = user['name'] + '/' + name\n",
        "local_dir = Path(\"/content/SaprotHub/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "################################################################################\n",
        "########################## Modify README  ######################################\n",
        "################################################################################\n",
        "import json\n",
        "\n",
        "md_path = local_dir / \"README.md\"\n",
        "\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    label_meanings_md = ''\n",
        "    for index, label in enumerate(label_meanings.split(', ')):\n",
        "      label_meanings_md += f'''\n",
        "{index}: {label.strip()}\n",
        "'''\n",
        "    label_meanings = label_meanings_md\n",
        "\n",
        "replace_data = {\n",
        "    '<slot name=\\'description\\'>': description,\n",
        "    '<slot name=\\'label_meanings\\'>': label_meanings\n",
        "}\n",
        "\n",
        "with open(md_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "for key, value in replace_data.items():\n",
        "    if value != \"\":\n",
        "        content = content.replace(key, value)\n",
        "\n",
        "with open(md_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1JgmrsBI7m"
      },
      "source": [
        "# **3: Use SaProt to Predict**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h8qHRJtIQxU4"
      },
      "outputs": [],
      "source": [
        "#@title **3.1: Classification&Regression Prediction** <a name=\"prediction\"></a>\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.1:-Classification-Regression-Prediction)\n",
        "\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "################################################################################\n",
        "################################# TASK #########################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "\n",
        "task_type = \"Protein-level Regression\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'The number of categories in your classification task:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################## MODEL #######################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "##@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "##@markdown\n",
        "##@markdown 1. Select a **base model**\n",
        "##@markdown\n",
        "##@markdown 2. By running this cell, you will see an **model combobox**. We provide two ways to select your adapter:\n",
        "##@markdown  - Select a **local model** from the combobox.\n",
        "##@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtHub/DeepLoc_cls10_35M\")\n",
        "##@markdown\n",
        "##@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtHub)\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\" #@param ['westlake-repl/SaProt_35M_AF2', 'westlake-repl/SaProt_650M_AF2'] {allow-input:true}\n",
        "use_model_from = \"Trained by yourself on ColabSaprot\" # @param [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\", \"Multi-models on SaprotHub\"]\n",
        "if use_model_from == \"Multi-models on SaprotHub\":\n",
        "  multi_lora = True\n",
        "else:\n",
        "  multi_lora = False\n",
        "\n",
        "# use_existing_model = True # @param {type:\"boolean\"}\n",
        "# use_existing_model = True\n",
        "# if use_existing_model:\n",
        "#   adapter_combobox = select_adapter()\n",
        "\n",
        "adapter_input = select_adapter_from(task_type, use_model_from)\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "# # @markdown Please ensure that the selected task type aligns with the training task type of the model you intend to utilize.\n",
        "\n",
        "## @markdown If you are conducting inference on a classification task, please ensure that the `num_of_category` matches the number of categories in the training dataset. Otherwise, you do not need to assign `num_of_category`.\n",
        "\n",
        "\n",
        "##@markdown You have two options to provide your protein sequences:\n",
        "##@markdown - **Single Sequence: Enter a single SA sequence** into the input box, you can get a SA Sequence by clicking <a href=\"#get_SA_seq\">here</a>\n",
        "##@markdown - **Multiple Sequences: Select a dataset**, you can upload a dataset from <a href=\"#upload_dataset\">here</a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(Fore.BLUE+f\"Data type: {data_type}\"+Style.RESET_ALL)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################ DATASET #######################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "data_type = \"Multiple PDB/CIF Structures\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\", \"A pair of PDB/CIF Structures\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "\n",
        "mode = \"Multiple Sequences\" if (data_type in data_type_list[4:8] or data_type in data_type_list[13:17]) else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "# [\"Single AA Sequence\",\"Single SA Sequence\",\"Single UniProt ID\",\"Single PDB/CIF Structure\",\"Multiple AA Sequences\",\"Multiple SA Sequences\",\"Multiple UniProt IDs\",\"Multiple PDB/CIF Structures\",\"SaprotHub Dataset\",\"A pair of AA Sequences\",\"A pair of SA Sequences\",\"A pair of UniProt IDs\",\"A pair of PDB/CIF Structures\",\"Multiple pairs of AA Sequences\",\"Multiple pairs of SA Sequences\",\"Multiple pairs of UniProt IDs\",\"Multiple pairs of PDB/CIF Structures\"]\n",
        "if mode == \"Multiple Sequences\":\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "def apply(button):\n",
        "  global single_sa_seq\n",
        "  button.disabled = True\n",
        "  button.description = 'Predicting...'\n",
        "  button.button_style = ''\n",
        "\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f'Current Model ({use_model_from}): {adapter_input.value}'+Style.RESET_ALL)\n",
        "  if data_type in [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\"]:\n",
        "    print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data.value}'+Style.RESET_ALL)\n",
        "  elif data_type == \"A pair of PDB/CIF Structures\":\n",
        "    print(Fore.BLUE+f'Current Dataset ({data_type}): Sequence 1: {raw_data[0]}, Sequence 2: {raw_data[1]}'+Style.RESET_ALL)\n",
        "  elif data_type in [\"A pair of AA Sequences\",\"A pair of SA Sequences\",\"A pair of UniProt IDs\"]:\n",
        "    print(Fore.BLUE+f'Current Dataset ({data_type}): Sequence 1: {raw_data[0].value}, Sequence 2: {raw_data[1].value}'+Style.RESET_ALL)\n",
        "  elif data_type == \"Single PDB/CIF Structure\":\n",
        "    print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data[0]}' +Style.RESET_ALL)\n",
        "  elif data_type in [\"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]:\n",
        "    print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data}'+Style.RESET_ALL)\n",
        "\n",
        "  predict()\n",
        "\n",
        "  # button_predict.on_click(predict)\n",
        "  # button_apply.layout.width = '500px'\n",
        "  # display(button_predict)\n",
        "\n",
        "button_apply = ipywidgets.Button(\n",
        "    description='Make Prediction',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_apply.on_click(apply)\n",
        "# button_apply.layout.width = '500px'\n",
        "display(button_apply)\n",
        "\n",
        "\n",
        "# def apply(button):\n",
        "#     global single_sa_seq\n",
        "#     # button.disabled = True\n",
        "#     # button.description = 'Clicked'\n",
        "#     # button.button_style = ''\n",
        "\n",
        "#     # print(Fore.BLUE+'Construct dataset...'+Style.RESET_ALL)\n",
        "#     if mode == \"Multiple Sequences\":\n",
        "#       raw_data = input_raw_data_by_data_type(data_type)\n",
        "#       csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "#     elif mode == \"Single Sequence\":\n",
        "#       single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "\n",
        "#     print()\n",
        "#     print('='*100)\n",
        "#     print(Fore.BLUE+f'Current Model ({use_model_from}): {adapter_input.value}'+Style.RESET_ALL)\n",
        "#     print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data.value}'+Style.RESET_ALL)\n",
        "\n",
        "# button_apply = ipywidgets.Button(\n",
        "#     description='Apply',\n",
        "#     disabled=False,\n",
        "#     button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "#     tooltip='Apply',\n",
        "#     icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "#     )\n",
        "# button_apply.on_click(apply)\n",
        "# # button_apply.layout.width = '500px'\n",
        "# if\n",
        "# display(button_apply)\n",
        "\n",
        "\n",
        "##@markdown <br>\n",
        "\n",
        "##@markdown  <font color=\"red\"> **Note that:** </font> If `use_model_from` is set to `Multi-models on SaprotHub`, each sample will be predicted using multiple models. For classification tasks, voting will be used to determine the final predicted category; for regression tasks, the predicted values from each model will be averaged.\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "def get_base_model(adapter_path):\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    adapter_config_dict = json.load(f)\n",
        "    base_model = adapter_config_dict['base_model_name_or_path']\n",
        "    if 'SaProt_650M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    elif 'SaProt_35M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    else:\n",
        "      raise RuntimeError(\"Please ensure the base model is \\\"SaProt_650M_AF2\\\" or \\\"SaProt_35M_AF2\\\"\")\n",
        "  return base_model\n",
        "\n",
        "def check_training_data_type(adapter_path, data_type):\n",
        "  metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "  if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      required_training_data_type = metadata['training_data_type']\n",
        "  else:\n",
        "    required_training_data_type = \"SA\"\n",
        "  assert required_training_data_type == training_data_type_dict[data_type], f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences. Please ensure your data type is also {required_training_data_type} sequences for accurate predictions.\"\n",
        "\n",
        "# def predict(button):\n",
        "#   button.disabled = True\n",
        "#   button.description = 'Predicting'\n",
        "#   button.button_style = ''\n",
        "def predict():\n",
        "  # base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "  if multi_lora:\n",
        "    if use_model_from == \"Multi-models on ColabSaprot\":\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / lora_config_path}) for lora_config_path in list(adapter_input.value)]\n",
        "    elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "      #1. get adapter_list\n",
        "      repo_id_list = adapter_input.value.replace(\" \", \"\").split(',')\n",
        "      #2. download adapters\n",
        "      for repo_id in repo_id_list:\n",
        "        snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / repo_id)\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "    assert len(config_list) > 0, \"Please select your models from the dropdown menu on the output of 3.1.1!\"\n",
        "    base_model = get_base_model(ADAPTER_HOME / task_type / config_list[0].lora_config_path)\n",
        "\n",
        "    for lora_config in config_list:\n",
        "      check_training_data_type(lora_config.lora_config_path, data_type)\n",
        "\n",
        "    lora_kwargs = EasyDict({\n",
        "      \"num_lora\": len(config_list),\n",
        "      \"config_list\": config_list\n",
        "    })\n",
        "\n",
        "\n",
        "  else:\n",
        "    if use_model_from == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / adapter_input.value)\n",
        "\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_input.value\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    check_training_data_type(adapter_path, data_type)\n",
        "    lora_kwargs = {\n",
        "        \"num_lora\": 1,\n",
        "        \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ##################################### config ###################################\n",
        "  ################################################################################\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "  # task\n",
        "  if task_type in [ \"classification\", \"token_classification\"]:\n",
        "    # config.model.kwargs.num_labels = num_of_categories.value\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  # config.model.save_path = model_save_path\n",
        "  config.model.kwargs.config_path = base_model\n",
        "\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### inference ##################################\n",
        "  ################################################################################\n",
        "  from peft import PeftModelForSequenceClassification\n",
        "\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  print(Fore.BLUE+f\"Inference task type: {task_type}\"+Style.RESET_ALL)\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"Dataset: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    if data_type == \"Single PDB/CIF Structure\":\n",
        "      print(Fore.BLUE+f'Dataset ({data_type}): {raw_data[0]}' +Style.RESET_ALL)\n",
        "    else:\n",
        "      print(Fore.BLUE+f\"Dataset: {raw_data.value}\"+Style.RESET_ALL)\n",
        "\n",
        "  if multi_lora:\n",
        "    print(Fore.BLUE+f\"Model: {base_model} - {[str(lora_config.lora_config_path) for lora_config in lora_kwargs.config_list]}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"Model: {base_model} - {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "  outputs_list=[]\n",
        "\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "\n",
        "  if mode == \"Single Sequence\":\n",
        "    if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "      df = pd.DataFrame({\n",
        "          'sequence_1': [single_sa_seq[0]],\n",
        "          'sequence_2': [single_sa_seq[1]]\n",
        "      })\n",
        "    else:\n",
        "      df = pd.DataFrame({\n",
        "          'sequence': [single_sa_seq]\n",
        "      })\n",
        "  elif mode == \"Multiple Sequences\":\n",
        "    df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+\"Prediction Result:\"+Style.RESET_ALL)\n",
        "\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "      input_1 = tokenizer(row['sequence_1'], return_tensors=\"pt\")\n",
        "      input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "      input_2 = tokenizer(row['sequence_2'], return_tensors=\"pt\")\n",
        "      input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "      with torch.no_grad(): outputs = model(input_1, input_2)\n",
        "      outputs_list.append(outputs)\n",
        "  else:\n",
        "    for index in tqdm(range(len(df))):\n",
        "      seq = df['sequence'].iloc[index]\n",
        "      inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      with torch.no_grad(): outputs = model(inputs)\n",
        "      outputs_list.append(outputs)\n",
        "\n",
        "\n",
        "    if task_type == \"pair_classification\":\n",
        "      softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "      print()\n",
        "      for index, output in enumerate(softmax_output_list):\n",
        "        print(f\"For Sequence {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "        df.loc[index, 'result'] = output.index(max(output))\n",
        "        df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "      df.to_csv(output_file, index=False)\n",
        "\n",
        "    elif task_type == \"pair_regression\":\n",
        "      print()\n",
        "      for index, output in enumerate(outputs_list):\n",
        "        print(f\"For Sequence {index}, Value {output.cpu().item()}\")\n",
        "      df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "      df.to_csv(output_file, index=False)\n",
        "\n",
        "    elif task_type == \"classification\":\n",
        "      print()\n",
        "      softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "      for index, output in enumerate(softmax_output_list):\n",
        "        print(f\"For Sequence {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "        df.loc[index, 'result'] = output.index(max(output))\n",
        "        df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "      df.to_csv(output_file, index=False)\n",
        "\n",
        "    elif task_type == \"regression\":\n",
        "      print()\n",
        "      for index, output in enumerate(outputs_list):\n",
        "        print(f\"For Sequence {index}, Value {output.item()}\")\n",
        "      df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "      df.to_csv(output_file, index=False)\n",
        "\n",
        "    elif task_type == \"token_classification\":\n",
        "      seq_prob_df_list = []\n",
        "      softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "      print(\"The probability of each category:\")\n",
        "      for seq_index, seq in enumerate(softmax_output_list):\n",
        "        seq_prob_df = pd.DataFrame(seq)[1:-1]\n",
        "        print('='*100)\n",
        "        print(f'Sequence {seq_index + 1}:')\n",
        "        print(seq_prob_df.to_string())\n",
        "        seq_prob_df['seq_index'] = seq_index\n",
        "        seq_prob_df['aa_index'] = seq_prob_df.index\n",
        "        seq_prob_df['sequence'] = df.loc[seq_index, 'sequence']\n",
        "        seq_prob_df_list.append(seq_prob_df)\n",
        "      combined_df = pd.concat(seq_prob_df_list, ignore_index=False)\n",
        "      combined_df.to_csv(output_file, index=True)\n",
        "\n",
        "    print()\n",
        "    print('='*100)\n",
        "    print(Fore.BLUE+f\"The prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "    file_download(output_file)\n",
        "\n",
        "# button_predict = ipywidgets.Button(\n",
        "#     description='Make Prediction',\n",
        "#     disabled=False,\n",
        "#     button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "#     tooltip='Apply',\n",
        "#     icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uxD_KOF1BI7n"
      },
      "outputs": [],
      "source": [
        "#@title **3.2: Mutational Effect Prediction** <a name=\"mutational_effect_prediction\"></a>\n",
        "#@markdown <font color=\"red\">Our model is pre-trained based on protein structures and performs best when provided with structural data.\n",
        "#@markdown **Therefore, if you only have the amino acid sequence, we strongly recommend using AF2 to predict the structure and generate a PDB file before proceeding with mutation prediction.**\n",
        "\n",
        "#@markdown <a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.2:-Mutational-Effect-Prediction)\n",
        "\n",
        "\n",
        "mutation_task = \"Saturation mutagenesis\" #@param [\"Single-site or Multi-site mutagenesis\", \"Saturation mutagenesis\"]\n",
        "\n",
        "# data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "data_type = \"Single SA Sequence\" # @param [\"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "  if mode == \"Single Sequence\":\n",
        "    input_mut = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder='Enter Single Mutation Information here',\n",
        "      # description='SA Sequence:',\n",
        "      disabled=False)\n",
        "    print(Fore.BLUE+\"Mutation:\"+Style.RESET_ALL)\n",
        "    input_mut.layout.width = '500px'\n",
        "    display(input_mut)\n",
        "\n",
        "def mutational_effect_predict(button):\n",
        "  button.disabled = True\n",
        "  button.description = 'Clicked'\n",
        "  button.button_style = ''\n",
        "\n",
        "\n",
        "  #@title 3.2.2: Get your Result\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# DATASET ###################################\n",
        "  ################################################################################\n",
        "  if mode == \"Single Sequence\":\n",
        "    seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  else:\n",
        "    dataset_csv_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# Task Info ####################################\n",
        "  ################################################################################\n",
        "  base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "  # clear_output(wait=True)\n",
        "\n",
        "  print(Fore.BLUE)\n",
        "  print(f\"Mutation task: {mutation_task}\")\n",
        "  print(f\"Mode: {mode}\")\n",
        "  print(f\"Model: {base_model}\")\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"Dataset: {dataset_csv_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"Dataset: {seq}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Style.RESET_ALL)\n",
        "\n",
        "  print(f\"Predicting...\")\n",
        "  timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "\n",
        "  ################################################################################\n",
        "  ################################# load model ###################################\n",
        "  ################################################################################\n",
        "\n",
        "  from saprot.model.saprot.saprot_foldseek_mutation_model import SaprotFoldseekMutationModel\n",
        "\n",
        "  config = {\n",
        "      \"foldseek_path\": None,\n",
        "      \"config_path\": base_model,\n",
        "      \"load_pretrained\": True,\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    zero_shot_model\n",
        "  except Exception:\n",
        "    zero_shot_model = SaprotFoldseekMutationModel(**config)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    zero_shot_model.to(device)\n",
        "\n",
        "  ################################################################################\n",
        "  ########################### Single Sequence ####################################\n",
        "  ################################################################################\n",
        "  if mode == \"Single Sequence\":\n",
        "\n",
        "    if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "      mut = input_mut.value\n",
        "      # validate mut\n",
        "      aa_seq = seq[0::2]\n",
        "      for m in mut.split(':'):\n",
        "        ori_aa = m[0]\n",
        "        pos = int(m[1:-1])\n",
        "        mut_aa = m[-1]\n",
        "        assert aa_seq[pos-1] == ori_aa, f\"The provided mutation information contains an error ({m}): the original amino acid at position {pos} ({ori_aa}) does not match your sequence ({aa_seq[pos-1]}).\"\n",
        "\n",
        "      score = zero_shot_model.predict_mut(seq, mut)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "      print(f\"The score of mutation {mut} is {Fore.BLUE}{score}{Style.RESET_ALL}\")\n",
        "\n",
        "    if mutation_task==\"Saturation mutagenesis\":\n",
        "      timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "      output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "      mut_dicts = []\n",
        "      for pos in tqdm(range(1, int(len(seq) / 2)+1), total=int(len(seq) / 2)+1, leave=False, desc=f\"Predicting\"):\n",
        "        mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "        mut_dicts.append(mut_dict)\n",
        "\n",
        "      mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "      df = pd.DataFrame(mut_list)\n",
        "      df.to_csv(output_path, index=None)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "      # files.download(output_path)\n",
        "      file_download(output_path)\n",
        "      print(f\"\\n{Fore.BLUE}The result has been saved to {output_path} and your local computer.{Style.RESET_ALL}\")\n",
        "\n",
        "  ################################################################################\n",
        "  ########################### Multiple Sequences #################################\n",
        "  ################################################################################\n",
        "  if mode == \"Multiple Sequences\":\n",
        "\n",
        "    dataset_df = pd.read_csv(dataset_csv_path)\n",
        "    results = []\n",
        "\n",
        "    if mutation_task==\"Single-site or Multi-site mutagenesis\":\n",
        "      for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "       seq = row['sequence']\n",
        "       mut_info = row['mutation']\n",
        "       results.append(zero_shot_model.predict_mut(seq, mut_info).cpu().item())\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "      # result_df = pd.DataFrame()\n",
        "      # result_df['sequence'] = dataset_df['sequence']\n",
        "      # result_df['mutation'] = dataset_df['mutation']\n",
        "      dataset_df['score'] = results\n",
        "\n",
        "      output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}.csv\"\n",
        "      dataset_df.to_csv(output_path, index=None)\n",
        "      file_download(output_path)\n",
        "      print(f\"{Fore.BLUE}The result has been saved to {output_path} and your local computer {Style.RESET_ALL}\")\n",
        "\n",
        "    else:\n",
        "      for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "        seq = row['sequence']\n",
        "        mut_dicts = []\n",
        "        for pos in range(1, int(len(seq) / 2)+1):\n",
        "          mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "          mut_dicts.append(mut_dict)\n",
        "        mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "        result_df = pd.DataFrame(mut_list)\n",
        "        results.append(result_df)\n",
        "\n",
        "      print()\n",
        "      print(\"=\"*100)\n",
        "      print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "      zip_files = []\n",
        "      for i in range(len(results)):\n",
        "        output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}_Sequence{i+1}.csv\"\n",
        "        results[i].to_csv(output_path, index=None)\n",
        "        zip_files.append(output_path)\n",
        "\n",
        "      # zip and download zip to local computer\n",
        "      zip_path = OUTPUT_HOME / f\"{timestamp}_{Path(dataset_csv_path).stem}.zip\"\n",
        "      with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "          for file in zip_files:\n",
        "              zipf.write(file, os.path.basename(file))\n",
        "      # files.download(zip_path)\n",
        "      print(f\"{Fore.BLUE}The result has been saved to {zip_path} and your local computer{Style.RESET_ALL}\")\n",
        "      file_download(zip_path)\n",
        "\n",
        "button_mutational_effect_predict = ipywidgets.Button(\n",
        "  description='Mutational Effect Predict',\n",
        "  disabled=False,\n",
        "  button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Apply',\n",
        "  # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "button_mutational_effect_predict.on_click(mutational_effect_predict)\n",
        "button_mutational_effect_predict.layout.width = '300px'\n",
        "display(button_mutational_effect_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlQdqTcBI7n"
      },
      "source": [
        "## **3.3: Inverse Folding Prediction** <a name=\"inverse_folding_prediction\"></a>\n",
        "\n",
        "<!-- Predict the amino acid sequence from protein backbone structure.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The protein backbone structure should be provided in .pdb/.cif file format. -->\n",
        "\n",
        "> üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/3.3:-Inverse-Folding-Prediction)\n",
        "\n",
        "\n",
        "<!-- Predict the residue sequence of a structure-aware sequence with masked amino acids (which could be all masked or partially masked).\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Enter a **SA sequence with masked amino acids** into the `sa_seq` input box.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example,\n",
        "**input** is a SA Sequence with masked amino acids:\n",
        "\n",
        "`#d#v#v#v#p#p#p#p#a#p#a#q#k#k#k#k#w`\n",
        "\n",
        "and the **output** predicted by model is an AA Sequence:\n",
        "\n",
        "`MEELGLPDLPPGGVVVV`.\n",
        "\n",
        "<br> -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "43bcba7abdfe423abdef9ec4925216a2",
            "f469cbc8326c4d74882de4a1d860854b",
            "682d12c78c9f4b79a104d3cf9edaf7ec",
            "14ba4e672efd48b2812f1adaef897c4f",
            "7bbeedb8def74786b81633f1d6428f97",
            "a98ba678e25b4fb186a589d416c238b3",
            "703318884d354d5f8fb8fec1de6eccb8",
            "a23903fc8c2d4ed88e77c1f3a41943f7",
            "558c855445594b8485b9fba5c1af3a84",
            "8952766795894d0a98b19f7cc1097701",
            "2fc9c110abd24ed1991bcf1d6bfd0ba5",
            "a81da4401d1243ae9d342b19416439a4"
          ]
        },
        "id": "DT7M_DU2BI7n",
        "outputId": "6c38957b-8566-4a3f-fe0f-0d47d7bd8e10"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.1: Upload .pdb/.cif structure file**\n",
        "\n",
        "#@markdown After clicking the run button, an upload button will appear for you to upload your .pdb/.cif structure file.\n",
        "\n",
        "#@markdown <font face=\"Consolas\" size=2 color='gray'>NoteÔºösince you may not know the AA type, you can simply populate your .pdb/.cif file with any random AA. If you want to predit partial positions given some accurate AA information in other positions, just input the accurate AA in these positions and any random AA in unknown positions.</fonte>\n",
        "\n",
        "#@markdown After uploading is finished, the .pdb/.cif structure will be transformed into the corresponding AA Sequence and Structure (3Di) Sequence.\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"\n",
        "# raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "def get_structure_file():\n",
        "  print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "  dropdown_type = ipywidgets.Dropdown(\n",
        "    value=\"PDB\",\n",
        "    options=[\"PDB\", \"AF2\"],\n",
        "    disabled=False)\n",
        "  dropdown_type.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "  display(dropdown_type)\n",
        "\n",
        "  input_chain = ipywidgets.Text(\n",
        "    value=\"A\",\n",
        "    placeholder=f'Enter the name of chain here',\n",
        "    disabled=False)\n",
        "  input_chain.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "  display(input_chain)\n",
        "\n",
        "  print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "  pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "  return pdb_file_path, pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "\n",
        "backbone_path, stem, dropdown_type, input_chain = get_structure_file()\n",
        "raw_data = (stem, dropdown_type, input_chain)\n",
        "\n",
        "sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "aa_seq = sa_seq[0::2]\n",
        "struc_seq = sa_seq[1::2]\n",
        "\n",
        "# masked_sa_seq = ''\n",
        "# for s in sa_seq[1::2]:\n",
        "#   masked_sa_seq += '#' + s\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "input_aa_seq = ipywidgets.Text(\n",
        "      value=aa_seq,\n",
        "      placeholder='Enter Amino Acid Sequence here',\n",
        "      disabled=False)\n",
        "print(Fore.BLUE+\"Amino Acid Sequence:\"+Style.RESET_ALL)\n",
        "input_aa_seq.layout.width = '500px'\n",
        "display(input_aa_seq)\n",
        "\n",
        "input_struc_seq = ipywidgets.Text(\n",
        "  value=struc_seq,\n",
        "  placeholder='Enter Structure Sequence here',\n",
        "  disabled=False)\n",
        "print(Fore.BLUE+\"Structure Sequence:\"+Style.RESET_ALL)\n",
        "input_struc_seq.layout.width = '500px'\n",
        "display(input_struc_seq)\n",
        "\n",
        "# print(Fore.RED+\"If you want to mask all amino acids and make prediction, simply clear the 'Amino Acid Sequence' box.\")\n",
        "\n",
        "backbone_name = os.path.basename(backbone_path)\n",
        "show_pdb(backbone_path, color=\"chain\").show()\n",
        "print(f\"Backbone visualization of {backbone_name} ({len(struc_seq)} amino acids)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "baiH-BrBl2Ge"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.2: Predict Amino Acid Sequence**\n",
        "\n",
        "#@markdown You can **mask partial or all amino acids** in the AA sequence with '#' at certain positions, allowing the model to make predictions for those masked amino acids.\n",
        "\n",
        "#@markdown <font color=\"red\">If you want to **mask all amino acids** and make prediction, simply clear the 'masked_aa_seq' box.</font>\n",
        "\n",
        "#@markdown | Original AA Sequence | Masked AA Sequence | Description                                |\n",
        "#@markdown | -------------------- | ------------------ | ------------------------------------------ |\n",
        "#@markdown | MEETMKLATMEDTVEYCL   | ME#T#KL#TMEDTVEYCL | Predict the 3rd, 5th, and 8th amino acids. |\n",
        "#@markdown | MEETMKLATMEDTVEYCL   |                    | Predict all amino acids.                   |\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "# #@markdown Click the run button to get the predicted Amino Acid Sequence\n",
        "masked_aa_seq = \"\" # @param {type:\"string\", placeholder:\"mask the amino acids with `#` and then paste the sequence here\"}\n",
        "method = \"multinomial\" # @param [\"argmax\", \"multinomial\"]\n",
        "num_samples = 10 # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown - `method` refers to the prediction method. It could be either \"argmax\" or \"multinomial\".\n",
        "#@markdown   - `argmax` selects the amino acid with the highest probability.\n",
        "#@markdown   - `multinomial` samples an amino acid from the multinomial distribution.\n",
        "\n",
        "\n",
        "#@markdown - `num_samples` refers to the number of output amino acid sequences.\n",
        "\n",
        "save_name = \"predicted_seq\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### Dataset ########################################\n",
        "################################################################################\n",
        "\n",
        "# masked_aa_seq = input_aa_seq.value\n",
        "if masked_aa_seq.strip() == \"\":\n",
        "  masked_aa_seq = \"#\" * len(input_struc_seq.value)\n",
        "\n",
        "masked_struc_seq = input_struc_seq.value\n",
        "\n",
        "# assert len(masked_aa_seq) == len(masked_struc_seq), f\"Please make sure that the amino acid sequence ({len(masked_aa_seq)}) and the structure sequence ({len(masked_struc_seq)}) have the same length.\"\n",
        "# masked_sa_seq = ''.join(a + b for a, b in zip(masked_aa_seq, masked_struc_seq))\n",
        "\n",
        "\n",
        "# if num_samples == 1:\n",
        "#   method = \"argmax\"\n",
        "# elif num_samples > 1:\n",
        "#   method = \"multinomial\"\n",
        "# else:\n",
        "#   raise BaseException(\"\\\"num_samples\\\" should be an integer greater than or equal to 1.\")\n",
        "\n",
        "################################################################################\n",
        "############################### Model ##########################################\n",
        "################################################################################\n",
        "# base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "base_model = \"westlake-repl/SaProt_650M_AF2_inverse_folding\"\n",
        "\n",
        "config = {\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "from saprot.model.saprot.saprot_if_model import SaProtIFModel\n",
        "try:\n",
        "  saprot_if_model\n",
        "except Exception:\n",
        "  saprot_if_model = SaProtIFModel(**config)\n",
        "  tokenizer = saprot_if_model.tokenizer\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  saprot_if_model.to(device)\n",
        "\n",
        "################################################################################\n",
        "############################### Predict ########################################\n",
        "################################################################################\n",
        "\n",
        "pred_aa_seqs = saprot_if_model.predict(masked_aa_seq, masked_struc_seq, method=method, num_samples=num_samples)\n",
        "\n",
        "clear_output(wait=True)\n",
        "print('='*100)\n",
        "print(Fore.BLUE+\"Outputs:\"+Style.RESET_ALL)\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.fasta\"\n",
        "with open(save_path, \"w\") as w:\n",
        "  for i, aa_seq in enumerate(pred_aa_seqs):\n",
        "    print(aa_seq)\n",
        "    w.write(f\">predicted_seq_{i}\\n{aa_seq}\")\n",
        "\n",
        "file_download(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idDDKW2pl2Gf"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.3: Predict the structure of generated sequence**\n",
        "\n",
        "#@markdown <font color=\"red\">**Warning: Please make sure you have enough RAM to run this cell. (Do not use Colab's T4 or local server with less than 64G RAM).**\n",
        "#@markdown\n",
        "#@markdown Otherwise, it will cause an out-of-memory error, and you will have to restart the notebook. We recommend you connect to a runtime\n",
        "#@markdown with more RAM to run the cell properly.</font>\n",
        "\n",
        "#@markdown Click the run button to predict the structure of generated sequence using ESMFold\n",
        "\n",
        "protein_sequence = \"QQVGSQLDLQEESVEYQIFPTQTHQNDTKNVKERLESILERINSIFIPYSQDYVWQEKELSFMISLGLQQGRPHLMGSTHFGDNIDDEWFLVNLLKQLSQVFPQLTAKISDSDGEFLLIEAADLMPKWLIPEGIENRVFVYNGNLMIIPFLLGRYSLIHYQLSKPSLDQAIDLLRNFPEETRASRDQQKLIHNRINGIIKSFLAGTHKAYCYIPRPIATLLKRKPSLVSHAVETFYYRDPIDVKNCRNMEDFTNAERLRVDVRFTRVLYAQLVSQSFNPPKQMGIEAPDPEDKEFKRELLGMKLTCGFAMMAANLLPSTVDPSLNGWAYLEQFKRFRENVEKGNATAKISEPDDQLELISAVRKFLRYIVEDHIDASILKSLLVVELHRQKQMLPESEEAIRKIKKTLLERWNPGWQMSEEYREKTVGQVENGGDSSCEALKSDSKRADLADLDMGRVQDLSRFIDKESRPLERSKISDLQPEVVMGMEQEEDAAAAVSKVYKGGPYLVPIADLKERPEAVHPKATQVVQGELLLISAEDQESKTSNRRVRFGQHGQSQDQAAPMLVGCDRMTALDSIVPEKEEDKVKKGLGYIHLEKSTNSLLTHAIYKIQGSVSHVAARLADRGIDVTSDNVPIKPQTMEEG\" # @param {type:\"string\"}\n",
        "save_name = \"predicted_structure\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Visualization settings\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### LOAD ESMFOLD ################################\n",
        "################################################################################\n",
        "try:\n",
        "  esmfold\n",
        "except Exception:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold.esm = esmfold.esm.half()\n",
        "  esmfold.trunk.set_chunk_size(64)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  esmfold.to(device)\n",
        "\n",
        "################################################################################\n",
        "################################## PREDICT ###################################\n",
        "################################################################################\n",
        "tokenized_input = tokenizer(\n",
        "    [protein_sequence],\n",
        "    return_tensors=\"pt\",\n",
        "    add_special_tokens=False,\n",
        "    max_length=1024,\n",
        "    truncation=True,\n",
        "    )['input_ids']\n",
        "\n",
        "tokenized_input = tokenized_input.to(esmfold.device)\n",
        "with torch.no_grad():\n",
        "  output = esmfold(tokenized_input)\n",
        "\n",
        "################################################################################\n",
        "#################################### SAVE ####################################\n",
        "################################################################################\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.pdb\"\n",
        "pdb = convert_outputs_to_pdb(output)\n",
        "with open(save_path, \"w\") as f:\n",
        "  f.write(\"\".join(pdb))\n",
        "\n",
        "################################################################################\n",
        "################################# VISUALIZE ##################################\n",
        "################################################################################\n",
        "show_pdb(save_path, show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\":\n",
        "  plot_plddt_legend().show()\n",
        "\n",
        "print(\"Predicted structure\")\n",
        "file_download(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PecOgaEEZw4C"
      },
      "outputs": [],
      "source": [
        "#@title **3.3.4: Align proteins using TMalign**\n",
        "\n",
        "#@markdown You can find the **uploaded proteins** from /content/SaprotHub/structures (if you connect to your local server, then the path is /SaprotHub/structures).\n",
        "\n",
        "#@markdown You can find the **predicted proteins** from /content/SaprotHub/output (if you connect to your local server, then the path is /SaprotHub/output).\n",
        "\n",
        "#@markdown Right click the **pdb file** to copy the path and then paste it into the box:\n",
        "pdb_path_1 = \"/SaprotHub/structures/1qsf.pdb\" # @param {type:\"string\"}\n",
        "pdb_path_2 = \"/SaprotHub/output/predicted_structure.pdb\" # @param {type:\"string\"}\n",
        "\n",
        "pdb_path_1 = f\"{root_dir}{pdb_path_1}\"\n",
        "pdb_path_2 = f\"{root_dir}{pdb_path_2}\"\n",
        "\n",
        "assert os.path.exists(pdb_path_1) and os.path.exists(pdb_path_2), \"Input proteins do not exist!\"\n",
        "\n",
        "cmd = f\"{root_dir}/SaprotHub/bin/TMalign {pdb_path_1} {pdb_path_2}\"\n",
        "print(os.popen(cmd).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIIoTQgJBI7o"
      },
      "source": [
        "# **4: (Optional) Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BgsBSLcmBI7o"
      },
      "outputs": [],
      "source": [
        "# @title **4.1: Get Structure-Aware Sequence** <a name=\"get_sa\"></a>\n",
        "\n",
        "#@markdown AA Sequence, UniProt ID, PDB/CIF file -> SA Sequence\n",
        "\n",
        "################################################################################\n",
        "################################ input #########################################\n",
        "################################################################################\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"  # @param [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "\n",
        "if data_type == data_type_list[7]:\n",
        "    # upload and unzip PDB files\n",
        "    print(Fore.BLUE + f\"Please upload your .zip file which contains {data_type} files\" + Style.RESET_ALL)\n",
        "    pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "    print(Fore.BLUE + \"Successfully upload your .zip file!\" + Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        file_names = zip_ref.namelist()\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    uploaded_csv_path = UPLOAD_FILE_HOME / f\"{pdb_zip_path.stem}.csv\"\n",
        "    df = pd.DataFrame(file_names, columns=['sequence'])\n",
        "    df.to_csv(uploaded_csv_path, index=False)\n",
        "    raw_data = uploaded_csv_path\n",
        "\n",
        "else:\n",
        "    raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "################################################################################\n",
        "############################### output #########################################\n",
        "################################################################################\n",
        "\n",
        "if data_type in [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\"]:\n",
        "    def apply(button):\n",
        "        button.disabled = True\n",
        "        button.description = 'Clicked'\n",
        "        button.button_style = ''\n",
        "        sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "        print(\"=\"*100)\n",
        "        print(f\"Amino Acid Sequence: {sa_seq[0::2]}\")\n",
        "        print(f\"Structure Sequence: {sa_seq[1::2]}\")\n",
        "        print(\"=\"*100)\n",
        "        print(\"Please note that structure tokens with a plDDT score lower than 70% are denoted as \\\"#\\\"\")\n",
        "        print(Fore.BLUE + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + Style.RESET_ALL)\n",
        "        print(sa_seq)\n",
        "\n",
        "    button_apply = ipywidgets.Button(\n",
        "        description='Apply',\n",
        "        disabled=False,\n",
        "        button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Apply',\n",
        "        icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "    button_apply.on_click(apply)\n",
        "    button_apply.layout.width = '500px'\n",
        "    display(button_apply)\n",
        "else:\n",
        "    csv_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    print(Fore.BLUE + \"The Structure-Aware Sequences are saved in a .csv file here:\" + Style.RESET_ALL)\n",
        "    print(csv_dataset)\n",
        "    file_download(csv_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDkm_OeABI7o"
      },
      "outputs": [],
      "source": [
        "#@title **4.2: Convert `.fa/.fasta` file to `.csv` file in the data format of \"Multiple AA Sequences\"**\n",
        "\n",
        "\n",
        "#@markdown `.fa/.fasta` -> Multiple AA Sequences `.csv` <a name=\"fa2csv\"></a>\n",
        "\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "aa_seq_dict = { 'sequence': [],\n",
        "                # \"label\": [],\n",
        "                # \"stage\":[]\n",
        "                }\n",
        "\n",
        "fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "assert Path(fa_file_path).name.split('.')[1] in ['fa', 'fasta'], \"Please upload a .fa or .fasta file.\"\n",
        "with fa_file_path.open(\"r\") as fa:\n",
        "  for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "      aa_seq_dict['sequence'].append(str(record.seq))\n",
        "\n",
        "fa_df = pd.DataFrame(aa_seq_dict)\n",
        "print(fa_df[5:])\n",
        "\n",
        "csv_file_path = UPLOAD_FILE_HOME / f'{fa_file_path.stem}.csv'\n",
        "fa_df.to_csv(csv_file_path, index=None)\n",
        "# files.download(csv_file_path)\n",
        "file_download(csv_file_path)\n",
        "\n",
        "################################################################################\n",
        "############################ .fa 2 .csv and split ##############################\n",
        "################################################################################\n",
        "\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# split = ['train', 'valid', 'test']\n",
        "\n",
        "# aa_seq_dict = { 'sequence': [],\n",
        "#                 \"label\": [],\n",
        "#                 \"stage\":[]}\n",
        "\n",
        "\n",
        "\n",
        "# if automatically_split_dataset:\n",
        "\n",
        "#   fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   label = fa_file_path.stem\n",
        "\n",
        "#   with fa_file_path.open(\"r\") as fa:\n",
        "#       for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "#           aa_seq_dict['sequence'].append(str(record.seq))\n",
        "#           aa_seq_dict[\"label\"].append(label)\n",
        "#   weights = [0.8, 0.1, 0.1]\n",
        "#   aa_seq_dict[\"stage\"] = np.random.choice(split, size=len(aa_seq_dict['sequence']), p=weights).tolist()\n",
        "\n",
        "# else:\n",
        "#   for i in range(3):\n",
        "#     print(Fore.BLUE+f\"Please upload a .fa file as your {split[i]} dataset\")\n",
        "#     fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#     label = fa_file_path.stem\n",
        "\n",
        "#     with fa_file_path.open(\"r\") as fa:\n",
        "#         for record in tqdm(SeqIO.parse(fa, 'fasta')):\n",
        "#             aa_seq_dict['sequence'].append(str(record.seq))\n",
        "#             aa_seq_dict[\"label\"].append(label)\n",
        "#             aa_seq_dict[\"stage\"].append(split[i])\n",
        "\n",
        "#     print()\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "# fa_df = pd.DataFrame(aa_seq_dict)\n",
        "# timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "# fa_df.to_csv(f'/content/SaprotHub/upload_files/{timestamp}.csv', index=None)\n",
        "# files.download(f'/content/SaprotHub/upload_files/{timestamp}.csv')\n",
        "# print(fa_df[5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xtadHW9vBI7o"
      },
      "outputs": [],
      "source": [
        "#@title **4.3: Dataset Split**\n",
        "\n",
        "#@markdown Randomly split your .csv dataset <a name=\"split_dataset\"></a>\n",
        "\n",
        "#@markdown Please click the run button to upload your .csv dataset\n",
        "\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "split = ['train', 'valid', 'test']\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "\n",
        "if ('stage' not in dataset_df.columns) or (dataset_df[\"stage\"].nunique()<3):\n",
        "  dataset_df[\"stage\"] = np.random.choice(split, size=len(dataset_df), p=split_ratio).tolist()\n",
        "\n",
        "dataset_df.to_csv(csv_dataset_path, index=None)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14ba4e672efd48b2812f1adaef897c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7bbeedb8def74786b81633f1d6428f97",
            "placeholder": "Enter the name of chain here",
            "style": "IPY_MODEL_a98ba678e25b4fb186a589d416c238b3",
            "value": "A"
          }
        },
        "2fc9c110abd24ed1991bcf1d6bfd0ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43bcba7abdfe423abdef9ec4925216a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "PDB",
              "AF2"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_f469cbc8326c4d74882de4a1d860854b",
            "style": "IPY_MODEL_682d12c78c9f4b79a104d3cf9edaf7ec"
          }
        },
        "558c855445594b8485b9fba5c1af3a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_703318884d354d5f8fb8fec1de6eccb8",
            "placeholder": "Enter Amino Acid Sequence here",
            "style": "IPY_MODEL_a23903fc8c2d4ed88e77c1f3a41943f7",
            "value": "ARIRHVQGDITEFQGDAIVNAANNYLKLGAGVAGAILRKGGPSIQEECDRIGKIRVGEAAVTGAGNLPVRYVIHAAVLGDEPASLETVRKATKSALEKAVELGLKTVAFTALGAWVGGLPAEAVLRVMLEEIKKAPDTLEVTGVHGTEKSAEAARRALLEHHHH"
          }
        },
        "682d12c78c9f4b79a104d3cf9edaf7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "703318884d354d5f8fb8fec1de6eccb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "7bbeedb8def74786b81633f1d6428f97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "8952766795894d0a98b19f7cc1097701": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "a23903fc8c2d4ed88e77c1f3a41943f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a81da4401d1243ae9d342b19416439a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8952766795894d0a98b19f7cc1097701",
            "placeholder": "Enter Structure Sequence here",
            "style": "IPY_MODEL_2fc9c110abd24ed1991bcf1d6bfd0ba5",
            "value": "daeaedadqqlqdqfaeeeaqaaqqrdqdddvsvvqcvqqhdvsvvvsvvvggddqlawdwdqshngnhgiyiyqhqhhphagdlvsllssllnslvvcvvvptaeyeyeqhcvpprvddsvssvvssvvnvvvhdnshyyyyydndpvvrvvvvvvvvvvvvd"
          }
        },
        "a98ba678e25b4fb186a589d416c238b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f469cbc8326c4d74882de4a1d860854b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
