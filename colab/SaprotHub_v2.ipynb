{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SaprotHub: Making Protein Modeling Accessible to All Biologists**\n",
        "\n",
        "<a href=\"https://www.biorxiv.org/content/10.1101/2024.05.24.595648v3\"><img src=\"https://img.shields.io/badge/Paper-bioRxiv-green\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/SaProtHub\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-red?label=SaprotHub\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://github.com/westlake-repl/SaprotHub\"><img src=\"https://img.shields.io/badge/Github-black?logo=github\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://theopmc.github.io/\"><img src=\"https://img.shields.io/badge/Website-OPMC-yellow\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://cbirt.net/no-coding-required-saprothub-brings-protein-modeling-to-every-biologist/\" alt=\"blog\"><img src=\"https://img.shields.io/badge/Blog-Medium-purple\" /></a>\n",
        "<a href=\"https://x.com/sokrypton/status/1795525127653986415\"><img src=\"https://img.shields.io/badge/Twitter-blue?logo=twitter\" style=\"max-width: 100%;\"></a>\n",
        "\n",
        "\n",
        "- This is **ColabSaprot**, the Colab version of [SaProt](https://github.com/westlake-repl/SaProt) - a pre-trained protein language model designed for [various protein prediction tasks](https://github.com/westlake-repl/SaprotHub/blob/main/task_list.md).\n",
        "\n",
        "- We have built **SaprotHub** ([website](https://huggingface.co/SaProtHub), [paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v4)), a repository for community-shared models. Explore them through our specialized [search engine](https://huggingface.co/spaces/SaProtHub/SaprotHub-search).\n",
        "\n",
        "- Visit and contribute to [OPMC](https://theopmc.github.io/).\n",
        "\n",
        "<font color=\"red\">‚ö†Ô∏è **New Notice**: Google Colab recommends Chrome browser. ColabSaprot may not work properly in Safari. Contact {sujin, yuanfajie}@westlake.edu.cn for any inquiries about ColabSaprot. We will try to respond within 24 hours.</font>"
      ],
      "metadata": {
        "id": "i7atSR7woQ-b"
      },
      "id": "i7atSR7woQ-b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to start (üé• [tutorial](https://www.youtube.com/watch?v=nmLtjlCI_7M))\n",
        "\n",
        "## Follow below steps to switch to GPU. Once connected, click the run-button <img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/run_button.png?raw=true' height='25px' width='25px' align='center'> to run ColabSaprot\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Switch_Runtime_2.png?raw=true\" align=\"center\">\n",
        "\n",
        "### GPU selection guide\n",
        "\n",
        "- **T4 GPU:** <font color=\"red\">**Free to use**</font>. Suitable for low-compute tasks such as\n",
        "training Saprot 35M model, protein property prediction, mutational effect prediction and protein sequence design. <font color=\"red\">**Note this GPU is unstable so you may lose connection or have memory issues during long-time training. To avoid this problem you could switch to L4 or A100 GPU.**\n",
        "</font>\n",
        "\n",
        "- **L4 GPU (need Colab Pro):** It has larger memory. Could be used for training Saprot 650M\n",
        "model but is slower than A100 GPU.\n",
        "\n",
        "- **A100 GPU (need Colab Pro):** Much more powerful GPU. Suitable for training large\n",
        "model such as Saprot 650M model."
      ],
      "metadata": {
        "id": "paX3gluumu7J"
      },
      "id": "paX3gluumu7J"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Click the run-button**\n",
        "\n",
        "#@markdown ### Hint:\n",
        "#@markdown - It takes 3-8 minutes for the initial installation.\n",
        "#@markdown - The run-button's \"inactive\" state (<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/run_button.png?raw=true' height='25px' width='25px' align='center'>) indicates the program is on standby and awaits your click to start. When clicked, the run-button transforms to show a dynamic loading animation (<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/run_button_working.png?raw=true' height='25px' width='25px' align='center'>), confirming the program is launching. Once the interface loads completely, you can begin using the program.\n",
        "\n",
        "#@markdown ### <font color=red> Important: </font>\n",
        "#@markdown - <font color=red>If changing runtime type or encountering connection issues, first click the run-button to stop the program. Once the run-button transforms from (<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/run_button_working.png?raw=true' height='25px' width='25px' align='center'>) to (<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/run_button.png?raw=true' height='25px' width='25px' align='center'>), you can reconnect and restart, see [here](https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#2-how-can-i-reconnect-when-encountering-connection-issues).</font>\n",
        "#@markdown - <font color=red>For any unknown issues, clear your session via Runtime > \"Manage Sessions\", see [here](https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#a-simple-way-to-handle-unexpected-issues).</font>\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "# Check whether the server is local or from google cloudA model id example\n",
        "root_dir = os.getcwd()\n",
        "\n",
        "from google.colab import output\n",
        "# output.enable_custom_widget_manager()\n",
        "\n",
        "try:\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "  import saprot\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################## global ######################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "\n",
        "  import ipywidgets\n",
        "  import pandas as pd\n",
        "  import torch\n",
        "  import numpy as np\n",
        "  import lmdb\n",
        "  import base64\n",
        "  import copy\n",
        "  import os\n",
        "  import json\n",
        "  import zipfile\n",
        "  import yaml\n",
        "  import argparse\n",
        "  import pprint\n",
        "  import subprocess\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import shutil\n",
        "  import torch.nn.functional as F\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "  from loguru import logger\n",
        "  from easydict import EasyDict\n",
        "  from colorama import init, Fore, Back, Style\n",
        "  from IPython.display import clear_output\n",
        "  from huggingface_hub import snapshot_download\n",
        "  from ipywidgets import HTML\n",
        "  from IPython.display import display\n",
        "  from google.colab import widgets\n",
        "  from google.colab import files\n",
        "  from pathlib import Path\n",
        "  from tqdm import tqdm\n",
        "  from datetime import datetime\n",
        "  from transformers import AutoTokenizer, EsmForProteinFolding, EsmTokenizer\n",
        "  from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "  from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "  from string import ascii_uppercase,ascii_lowercase\n",
        "  from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "  from saprot.data.parse import get_chain_ids\n",
        "  from saprot.scripts.training import my_load_model\n",
        "  from safetensors import safe_open\n",
        "\n",
        "  print(\"SaProt is installed successfully!\")\n",
        "\n",
        "except ImportError:\n",
        "  print(\"Installing SaProt...\")\n",
        "  os.system(f\"rm -rf {root_dir}/SaprotHub\")\n",
        "  # !rm -rf /content/SaprotHub/\n",
        "\n",
        "  !git clone https://github.com/westlake-repl/SaprotHub.git\n",
        "\n",
        "  # !pip install /content/SaprotHub/saprot-0.4.7-py3-none-any.whl\n",
        "  os.system(f\"pip install -r {root_dir}/SaprotHub/requirements.txt\")\n",
        "  # !pip install -r /content/SaprotHub/requirements.txt\n",
        "\n",
        "  os.system(f\"pip install {root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/LMDB\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/bin\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/output\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/datasets\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/token_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/structures\")\n",
        "\n",
        "  os.system(\"pip install jupyter_ui_poll\")\n",
        "\n",
        "  # !mkdir -p /content/SaprotHub/LMDB\n",
        "  # !mkdir -p /content/SaprotHub/bin\n",
        "  # !mkdir -p /content/SaprotHub/output\n",
        "  # !mkdir -p /content/SaprotHub/datasets\n",
        "  # !mkdir -p /content/SaprotHub/adapters/classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/token_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/structures\n",
        "\n",
        "  # !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  # os.system(\n",
        "  #   f\"wget 'https://drive.usercontent.google.com/download?id=1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re&export=download&authuser=0' -O {root_dir}/SaprotHub/bin/foldseek\"\n",
        "  # )\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "  # !chmod +x /content/SaprotHub/bin/foldseek\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "\n",
        "  # !mv /content/SaprotHub/ColabSaprotSetup/foldseek /content/SaprotHub/bin/\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import lmdb\n",
        "import base64\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import yaml\n",
        "import argparse\n",
        "import pprint\n",
        "import subprocess\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from loguru import logger\n",
        "from easydict import EasyDict\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import snapshot_download\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, EsmForProteinFolding, EsmTokenizer\n",
        "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from saprot.data.parse import get_chain_ids\n",
        "from saprot.scripts.training import my_load_model\n",
        "from safetensors import safe_open\n",
        "\n",
        "DATASET_HOME = Path(f'{root_dir}/SaprotHub/datasets')\n",
        "ADAPTER_HOME = Path(f'{root_dir}/SaprotHub/adapters')\n",
        "STRUCTURE_HOME = Path(f\"{root_dir}/SaprotHub/structures\")\n",
        "LMDB_HOME = Path(f'{root_dir}/SaprotHub/LMDB')\n",
        "OUTPUT_HOME = Path(f'{root_dir}/SaprotHub/output')\n",
        "UPLOAD_FILE_HOME = Path(f'{root_dir}/SaprotHub/upload_files')\n",
        "FOLDSEEK_PATH = Path(f\"{root_dir}/SaprotHub/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"SaprotHub Dataset\",\n",
        "                  \"A pair of AA Sequences\",\n",
        "                  \"A pair of SA Sequences\",\n",
        "                  \"A pair of UniProt IDs\",\n",
        "                  \"A pair of PDB/CIF Structures\",\n",
        "                  \"Multiple pairs of AA Sequences\",\n",
        "                  \"Multiple pairs of SA Sequences\",\n",
        "                  \"Multiple pairs of UniProt IDs\",\n",
        "                  \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_single = [\n",
        "    \"Single AA Sequence\",\n",
        "    \"Single SA Sequence\",\n",
        "    \"Single UniProt ID\",\n",
        "    \"Single PDB/CIF Structure\",\n",
        "    \"A pair of AA Sequences\",\n",
        "    \"A pair of SA Sequences\",\n",
        "    \"A pair of UniProt IDs\",\n",
        "    \"A pair of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_multiple = [\n",
        "    \"Multiple AA Sequences\",\n",
        "    \"Multiple SA Sequences\",\n",
        "    \"Multiple UniProt IDs\",\n",
        "    \"Multiple PDB/CIF Structures\",\n",
        "    \"Multiple pairs of AA Sequences\",\n",
        "    \"Multiple pairs of SA Sequences\",\n",
        "    \"Multiple pairs of UniProt IDs\",\n",
        "    \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Protein-level Classification\": \"classification\",\n",
        "  \"Residue-level Classification\" : \"token_classification\",\n",
        "  \"Protein-level Regression\" : \"regression\",\n",
        "  \"Protein-protein Classification\": \"pair_classification\",\n",
        "  \"Protein-protein Regression\": \"pair_regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_model\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_dataset\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_dataset\",\n",
        "}\n",
        "training_data_type_dict = {\n",
        "  \"Single AA Sequence\": \"AA\",\n",
        "  \"Single SA Sequence\": \"SA\",\n",
        "  \"Single UniProt ID\": \"SA\",\n",
        "  \"Single PDB/CIF Structure\": \"SA\",\n",
        "  \"Multiple AA Sequences\": \"AA\",\n",
        "  \"Multiple SA Sequences\": \"SA\",\n",
        "  \"Multiple UniProt IDs\": \"SA\",\n",
        "  \"Multiple PDB/CIF Structures\": \"SA\",\n",
        "  \"SaprotHub Dataset\": \"SA\",\n",
        "  \"A pair of AA Sequences\": \"AA\",\n",
        "  \"A pair of SA Sequences\": \"SA\",\n",
        "  \"A pair of UniProt IDs\": \"SA\",\n",
        "  \"A pair of PDB/CIF Structures\": \"SA\",\n",
        "  \"Multiple pairs of AA Sequences\": \"AA\",\n",
        "  \"Multiple pairs of SA Sequences\": \"SA\",\n",
        "  \"Multiple pairs of UniProt IDs\": \"SA\",\n",
        "  \"Multiple pairs of PDB/CIF Structures\": \"SA\",\n",
        "}\n",
        "\n",
        "\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### adapters #######################################\n",
        "################################################################################\n",
        "def get_adapters_list(task_type=None):\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    if task_type:\n",
        "      raise\n",
        "      # for file_path in (ADAPTER_HOME / task_type).glob('**/adapter_config.json'):\n",
        "        # adapters_list.append(file_path.relative_to(ADAPTER_HOME / task_type).parent)\n",
        "    else:\n",
        "      for file_path in ADAPTER_HOME.glob('Local/**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.relative_to(ADAPTER_HOME).parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    # options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    options=adapters_list,\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def adapters_selectmultiple(adapters_list):\n",
        "  selectmulitiple = ipywidgets.SelectMultiple(\n",
        "  # options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "  options=adapters_list,\n",
        "  value=[],\n",
        "  #rows=10,\n",
        "  placeholder='Select multiple models',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(selectmulitiple)\n",
        "\n",
        "  return selectmulitiple\n",
        "\n",
        "def adapters_textmultiple(adapters_list):\n",
        "  textmultiple = ipywidgets.Text(\n",
        "  value=None,\n",
        "  placeholder='Enter multiple SaprotHub Model IDs, separated by commas.',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(textmultiple)\n",
        "\n",
        "  return textmultiple\n",
        "\n",
        "\n",
        "def select_adapter_from(task_type, use_model_from):\n",
        "  adapters_list = get_adapters_list(task_type)\n",
        "\n",
        "  if use_model_from == 'Trained by yourself on ColabSaprot':\n",
        "    return adapters_dropdown(adapters_list)\n",
        "\n",
        "  elif use_model_from == 'Shared by peers on SaprotHub':\n",
        "    print(Fore.BLUE+\"SaprotHub Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Saved in your local computer\":\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    if task_type:\n",
        "      adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    else:\n",
        "      adapter_upload_path = ADAPTER_HOME / \"Local\"\n",
        "\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "\n",
        "    # # 4. move to correct folder\n",
        "    # num_labels, task_type = get_num_labels_and_task_type_by_adapter(adapter_path)\n",
        "    # shutil.move(adapter_path, ADAPTER_HOME / task_type)\n",
        "\n",
        "    return EasyDict({\"value\":  f\"Local/{adapter_zip_path.stem}\"})\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ColabSaprot\":\n",
        "    # 1. select the list of adapters\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Multiple values can be selected with \\\"shift\\\" and/or \\\"ctrl\\\" (or \\\"command\\\") pressed and mouse clicks or arrow keys.\"+Style.RESET_ALL)\n",
        "    return adapters_selectmultiple(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "    # 1. enter the list of adapters\n",
        "    print(Fore.BLUE+f\"SaprotHub Model IDs, separated by commas ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_textmultiple(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "def read_csv_dataset(uploaded_csv_path):\n",
        "  df = pd.read_csv(uploaded_csv_path)\n",
        "  df.columns = df.columns.str.lower()\n",
        "  return df\n",
        "\n",
        "def check_column_label_and_stage(csv_dataset_path):\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "  assert {'label', 'stage'}.issubset(df.columns), f\"Make sure your CSV dataset includes both `label` and `stage` columns!\\nCurrent columns: {df.columns}\"\n",
        "  column_values = set(df['stage'].unique())\n",
        "  assert all(value in column_values for value in ['train', 'valid', 'test']), f\"Ensure your dataset includes samples for all three stages: `train`, `valid` and `test`.\\nCurrent columns: {df.columns}\"\n",
        "\n",
        "def get_data_type(csv_dataset_path):\n",
        "  # AA, SA, Pair AA, Pair SA\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "\n",
        "  # AA, SA\n",
        "  if 'protein' in df.columns:\n",
        "    second_token = df.loc[0, 'protein'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  # Pair AA, Pair SA\n",
        "  elif 'protein_1' in df.columns and 'protein_2' in df.columns:\n",
        "    second_token = df.loc[0, 'protein_1'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple pairs of AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple pairs of SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  else:\n",
        "    print(df)\n",
        "    raise Exception(\"Please check your dataset format. You are expected to strictly follow the examples we provide.\")\n",
        "\n",
        "def check_task_type_and_data_type(original_task_type, data_type):\n",
        "  if \"Protein-protein\" in original_task_type:\n",
        "    assert data_type == \"SaprotHub Dataset\" or \"pair\" in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please use Pair Sequence Datset for {original_task_type} task!\"\n",
        "  else:\n",
        "    assert \"pair\" not in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please avoid using the Pair Sequence Dataset({data_type}) for the {original_task_type} task!\"\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == 'Single PDB/CIF Structure':\n",
        "    input_chain = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain here',\n",
        "      disabled=False)\n",
        "    input_chain.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain (to be extracted from the structure):\"+Style.RESET_ALL)\n",
        "    display(input_chain)\n",
        "\n",
        "    print(Fore.BLUE+\"Click to upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    print(input_chain)\n",
        "    return pdb_file_path.stem, input_chain\n",
        "\n",
        "  # 4-7 & 13-16. Multiple Sequences\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type in ['Multiple PDB/CIF Structures', 'Multiple pairs of PDB/CIF Structures']:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Copy and paste the SaprotHub Dataset ID here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "  # 9-11. A pair of seq\n",
        "  elif data_type in [\"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\"]:\n",
        "    print()\n",
        "\n",
        "    seq_type = data_type[len(\"A pair of \"):-1]\n",
        "\n",
        "    input_seq1 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 1 here',\n",
        "      disabled=False)\n",
        "    input_seq1.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 1:\"+Style.RESET_ALL)\n",
        "    display(input_seq1)\n",
        "\n",
        "    input_seq2 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 2 here',\n",
        "      disabled=False)\n",
        "    input_seq2.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 2:\"+Style.RESET_ALL)\n",
        "    display(input_seq2)\n",
        "\n",
        "    return (input_seq1, input_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  elif data_type == 'A pair of PDB/CIF Structures':\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type1 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The first structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type1)\n",
        "\n",
        "    input_chain1 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the first structure here',\n",
        "      disabled=False)\n",
        "    input_chain1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the first structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain1)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path1 = upload_file(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    dropdown_type2 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The second structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type2)\n",
        "\n",
        "    input_chain2 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the second structure here',\n",
        "      disabled=False)\n",
        "    input_chain2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the second structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain2)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path2 = upload_file(STRUCTURE_HOME)\n",
        "    return (pdb_file_path1.stem, dropdown_type1, input_chain1, pdb_file_path2.stem, dropdown_type2, input_chain2)\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # Multiple sequences\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  if data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=DATASET_HOME / REPO_ID)\n",
        "    csv_dataset_path = DATASET_HOME / REPO_ID / 'dataset.csv'\n",
        "    assert csv_dataset_path.exists(), f\"Can't find {csv_dataset_path}\"\n",
        "    protein_df = read_csv_dataset(csv_dataset_path)\n",
        "\n",
        "    data_type = get_data_type(csv_dataset_path)\n",
        "\n",
        "    return get_SA_sequence_by_data_type(data_type, csv_dataset_path)\n",
        "\n",
        "    # # AA, SA\n",
        "    # if data_type == \"Multiple AA Sequences\":\n",
        "    #   for index, value in protein_df['sequence'].items():\n",
        "    #     sa_seq = ''\n",
        "    #     for aa in value:\n",
        "    #       sa_seq += aa + '#'\n",
        "    #     protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "    # # Pair AA, Pair SA\n",
        "    # elif data_type in [\"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\"]:\n",
        "    #   for i in ['1', '2']:\n",
        "    #     if data_type == \"Multiple pairs of AA Sequences\":\n",
        "    #       for index, value in protein_df[f'sequence_{i}'].items():\n",
        "    #         sa_seq = ''\n",
        "    #         for aa in value:\n",
        "    #           sa_seq += aa + '#'\n",
        "    #         protein_df.at[index, f'sequence_{i}'] = sa_seq\n",
        "\n",
        "    #     protein_df[f'name_{i}'] = f'name_{i}'\n",
        "    #     protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "    # protein_df.to_csv(csv_dataset_path, index=None)\n",
        "\n",
        "    # return csv_dataset_path\n",
        "\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path\n",
        "    protein_df = read_csv_dataset(uploaded_csv_path)\n",
        "\n",
        "    if 'pair' in data_type:\n",
        "      assert {'protein_1', 'protein_2'}.issubset(protein_df.columns), f\"The CSV dataset ({uploaded_csv_path}) must contain `protein_1` and `protein_2` columns. \\n Current columns:{protein_df.columns}\"\n",
        "    else:\n",
        "      assert 'protein' in protein_df.columns, f\"The CSV Dataset({uploaded_csv_path}) must contain a `protein` column. \\n Current columns:{protein_df.columns}\"\n",
        "\n",
        "    # 4. Multiple AA Sequences\n",
        "    if data_type == 'Multiple AA Sequences':\n",
        "      for index, value in protein_df['protein'].items():\n",
        "        sa_seq = ''\n",
        "        for aa in value:\n",
        "          sa_seq += aa + '#'\n",
        "        protein_df.at[index, 'protein'] = sa_seq\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 5. Multiple SA Sequences\n",
        "    elif data_type == 'Multiple SA Sequences':\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 6. Multiple UniProt IDs\n",
        "    elif data_type == 'Multiple UniProt IDs':\n",
        "      protein_list = protein_df.loc[:, 'protein'].tolist()\n",
        "      uniprot2pdb(protein_list)\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['protein'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 7. Multiple PDB/CIF Structures\n",
        "    elif data_type == 'Multiple PDB/CIF Structures':\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "      protein_list = []\n",
        "      for row_tuple in protein_df.itertuples(index=False):\n",
        "        assert row_tuple.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "        protein_list.append(row_tuple)\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['protein'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 13. Pair Multiple AA Sequences\n",
        "    elif data_type == \"Multiple pairs of AA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        for index, value in protein_df[f'protein_{i}'].items():\n",
        "          sa_seq = ''\n",
        "          for aa in value:\n",
        "            sa_seq += aa + '#'\n",
        "          protein_df.at[index, f'protein_{i}'] = sa_seq\n",
        "\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 14. Pair Multiple SA Sequences\n",
        "    elif data_type == \"Multiple pairs of SA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 15. Pair Multiple UniProt IDs\n",
        "    elif data_type == \"Multiple pairs of UniProt IDs\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = protein_df.loc[:, f'protein_{i}'].tolist()\n",
        "        uniprot2pdb(protein_list)\n",
        "        protein_df[f'name_{i}'] = protein_list\n",
        "        protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        protein_df[f'protein_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    elif data_type ==  \"Multiple pairs of PDB/CIF Structures\":\n",
        "      # columns: sequence_1, sequence_2, type_1, type_2, chain_1, chain_2, label, stage\n",
        "\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = []\n",
        "        for index, row in protein_df.iterrows():\n",
        "          assert row[f\"type_{i}\"] in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "          row_tuple = (row[f\"protein_{i}\"], row[f\"type_{i}\"], row[f\"chain_{i}\"])\n",
        "          protein_list.append(row_tuple)\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        # add name column, del type column\n",
        "        protein_df[f'name_{i}'] = protein_df[f'protein_{i}'].apply(lambda x: x.split('.')[0])\n",
        "        protein_df.drop(f\"type_{i}\", axis=1, inplace=True)\n",
        "        protein_df[f'protein_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "\n",
        "      # columns: name_1, name_2, chain_1, chain_2, sequence_1, sequence_2, label, stage\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "  else:\n",
        "    # 0. Single AA Sequence\n",
        "    if data_type == 'Single AA Sequence':\n",
        "      input_seq = raw_data\n",
        "      aa_seq = input_seq.value\n",
        "\n",
        "      sa_seq = ''\n",
        "      for aa in aa_seq:\n",
        "          sa_seq += aa + '#'\n",
        "      return sa_seq\n",
        "\n",
        "    # 1. Single SA Sequence\n",
        "    elif data_type == 'Single SA Sequence':\n",
        "      input_seq = raw_data\n",
        "      sa_seq = input_seq.value\n",
        "\n",
        "      return sa_seq\n",
        "\n",
        "    # 2. Single UniProt ID\n",
        "    elif data_type == 'Single UniProt ID':\n",
        "      input_seq = raw_data\n",
        "      uniprot_id = input_seq.value\n",
        "\n",
        "\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\")]\n",
        "      uniprot2pdb([protein_list[0][0]])\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 3. Single PDB/CIF Structure\n",
        "    elif data_type == 'Single PDB/CIF Structure':\n",
        "      uniprot_id = raw_data[0]\n",
        "      chain = raw_data[1].value\n",
        "\n",
        "      protein_list = [(uniprot_id, chain)]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      assert len(seqs)>0, \"Unable to convert to SA sequence. Please check the `type`, `chain`, and `.pdb/.cif file`.\"\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 9. Pair Single AA Sequences\n",
        "    elif data_type == \"A pair of AA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 10. Pair Single SA Sequences\n",
        "    elif data_type ==  \"A pair of SA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 11. Pair Single UniProt IDs\n",
        "    elif data_type ==  \"A pair of UniProt IDs\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 12. Pair Single PDB/CIF Structure\n",
        "    elif data_type == \"A pair of PDB/CIF Structures\":\n",
        "      uniprot_id1 = raw_data[0]\n",
        "      struc_type1 = raw_data[1].value\n",
        "      chain1 = raw_data[2].value\n",
        "\n",
        "      protein_list1 = [(uniprot_id1, struc_type1, chain1)]\n",
        "      mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs1 = mprs1.run()\n",
        "      sa_seq1 = seqs1[0].split('\\t')[1]\n",
        "\n",
        "      uniprot_id2 = raw_data[3]\n",
        "      struc_type2 = raw_data[4].value\n",
        "      chain2 = raw_data[5].value\n",
        "\n",
        "      protein_list2 = [(uniprot_id2, struc_type2, chain2)]\n",
        "      mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs2 = mprs2.run()\n",
        "      sa_seq2 = seqs2[0].split('\\t')[1]\n",
        "      return sa_seq1, sa_seq2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "  file_path = row_tuple[0]\n",
        "  chain = row_tuple[1]\n",
        "  plddt_mask= True\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "  try:\n",
        "    seq = get_struc_seq(FOLDSEEK_PATH, file_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    writer.write(f\"{file_path}\\t{seq}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    raise e\n",
        "\n",
        "\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "          \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "          \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "          \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "          \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "def convert_outputs_to_pdb(outputs):\n",
        "\tfinal_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
        "\toutputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
        "\tfinal_atom_positions = final_atom_positions.cpu().numpy()\n",
        "\tfinal_atom_mask = outputs[\"atom37_atom_exists\"]\n",
        "\tpdbs = []\n",
        "\toutputs[\"plddt\"] *= 100\n",
        "\n",
        "\tfor i in range(outputs[\"aatype\"].shape[0]):\n",
        "\t\taa = outputs[\"aatype\"][i]\n",
        "\t\tpred_pos = final_atom_positions[i]\n",
        "\t\tmask = final_atom_mask[i]\n",
        "\t\tresid = outputs[\"residue_index\"][i] + 1\n",
        "\t\tpred = OFProtein(\n",
        "\t\t    aatype=aa,\n",
        "\t\t    atom_positions=pred_pos,\n",
        "\t\t    atom_mask=mask,\n",
        "\t\t    residue_index=resid,\n",
        "\t\t    b_factors=outputs[\"plddt\"][i],\n",
        "\t\t    chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
        "\t\t)\n",
        "\t\tpdbs.append(to_pdb(pred))\n",
        "\treturn pdbs\n",
        "\n",
        "\n",
        "# This function is copied from ColabFold!\n",
        "def show_pdb(path, show_sidechains=False, show_mainchains=False, color=\"lddt\"):\n",
        "  file_type = str(path).split(\".\")[-1]\n",
        "  if file_type == \"cif\":\n",
        "    file_type == \"mmcif\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(path,'r').read(),file_type)\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(get_chain_ids(path))\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###############   Download file to local computer   ##################\n",
        "################################################################################\n",
        "def file_download(path: str):\n",
        "  # Automatically download file if the server is from google cloud.\n",
        "  if root_dir == \"/content\":\n",
        "    btn = ipywidgets.Button(\n",
        "      description=\"Download File\",\n",
        "      button_style=\"success\",\n",
        "      )\n",
        "\n",
        "    btn.on_click(lambda b: files.download(path))\n",
        "    display(btn)\n",
        "\n",
        "    files.download(path)\n",
        "\n",
        "  else:\n",
        "    with open(path, \"rb\") as r:\n",
        "      res = r.read()\n",
        "\n",
        "    #FILE\n",
        "    filename = os.path.basename(path)\n",
        "    b64 = base64.b64encode(res)\n",
        "    payload = b64.decode()\n",
        "\n",
        "    #BUTTONS\n",
        "    html_buttons = '''<html>\n",
        "    <head>\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    </head>\n",
        "    <body>\n",
        "    <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "    </a>\n",
        "    </body>\n",
        "    </html>\n",
        "    '''\n",
        "\n",
        "    html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "    display(HTML(html_button))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################ MODEL INFO #######################################\n",
        "################################################################################\n",
        "def get_base_model(adapter_path):\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    adapter_config_dict = json.load(f)\n",
        "    base_model = adapter_config_dict['base_model_name_or_path']\n",
        "    if 'SaProt_650M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    elif 'SaProt_35M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    else:\n",
        "      raise RuntimeError(\"Please ensure the base model is \\\"SaProt_650M_AF2\\\" or \\\"SaProt_35M_AF2\\\"\")\n",
        "  return base_model\n",
        "\n",
        "def check_training_data_type(adapter_path, data_type):\n",
        "  metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "  if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      required_training_data_type = metadata['training_data_type']\n",
        "  else:\n",
        "    required_training_data_type = \"SA\"\n",
        "\n",
        "  if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with AA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) includes structural information, which will not be used for predictions.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "  elif (required_training_data_type == \"SA\") and (\"AA\" in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with SA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) does not include structural information, which may lead to weak prediction performance.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"If you only have the amino acid sequence, we strongly recommend using AF2 to predict the structure and generate a PDB file before prediction.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "\n",
        "  return required_training_data_type\n",
        "\n",
        "def mask_struc_token(sequence):\n",
        "    return ''.join('#' if i % 2 == 1 and char.islower() else char for i, char in enumerate(sequence))\n",
        "\n",
        "def get_num_labels_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "        file_path = adapter_path / 'adapter_model.safetensors'\n",
        "        with safe_open(file_path, framework=\"pt\") as f:\n",
        "          if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "          elif 'base_model.model.classifier.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "          else:\n",
        "              raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path)\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in f.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    return num_labels\n",
        "\n",
        "def get_num_labels_and_task_type_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    task_type = None\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "      file_path = adapter_path / 'adapter_model.safetensors'\n",
        "      with safe_open(file_path, framework=\"pt\") as f:\n",
        "        if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "          tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "        elif 'base_model.model.classifier.bias' in f.keys():\n",
        "          task_type = 'token_classification'\n",
        "          tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "        elif 'base_model.model.classifier.2.bias' in f.keys():\n",
        "          task_type = 'pair'\n",
        "          tensor = f.get_tensor('base_model.model.classifier.2.bias')\n",
        "        else:\n",
        "          raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path, map_location=\"cpu\")\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in state_dict.keys():\n",
        "        task_type = 'token_classification'\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      elif 'base_model.model.classifier.2.bias' in state_dict.keys():\n",
        "        task_type = 'pair'\n",
        "        tensor = state_dict['base_model.model.classifier.2.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    if task_type != 'token_classification':\n",
        "      if task_type != \"pair\":\n",
        "        if num_labels > 1:\n",
        "          task_type = 'classification'\n",
        "        elif num_labels == 1:\n",
        "          task_type = 'regression'\n",
        "\n",
        "      else:\n",
        "        if num_labels > 1:\n",
        "          task_type = 'pair_classification'\n",
        "        elif num_labels == 1:\n",
        "          task_type = 'pair_regression'\n",
        "\n",
        "    return num_labels, task_type\n",
        "\n",
        "################################################################################\n",
        "############################ INFO ##############################################\n",
        "################################################################################\n",
        "clear_output(wait=True)\n",
        "\n",
        "import markdown\n",
        "import ipywidgets\n",
        "import random\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from jupyter_ui_poll import ui_events\n",
        "from ipywidgets import Button, Label, HTML, Layout\n",
        "from IPython.display import display, clear_output\n",
        "from functools import partial\n",
        "from saprot.utils.foldseek_util import get_struc_seq\n",
        "from saprot.utils.generate_lmdb import get_length\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#            Backend functions             #\n",
        "######################################################################\n",
        "def red_print(text, **kwargs):\n",
        "  print(f\"\\033[31m{text}\\033[0m\", **kwargs)\n",
        "\n",
        "\n",
        "def get_upload_file_path(upload_items):\n",
        "  upload_size_box, _, google_drive_id, upload_button = upload_items[:4]\n",
        "  if upload_size_box.value == \"No\":\n",
        "    # User mannually uploads file\n",
        "    save_path = f\"{UPLOAD_FILE_HOME}/{upload_button.description}\"\n",
        "    assert os.path.exists(save_path), \"Please upload your file!\"\n",
        "\n",
        "  else:\n",
        "    # Disable all widgets\n",
        "    for widget in ipywidgets.Widget.widgets.values():\n",
        "      widget.disabled = True\n",
        "\n",
        "    google_drive_dir = f\"{UPLOAD_FILE_HOME}/google_drive/\"\n",
        "    os.makedirs(google_drive_dir, exist_ok=True)\n",
        "\n",
        "    # Download file from Google Drive\n",
        "    print(\"Downloading file from Google Drive...\")\n",
        "    file_link = google_drive_id.value\n",
        "    cmd = f\"cd {google_drive_dir} && gdown --fuzzy {file_link}\"\n",
        "    os.system(cmd)\n",
        "\n",
        "    # Enable all widgets\n",
        "    for widget in ipywidgets.Widget.widgets.values():\n",
        "      widget.disabled = False\n",
        "\n",
        "    # Check file validity\n",
        "    assert len(os.listdir(google_drive_dir)) > 0, \"Download failed. Please ensure your link is correct and publicly available!\"\n",
        "\n",
        "    file = os.listdir(google_drive_dir)[0]\n",
        "    save_path = f\"{UPLOAD_FILE_HOME}/{file}\"\n",
        "    cmd = f\"mv '{google_drive_dir}/{file}' '{save_path}'\"\n",
        "    os.system(cmd)\n",
        "\n",
        "  suffix = os.path.splitext(save_path)[-1]\n",
        "  new_save_path = f\"{os.path.dirname(save_path)}/{time.time()}{suffix}\"\n",
        "  cmd = f\"ln -s '{save_path}' {new_save_path}\"\n",
        "  os.system(cmd)\n",
        "  return new_save_path\n",
        "\n",
        "\n",
        "def set_upload_visibility(upload_items, mode=\"default\"):\n",
        "  assert mode in [\"default\", \"none\"]\n",
        "\n",
        "  if mode == \"default\":\n",
        "    upload_size_box, google_driver_tutorial, google_drive_id, upload_button, google_upload_html = upload_items\n",
        "    upload_size_box.value = \"No\"\n",
        "    upload_size_box.layout.display = None\n",
        "    google_driver_tutorial.layout.display = \"none\"\n",
        "    google_drive_id.layout.display = \"none\"\n",
        "    upload_button.layout.display = None\n",
        "    google_upload_html.layout.display = \"none\"\n",
        "\n",
        "  else:\n",
        "    for item in upload_items:\n",
        "      item.layout.display = \"none\"\n",
        "\n",
        "\n",
        "def get_upload_box(btn_desc=None):\n",
        "  import base64 as _base64\n",
        "  import collections as _collections\n",
        "  from http import server as _http_server\n",
        "  import json as _json\n",
        "  import os as _os\n",
        "  import pkgutil as _pkgutil\n",
        "  import socket as _socket\n",
        "  import socketserver as _socketserver\n",
        "  import urllib as _urllib\n",
        "  import uuid as _uuid\n",
        "\n",
        "  from google.colab import output as _output\n",
        "  import IPython as _IPython\n",
        "\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT = \"30px\"\n",
        "\n",
        "  upload_size_box = ipywidgets.RadioButtons(\n",
        "    options=['Yes', 'No'],\n",
        "    value=\"No\",\n",
        "    disabled=False,\n",
        "    layout={'width': 'max-content'}, # If the items' names are long\n",
        "    description=\"Is the file size > 5MB?\",\n",
        "    style={'description_width': 'initial'},\n",
        "    )\n",
        "\n",
        "  google_drive_id = ipywidgets.Text(\n",
        "      placeholder=\"Upload your file to Google Drive and copy its path into this box\",\n",
        "      description=\"File path:\",\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT, display=\"none\")\n",
        "      )\n",
        "  google_driver_tutorial = HTML(markdown.markdown(\n",
        "      \"<a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-%28latest%29#get-your-file-from-google-drive' target='blank'>Quick tutorial</a>\"\n",
        "      ),layout=Layout(display=\"none\"))\n",
        "\n",
        "  upload_button = ipywidgets.Button(\n",
        "      description=\"Upload your file\" if btn_desc is None else btn_desc,\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "      button_style=\"info\",\n",
        "      )\n",
        "\n",
        "  multiple = False\n",
        "  upload_id = str(_uuid.uuid4())\n",
        "  input_id = 'files-' + upload_id\n",
        "  output_id = 'result-' + upload_id\n",
        "  files_js = _pkgutil.get_data(\"google.colab.files\", 'resources/files.js').decode('utf8')\n",
        "\n",
        "  google_upload_html = HTML(\n",
        "          \"\"\"\n",
        "      <input type=\"file\" id=\"{input_id}\" name=\"files[]\" {multiple_text} disabled\n",
        "        style=\"border:none\" /><br><font color=red>Note: If you upload the same file, the upload will be ignored.</font></br>\n",
        "      <output id=\"{output_id}\">\n",
        "      Upload widget is only available when the cell has been executed in the\n",
        "      current browser session. Please rerun this cell to enable.\n",
        "      </output>\n",
        "      <script>{files_js}</script> \"\"\".format(\n",
        "              input_id=input_id,\n",
        "              output_id=output_id,\n",
        "              multiple_text='multiple' if multiple else '',\n",
        "              files_js=files_js,\n",
        "          ),layout=Layout(display=\"none\")\n",
        "\n",
        "      )\n",
        "\n",
        "  def switch_file_size(change):\n",
        "    now_type = change[\"new\"]\n",
        "    if now_type == \"No\":\n",
        "      google_drive_id.layout.display = \"none\"\n",
        "      google_driver_tutorial.layout.display = \"none\"\n",
        "      upload_button.layout.display = None\n",
        "\n",
        "    else:\n",
        "      google_drive_id.layout.display = None\n",
        "      google_driver_tutorial.layout.display = None\n",
        "      upload_button.layout.display = \"none\"\n",
        "\n",
        "  def google_file_upload(button):\n",
        "    def _get_unique_filename(filename):\n",
        "      if not _os.path.lexists(filename):\n",
        "        return filename\n",
        "      counter = 1\n",
        "      while True:\n",
        "        path, ext = _os.path.splitext(filename)\n",
        "        new_filename = '{} ({}){}'.format(path, counter, ext)\n",
        "        if not _os.path.lexists(new_filename):\n",
        "          return new_filename\n",
        "        counter += 1\n",
        "\n",
        "    try:\n",
        "      # Disable all widgets\n",
        "      for widget in ipywidgets.Widget.widgets.values():\n",
        "        widget.disabled = True\n",
        "      google_upload_html.disabled = False\n",
        "\n",
        "      ori_desc = button.description\n",
        "      button.description = \"All other buttons are disabled. Please first upload file or cancel upload!\"\n",
        "      button.button_style = \"danger\"\n",
        "      google_upload_html.layout.display = None\n",
        "\n",
        "      # First result is always an indication that the file picker has completed.\n",
        "      filename = None\n",
        "      result = _output.eval_js(\n",
        "          'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n",
        "              input_id=input_id, output_id=output_id\n",
        "          )\n",
        "      )\n",
        "      files = _collections.defaultdict(bytes)\n",
        "\n",
        "      while result['action'] != 'complete':\n",
        "        result = _output.eval_js(\n",
        "            'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
        "                output_id=output_id\n",
        "            )\n",
        "        )\n",
        "        if result['action'] != 'append':\n",
        "          # JS side uses a generator of promises to process all of the files- some\n",
        "          # steps may not produce data for the Python side, so just proceed onto the\n",
        "          # next message.\n",
        "          continue\n",
        "        files[result['file']] += _base64.b64decode(result['data'])\n",
        "\n",
        "      uploaded_files = dict(files)\n",
        "      # Mapping from original filename to filename as saved locally.\n",
        "\n",
        "      upload_path = Path(UPLOAD_FILE_HOME)\n",
        "      upload_path.mkdir(parents=True, exist_ok=True)\n",
        "      for filename, data in uploaded_files.items():\n",
        "        save_path = upload_path / filename\n",
        "        with open(save_path, 'wb') as f:\n",
        "          f.write(data)\n",
        "\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "\n",
        "    finally:\n",
        "      # Enable items\n",
        "      for widget in ipywidgets.Widget.widgets.values():\n",
        "        widget.disabled = False\n",
        "\n",
        "      button.description = filename if filename is not None else ori_desc\n",
        "      button.button_style = \"info\"\n",
        "      google_upload_html.layout.display = \"none\"\n",
        "\n",
        "  # Set click events\n",
        "  upload_size_box.observe(switch_file_size, names=\"value\")\n",
        "  upload_button.on_click(google_file_upload)\n",
        "\n",
        "  upload_hbox = ipywidgets.HBox([upload_size_box, google_drive_id, upload_button, google_upload_html])\n",
        "  upload_items = [\n",
        "      upload_size_box,\n",
        "      google_driver_tutorial,\n",
        "      google_drive_id,\n",
        "      upload_button,\n",
        "      google_upload_html\n",
        "  ]\n",
        "\n",
        "  return upload_items\n",
        "\n",
        "\n",
        "# Disable a button during function execution and enable it after execution done\n",
        "def disable_wrapper(func):\n",
        "  def wrapper(button):\n",
        "    button.disabled = True\n",
        "    try:\n",
        "      func(button)\n",
        "\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "\n",
        "    finally:\n",
        "      button.disabled = False\n",
        "\n",
        "  return wrapper\n",
        "\n",
        "\n",
        "def show_upload_file(change, button):\n",
        "  name = list(button.value.keys())[0]\n",
        "  button.description = name\n",
        "  button._counter = 1\n",
        "\n",
        "\n",
        "# Convert protein structures into structure-aware sequences\n",
        "def pdb2sa(pdb_list):\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "  from utils.mpr import MultipleProcessRunnerSimplifier\n",
        "\n",
        "  def do(process_id, idx, item, writer):\n",
        "    path, chain = item\n",
        "    name = os.path.basename(path)\n",
        "    sa_seq = get_struc_seq(FOLDSEEK_PATH, path, process_id=process_id, plddt_mask=True, chains=[chain])[chain][-1]\n",
        "    writer.write(f\"{sa_seq}\\n\")\n",
        "\n",
        "  mprs = MultipleProcessRunnerSimplifier(pdb_list, do, n_process=1, return_results=True)\n",
        "  outputs = mprs.run()\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def save_uploaded_file(button):\n",
        "  upload_path = Path(UPLOAD_FILE_HOME)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "\n",
        "  # Write to specific path\n",
        "  assert len(list(button.value.keys())) > 0, \"\\033[31m\\nYou must upload the data file firstly!\\033[0m\"\n",
        "\n",
        "  name = list(button.value.keys())[0]\n",
        "  content = button.value[name][\"content\"]\n",
        "  save_path = upload_path / name\n",
        "\n",
        "  with open(save_path, \"wb\") as wb:\n",
        "    wb.write(content)\n",
        "\n",
        "  return name, str(save_path)\n",
        "\n",
        "\n",
        "# Protein property prediction\n",
        "def make_predictions(df, rows, num_labels, model_type, model_arg):\n",
        "  task_type = load_task_type_from_model(model_type, str(model_arg).split(\"\\n\")[0].strip())\n",
        "  original_task_type = task_type\n",
        "  task_type = task_type_dict[task_type]\n",
        "\n",
        "  if model_type == \"Multi-models on SaprotHub\":\n",
        "    #1. get adapter_list\n",
        "    repo_id_list = [repo_id.strip() for repo_id in model_arg.strip().split(\"\\n\")]\n",
        "    #2. download adapters\n",
        "    for repo_id in repo_id_list:\n",
        "      snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / repo_id)\n",
        "    config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "    assert len(config_list) > 0, \"Please choose at least one model!\"\n",
        "    base_model = get_base_model(ADAPTER_HOME / config_list[0].lora_config_path)\n",
        "\n",
        "    lora_kwargs = EasyDict({\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": len(config_list),\n",
        "      \"config_list\": config_list\n",
        "    })\n",
        "\n",
        "  else:\n",
        "    if model_type == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=model_arg, repo_type=\"model\", local_dir=ADAPTER_HOME / model_arg)\n",
        "\n",
        "    adapter_path = ADAPTER_HOME / model_arg\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    lora_kwargs = {\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "  # task\n",
        "  if task_type in [ \"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_labels\n",
        "\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  # Load model\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  # Start prediction\n",
        "  logits = []\n",
        "  pred_labels = []\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    for sa_seq_1, sa_seq_2 in tqdm(rows):\n",
        "      input_1 = tokenizer(sa_seq_1, return_tensors=\"pt\")\n",
        "      input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "      input_2 = tokenizer(sa_seq_2, return_tensors=\"pt\")\n",
        "      input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "      with torch.no_grad():\n",
        "        pred = model(input_1, input_2)\n",
        "\n",
        "      if \"regression\" in task_type:\n",
        "        pred_labels.append(pred.item())\n",
        "      else:\n",
        "        logits.append(pred[0].softmax(dim=-1).cpu().numpy().tolist())\n",
        "        pred_labels.append(pred.argmax(dim=-1)[0].cpu().numpy().tolist())\n",
        "\n",
        "  else:\n",
        "    for sa_seq in tqdm(rows):\n",
        "      inputs = tokenizer(sa_seq, return_tensors=\"pt\")\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      with torch.no_grad():\n",
        "        pred = model(inputs)\n",
        "\n",
        "      if \"regression\" in task_type:\n",
        "        pred_labels.append(pred.item())\n",
        "      else:\n",
        "        logits.append(pred[0].softmax(dim=-1).cpu().numpy().tolist())\n",
        "        pred_labels.append(pred.argmax(dim=-1)[0].cpu().numpy().tolist())\n",
        "\n",
        "  if \"classification\" in task_type:\n",
        "    df[\"predicted_probabilities\"] = logits\n",
        "  df[\"predicted_label\"] = pred_labels\n",
        "\n",
        "  return_label = pred_labels[0] if len(pred_labels) == 1 else None\n",
        "\n",
        "  # Save predictions\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "  df.to_csv(output_file, index=False)\n",
        "\n",
        "  return return_label, output_file\n",
        "\n",
        "\n",
        "# Get data type that is compatible with the model\n",
        "def load_data_type_from_model(model_type, model_arg):\n",
        "  if model_type == \"Official SaProt (35M)\":\n",
        "    return \"SA\"\n",
        "\n",
        "  elif model_type == \"Official SaProt (650M)\":\n",
        "    return \"SA\"\n",
        "\n",
        "  else:\n",
        "    adapter_path = ADAPTER_HOME / model_arg\n",
        "\n",
        "    if model_type == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=model_arg, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      return metadata['training_data_type']\n",
        "\n",
        "\n",
        "# Get task type that is compatible with the model\n",
        "def load_task_type_from_model(model_type, model_arg):\n",
        "  try:\n",
        "    adapter_path = ADAPTER_HOME / model_arg\n",
        "\n",
        "    if model_type == \"Shared by peers on SaprotHub\" or model_type == \"Multi-models on SaprotHub\":\n",
        "      snapshot_download(repo_id=model_arg, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      return metadata['training_task_type']\n",
        "\n",
        "  except Exception as e:\n",
        "    raise Exception(\"\\033[31mPlease check your model input!\\033[0m\")\n",
        "\n",
        "\n",
        "def generate_download_btn(path: str):\n",
        "  if root_dir == \"/content\":\n",
        "    btn = ipywidgets.Button(\n",
        "      description=\"Download File\",\n",
        "      button_style=\"success\",\n",
        "      )\n",
        "\n",
        "    btn.on_click(lambda b: files.download(path))\n",
        "    return btn\n",
        "\n",
        "  else:\n",
        "    with open(path, \"rb\") as r:\n",
        "      res = r.read()\n",
        "\n",
        "    #FILE\n",
        "    filename = os.path.basename(path)\n",
        "    b64 = base64.b64encode(res)\n",
        "    payload = b64.decode()\n",
        "\n",
        "    #BUTTONS\n",
        "    html_buttons = '''<html>\n",
        "    <head>\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    </head>\n",
        "    <body>\n",
        "    <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "    </a>\n",
        "    </body>\n",
        "    </html>\n",
        "    '''\n",
        "\n",
        "    html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "    return HTML(html_button)\n",
        "\n",
        "\n",
        "def load_embedding_generation_model(model_type, model_arg):\n",
        "  if model_type == \"Official SaProt (35M)\":\n",
        "    base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    lora_kwargs = None\n",
        "\n",
        "  elif model_type == \"Official SaProt (650M)\":\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    lora_kwargs = None\n",
        "\n",
        "  else:\n",
        "    adapter_path = ADAPTER_HOME / model_arg\n",
        "\n",
        "    if model_type == \"Shared by peers on SaprotHub\":\n",
        "      snapshot_download(repo_id=model_arg, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    lora_kwargs = {\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "  if model_type in [\"Official SaProt (35M)\", \"Official SaProt (650M)\"]:\n",
        "    num_labels, task_type = 1, 'classification'\n",
        "  else:\n",
        "    num_labels, task_type = get_num_labels_and_task_type_by_adapter(lora_kwargs[\"config_list\"][0][\"lora_config_path\"])\n",
        "\n",
        "  config.model.kwargs.num_labels = num_labels\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def generate_embeddings(protein_list, model_type, model_arg):\n",
        "  # If the uploaded file is protein structure\n",
        "  if isinstance(protein_list, list):\n",
        "    from saprot.utils.foldseek_util import get_struc_seq\n",
        "    from utils.mpr import MultipleProcessRunnerSimplifier\n",
        "\n",
        "    def do(process_id, idx, path, writer):\n",
        "      name = os.path.basename(path)\n",
        "      seq_dict = get_struc_seq(FOLDSEEK_PATH, path, plddt_mask=True, process_id=process_id)\n",
        "      for chain, seq_tuple in seq_dict.items():\n",
        "        writer.write(f\"{name}.chain-{chain}\\t{seq_tuple[-1]}\\n\")\n",
        "\n",
        "\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, do, n_process=2, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "  # Else process fasta file\n",
        "  else:\n",
        "    from Bio import SeqIO\n",
        "    outputs = []\n",
        "    for record in SeqIO.parse(protein_list, \"fasta\"):\n",
        "      aa_seq = str(record.seq)\n",
        "      sa_seq = \"\".join(aa + \"#\" for aa in aa_seq)\n",
        "      outputs.append(f\"{record.id}\\t{sa_seq}\\n\")\n",
        "\n",
        "  model = load_embedding_generation_model(model_type, model_arg)\n",
        "\n",
        "  save_name_path = OUTPUT_HOME / \"embedding_seqs.fasta\"\n",
        "  save_embedding_path = OUTPUT_HOME / \"embeddings.pt\"\n",
        "\n",
        "  embedding_list = []\n",
        "  name_list = []\n",
        "  with torch.no_grad(), open(save_name_path, \"w\") as w:\n",
        "    for line in tqdm(outputs):\n",
        "      name, sa_seq = line.strip().split(\"\\t\")\n",
        "      w.write(f\">{name}\\n{sa_seq}\\n\")\n",
        "      embedding = model.get_hidden_states_from_seqs([sa_seq], reduction='mean')\n",
        "      embedding_list.append(embedding[0])\n",
        "      name_list.append(name)\n",
        "\n",
        "  embeddings = torch.stack(embedding_list)\n",
        "  torch.save(embeddings, save_embedding_path)\n",
        "\n",
        "  # Compress the fasta and embedding file into a .zip file\n",
        "  zip_path = OUTPUT_HOME / \"generated_embedding.zip\"\n",
        "  !cd $OUTPUT_HOME && zip -r $zip_path \"embedding_seqs.fasta\" \"embeddings.pt\"\n",
        "\n",
        "  return zip_path\n",
        "\n",
        "\n",
        "def load_zeroshot_model():\n",
        "  try:\n",
        "    zero_shot_model\n",
        "  except Exception:\n",
        "    from saprot.model.saprot.saprot_foldseek_mutation_model import SaprotFoldseekMutationModel\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    config = {\n",
        "      \"foldseek_path\": None,\n",
        "      \"config_path\": base_model,\n",
        "      \"load_pretrained\": True,\n",
        "    }\n",
        "\n",
        "    zero_shot_model = SaprotFoldseekMutationModel(**config)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    zero_shot_model.to(device)\n",
        "\n",
        "  return zero_shot_model\n",
        "\n",
        "\n",
        "# Zero-shot prediction\n",
        "def predict_mut(sa_seq, mut_info):\n",
        "  zero_shot_model = load_zeroshot_model()\n",
        "  score = zero_shot_model.predict_mut(sa_seq, mut_info)\n",
        "  return score\n",
        "\n",
        "\n",
        "# Zero-shot prediction for single-site saturation mutagenesis\n",
        "def predict_all_mut(sa_seq):\n",
        "  zero_shot_model = load_zeroshot_model()\n",
        "\n",
        "  timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "  output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "  mut_dicts = []\n",
        "  aa_seq = sa_seq[0::2]\n",
        "  for i in tqdm(range(len(aa_seq)), leave=False, desc=f\"Predicting\"):\n",
        "    mut_dict = zero_shot_model.predict_pos_mut(sa_seq, i+1)\n",
        "    mut_dicts.append(mut_dict)\n",
        "\n",
        "  mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "  df = pd.DataFrame(mut_list)\n",
        "  df.to_csv(output_path, index=None)\n",
        "  return output_path\n",
        "\n",
        "\n",
        "# Perform protein sequence design\n",
        "def inverse_folding(aa_seq, struc_seq, method, num_samples):\n",
        "  try:\n",
        "    saprot_if_model\n",
        "\n",
        "  except Exception:\n",
        "    from saprot.model.saprot.saprot_if_model import SaProtIFModel\n",
        "\n",
        "    base_model = \"westlake-repl/SaProt_650M_AF2_inverse_folding\"\n",
        "    config = {\n",
        "        \"config_path\": base_model,\n",
        "        \"load_pretrained\": True,\n",
        "    }\n",
        "\n",
        "    saprot_if_model = SaProtIFModel(**config)\n",
        "    tokenizer = saprot_if_model.tokenizer\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    saprot_if_model.to(device)\n",
        "\n",
        "  pred_aa_seqs = saprot_if_model.predict(aa_seq, struc_seq, method=method, num_samples=num_samples)\n",
        "  return pred_aa_seqs\n",
        "\n",
        "\n",
        "# Perform protein structure using ESMFold\n",
        "def predict_structure(seq):\n",
        "  try:\n",
        "    esmfold\n",
        "  except Exception:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
        "    esmfold = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
        "    esmfold.esm = esmfold.esm.half()\n",
        "    esmfold.trunk.set_chunk_size(64)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    esmfold.to(device)\n",
        "\n",
        "  tokenized_input = tokenizer(\n",
        "    [seq],\n",
        "    return_tensors=\"pt\",\n",
        "    add_special_tokens=False,\n",
        "    )['input_ids']\n",
        "\n",
        "  tokenized_input = tokenized_input.to(esmfold.device)\n",
        "  with torch.no_grad():\n",
        "    output = esmfold(tokenized_input)\n",
        "\n",
        "  save_path = f\"{root_dir}/SaprotHub/output/predicted_structure.pdb\"\n",
        "  pdb = convert_outputs_to_pdb(output)\n",
        "  with open(save_path, \"w\") as f:\n",
        "    f.write(\"\".join(pdb))\n",
        "\n",
        "  return save_path\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#          Training or Prediction            #\n",
        "######################################################################\n",
        "def train_or_pred():\n",
        "  global refresh_module\n",
        "  refresh_module = train_or_pred\n",
        "\n",
        "  question = HTML(markdown.markdown(\"## Please choose what you want to do with ColabSaprot\"))\n",
        "  option_intro = HTML(markdown.markdown(\n",
        "    \"<a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#introduction-of-options' target='blank'>Introduction of these options</a>\"\n",
        "    ))\n",
        "\n",
        "  train_btn = Button(description='I want to train my own model', layout=Layout(width='400px', height='30px'))\n",
        "  pred_btn = Button(description='I want to use existing models to make prediction', layout=Layout(width='400px', height='30px'))\n",
        "  share_btn = Button(description='I want to share my model publicly', layout=Layout(width='400px', height='30px'))\n",
        "\n",
        "  items = [\n",
        "      question,\n",
        "      option_intro,\n",
        "      train_btn,\n",
        "      pred_btn,\n",
        "      share_btn\n",
        "      ]\n",
        "\n",
        "  # Set click events\n",
        "  train_btn.on_click(partial(jump, next=choose_training_task))\n",
        "  pred_btn.on_click(partial(jump, next=choose_pred_task))\n",
        "  share_btn.on_click(partial(jump, next=share_model))\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#          Choose training task             #\n",
        "######################################################################\n",
        "def choose_training_task():\n",
        "  global refresh_module\n",
        "  refresh_module = choose_training_task\n",
        "\n",
        "  title = HTML(markdown.markdown(\"## Please finish the setting of your training task\"))\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT= \"30px\"\n",
        "\n",
        "  task_hint = HTML(markdown.markdown(\"### Task setting:\"))\n",
        "  task_name = ipywidgets.Text(\n",
        "              value=\"Demo\",\n",
        "              placeholder=f'Enter the task name',\n",
        "              disabled=False,\n",
        "              description=\"Name your task:\",\n",
        "              layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "              style={'description_width': 'initial'},\n",
        "              )\n",
        "  task_type = ipywidgets.Dropdown(\n",
        "            options=['Protein-level Classification', 'Protein-level Regression', 'Residue-level Classification', \"Protein-protein Classification\", \"Protein-protein Regression\"],\n",
        "            value='Protein-level Classification',\n",
        "            description='Task type:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "          )\n",
        "\n",
        "  num_label = ipywidgets.BoundedIntText(\n",
        "        value=2,\n",
        "        min=2,\n",
        "        max=100000000,\n",
        "        step=1,\n",
        "        description='Number of categories:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "      )\n",
        "\n",
        "  task_intro_dict = {\n",
        "      'Protein-level Classification': \"<font color=red>What is **Protein-level Classification:** Given a protein, you have some categories and you want to predict which category the protein belongs to.</font>\",\n",
        "      \"Protein-level Regression\": \"<font color=red>What is **Protein-level Regression:** Given a protein, you want to predict a score about its property such as stability or enzyme activity.</font>\",\n",
        "      \"Residue-level Classification\": \"<font color=red>What is **Residue-level Classification:** Given a protein, you have some categories and for every amino acid you want to predict which category it belongs to.</font>\",\n",
        "      \"Protein-protein Classification\": \"<font color=red>What is **Protein-protein Classification:** Given a pair of proteins, you have some categories and you want to predict which category the pair belongs to.</font>\",\n",
        "      \"Protein-protein Regression\": \"<font color=red>What is **Protein-protein Regression:** Given a pair of proteins, you want to predict a score about its property such as binding affinity.</font>\"\n",
        "  }\n",
        "\n",
        "  task_intro = HTML(markdown.markdown(\n",
        "    \"<font color=red>What is **Protein-level Classification:** Given a protein, you have some categories and you want to predict which category the protein belongs to.</font>\"\n",
        "    ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  model_hint = HTML(markdown.markdown(\"### Model setting:\"))\n",
        "  model_type = ipywidgets.Dropdown(\n",
        "            options=['Official SaProt (35M)', \"Official SaProt (650M)\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"],\n",
        "            value='Official SaProt (35M)',\n",
        "            description='Base model:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "          )\n",
        "  model_arg_box = HTML(markdown.markdown(\"dummy box\"))\n",
        "  model_arg_box.layout.display = \"none\"\n",
        "\n",
        "  upload_model_items = get_upload_box(btn_desc=\"Upload Model-<task_name>-<model_size>.zip file\")\n",
        "  set_upload_visibility(upload_model_items, mode=\"none\")\n",
        "\n",
        "  saprot_650m_hint = HTML(markdown.markdown(\n",
        "      \"<font color=red>Warning: You are training Saprot 650M version, please switch to A100 GPU (need Colab Pro) to avoid the **Out of Memory** problem!</font>\"\n",
        "      ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  saprothub_link = HTML(markdown.markdown(\n",
        "      \"<font color=red>You could search models using our <a href='https://huggingface.co/spaces/SaProtHub/SaprotHub-search' target='blank'>search engine</a> or from <a href='https://huggingface.co/SaProtHub' target='blank'>SaprotHub</a>\\n\\n\"\n",
        "      \"A model id example: <a href='https://huggingface.co/SaProtHub/Model-Binary_Localization-650M' target='blank'>SaProtHub/Model-Binary_Localization-650M</a></font>\"\n",
        "      ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  dataset_hint = HTML(markdown.markdown(\"### Dataset setting:\"))\n",
        "  data_type_hint = HTML(markdown.markdown(\"**Your training data type:**\"))\n",
        "  data_type = ipywidgets.RadioButtons(\n",
        "      options=['protein sequence', 'protein structure'],\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      )\n",
        "\n",
        "  data_src_hint = HTML(markdown.markdown(\"**Where is your data from:**\"))\n",
        "  data_src_type = ipywidgets.RadioButtons(\n",
        "      options=['SaprotHub in HuggingFace', 'Upload mannually'],\n",
        "      value=\"Upload mannually\",\n",
        "      layout={'width': 'max-content'}, # If the items' names are long\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      )\n",
        "\n",
        "  # data_split_hint = HTML(markdown.markdown(\"**Have you already split your data into training, validation and test set?**\"))\n",
        "  # data_split_type = ipywidgets.RadioButtons(\n",
        "  #             options=['Yes', 'No (We will randomly split your data with 8:1:1 for train:valid:test)'],\n",
        "  #           layout={'width': 'max-content'}, # If the items' names are long\n",
        "  #             disabled=False,\n",
        "  #             style={'description_width': 'initial'},\n",
        "  #         )\n",
        "\n",
        "  saprothub_data_id = ipywidgets.Text(\n",
        "              value=None,\n",
        "              placeholder=f'Enter SaprotHub dataset id in HuggingFace',\n",
        "              disabled=False,\n",
        "              description=\"Dataset id:\",\n",
        "              layout=Layout(width=WIDTH, height=HEIGHT, display=\"none\"),\n",
        "              )\n",
        "  saprothub_data_id_hint = HTML(markdown.markdown(\n",
        "      \"<font color=red>You could search datasets using our <a href='https://huggingface.co/spaces/SaProtHub/SaprotHub-search' target='blank'>search engine</a> or from <a href='https://huggingface.co/SaProtHub' target='blank'>SaprotHub</a>\\n\\n\"\n",
        "      \"A dataset id example: <a href='https://huggingface.co/datasets/SaProtHub/Dataset-Binary_Localization-DeepLoc' target='blank'>SaProtHub/Dataset-Binary_Localization-DeepLoc</a></font>\"\n",
        "      ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  upload_hint = HTML(markdown.markdown(\n",
        "            \"**Upload your training data:**\\n\\n\"\n",
        "            \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_classification.jpg?raw=true' height='200px' width='700px' align='center'>\\n\\n\"\n",
        "            \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "            ))\n",
        "  data_example_hint = HTML(markdown.markdown(\n",
        "            f\"<a href='https://github.com/westlake-repl/SaprotHub/tree/main/upload_files/ColabSaprot_v2_example_csv_dataset/train/' target='blank'>Here</a> are some toy examples.\"\n",
        "            ))\n",
        "\n",
        "  upliad_items_hint_1 = HTML(markdown.markdown(\n",
        "            \"**Upload .csv file:**\"\n",
        "            ))\n",
        "  upload_items_1 = get_upload_box()\n",
        "\n",
        "  upliad_items_hint_2 = HTML(markdown.markdown(\n",
        "          \"**Upload .zip file:**\"\n",
        "          ), layout=Layout(display=\"none\"))\n",
        "  upload_items_2 = get_upload_box()\n",
        "  set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "\n",
        "  hyperparameter_hint = HTML(markdown.markdown(\n",
        "      \"### Training hyper-parameters:\\n\\n\"\n",
        "      \"<a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-%28latest%29#hyper-parameters' target='blank'>What are these hyper-parameters?</a>\"\n",
        "      ))\n",
        "  batch_size = ipywidgets.Dropdown(\n",
        "            options=[\"Adaptive\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"],\n",
        "            value='Adaptive',\n",
        "            description='Batch size:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "            )\n",
        "\n",
        "  epoch = ipywidgets.BoundedIntText(\n",
        "        value=5,\n",
        "        min=1,\n",
        "        max=100,\n",
        "        step=1,\n",
        "        description='Epoch:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "      )\n",
        "\n",
        "  lr = ipywidgets.BoundedFloatText(\n",
        "      value=5e-4,\n",
        "      min=0,\n",
        "      max=1e-2,\n",
        "      step=1e-4,\n",
        "      description='Learning rate:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "\n",
        "  advanced_setting = Button(description='Show advanced setting‚¨áÔ∏è', layout=Layout(width=WIDTH, height=HEIGHT))\n",
        "  lora_hint = HTML(markdown.markdown(f\"### Lora config\"))\n",
        "  lora_r = ipywidgets.BoundedIntText(\n",
        "        value=8,\n",
        "        min=1,\n",
        "        max=128,\n",
        "        step=1,\n",
        "        description='r:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "      )\n",
        "  lora_dropout_box = ipywidgets.FloatText(\n",
        "      value=0.0,\n",
        "      description='Lora dropout:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "  lora_alpha_box = ipywidgets.BoundedIntText(\n",
        "        value=16,\n",
        "        min=1,\n",
        "        max=128,\n",
        "        step=1,\n",
        "        description='Lora alpha:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "      )\n",
        "\n",
        "  trainer_hint = HTML(markdown.markdown(f\"### PyTorch Lightning trainer config\"))\n",
        "  val_check_interval_box = ipywidgets.FloatText(\n",
        "      value=0.5,\n",
        "      description='val_check_interval:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "  limit_train_batches_box = ipywidgets.FloatText(\n",
        "      value=1.0,\n",
        "      description='limit_train_batches:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "  limit_val_batches_box = ipywidgets.FloatText(\n",
        "      value=1.0,\n",
        "      description='limit_val_batches:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "  limit_test_batches_box = ipywidgets.FloatText(\n",
        "      value=1.0,\n",
        "      description='limit_test_batches:',\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "  )\n",
        "\n",
        "  start_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "  stable_control_hint = HTML(markdown.markdown(\n",
        "      \"<font color=red>Warning: Your session may disconnect during long-time (e.g. >2 hours) training. \"\n",
        "      \"See our quick fix <a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#how-to-handle-long-time-training' target='blank'>here</a>.</font>\"\n",
        "      ))\n",
        "  start_btn = Button(description='Start training', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "\n",
        "  items = [\n",
        "      title,\n",
        "      task_hint,\n",
        "      task_name,\n",
        "      task_type,\n",
        "      num_label,\n",
        "      task_intro,\n",
        "      model_hint,\n",
        "      model_type,\n",
        "      model_arg_box,\n",
        "      saprot_650m_hint,\n",
        "      saprothub_link,\n",
        "      dataset_hint,\n",
        "      data_src_hint,\n",
        "      data_src_type,\n",
        "      data_type_hint,\n",
        "      data_type,\n",
        "      saprothub_data_id,\n",
        "      saprothub_data_id_hint,\n",
        "      upload_hint,\n",
        "      data_example_hint,\n",
        "      upliad_items_hint_1,\n",
        "      *upload_items_1,\n",
        "      upliad_items_hint_2,\n",
        "      *upload_items_2,\n",
        "      hyperparameter_hint,\n",
        "      batch_size,\n",
        "      epoch,\n",
        "      lr,\n",
        "      advanced_setting,\n",
        "      lora_hint,\n",
        "      lora_r,\n",
        "      lora_dropout_box,\n",
        "      lora_alpha_box,\n",
        "      trainer_hint,\n",
        "      limit_train_batches_box,\n",
        "      limit_val_batches_box,\n",
        "      limit_test_batches_box,\n",
        "      start_hint,\n",
        "      stable_control_hint,\n",
        "      start_btn\n",
        "      ]\n",
        "\n",
        "  # Used to replace input box based on model type\n",
        "  model_arg_box_idx = items.index(model_arg_box)\n",
        "\n",
        "  # Set click events\n",
        "  def change_task_type(change):\n",
        "    now_type = change[\"new\"]\n",
        "    task_intro.value = markdown.markdown(task_intro_dict[now_type])\n",
        "    set_upload_hint()\n",
        "    if \"Classification\" in now_type:\n",
        "      num_label.layout.display = None\n",
        "    else:\n",
        "      num_label.layout.display = \"none\"\n",
        "\n",
        "  def change_model_type(change):\n",
        "    model_type_value = change[\"new\"]\n",
        "    task_type_value = task_type_dict[task_type.value]\n",
        "\n",
        "    saprot_650m_hint.layout.display = None if \"650M\" in model_type_value else \"none\"\n",
        "\n",
        "    if model_type_value == \"Trained by yourself on ColabSaprot\":\n",
        "      model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "      model_arg_box.layout.width = WIDTH\n",
        "      model_arg_box.description = \"Select your local model:\"\n",
        "      model_arg_box.style = {'description_width': 'initial'}\n",
        "      items[model_arg_box_idx] = model_arg_box\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "      custom_display(*items)\n",
        "\n",
        "    elif model_type_value == \"Shared by peers on SaprotHub\":\n",
        "      model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "      model_arg_box.layout.width = WIDTH\n",
        "      model_arg_box.placeholder = \"Enter SaprotHub model id in HuggingFace\"\n",
        "      model_arg_box.description = \"Model id:\"\n",
        "      items[model_arg_box_idx] = model_arg_box\n",
        "      saprothub_link.layout.display = None\n",
        "      custom_display(*items)\n",
        "\n",
        "    elif model_type_value == \"Saved in your local computer\":\n",
        "      set_upload_visibility(upload_model_items, \"default\")\n",
        "      new_model_arg_box = upload_model_items\n",
        "      new_items = items[:model_arg_box_idx] + new_model_arg_box + items[model_arg_box_idx+1:]\n",
        "      custom_display(*new_items)\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "\n",
        "    else:\n",
        "      items[model_arg_box_idx].layout.display = \"none\"\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "      custom_display(*items)\n",
        "\n",
        "  def change_data_type(change):\n",
        "    now_type = change[\"new\"]\n",
        "    if now_type == \"protein sequence\":\n",
        "      upliad_items_hint_2.layout.display = \"none\"\n",
        "      set_upload_visibility(upload_items_2, \"none\")\n",
        "    else:\n",
        "      upliad_items_hint_2.layout.display = None\n",
        "      set_upload_visibility(upload_items_2, \"default\")\n",
        "\n",
        "    set_upload_hint()\n",
        "\n",
        "  def change_data_src_type(change):\n",
        "    data_src_type_value = change[\"new\"]\n",
        "    if data_src_type_value == \"SaprotHub in HuggingFace\":\n",
        "      saprothub_data_id.layout.display = None\n",
        "      saprothub_data_id_hint.layout.display = None\n",
        "      for item in [data_type_hint, data_type, upload_hint, data_example_hint]:\n",
        "        item.layout.display = \"none\"\n",
        "\n",
        "      upliad_items_hint_1.layout.display = \"none\"\n",
        "      set_upload_visibility(upload_items_1, \"none\")\n",
        "      upliad_items_hint_2.layout.display = \"none\"\n",
        "      set_upload_visibility(upload_items_2, \"none\")\n",
        "\n",
        "    else:\n",
        "      saprothub_data_id.layout.display = \"none\"\n",
        "      saprothub_data_id_hint.layout.display = \"none\"\n",
        "      for item in [data_type_hint, data_type, upload_hint, data_example_hint]:\n",
        "        item.layout.display = None\n",
        "\n",
        "      upliad_items_hint_1.layout.display = None\n",
        "      set_upload_visibility(upload_items_1, \"default\")\n",
        "\n",
        "      if data_type.value == \"protein structure\":\n",
        "        upliad_items_hint_2.layout.display = None\n",
        "        set_upload_visibility(upload_items_2, \"default\")\n",
        "\n",
        "  def set_upload_hint():\n",
        "    if task_type.value == \"Protein-level Classification\" and data_type.value == \"protein sequence\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_classification.jpg?raw=true' height='200px' width='700px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-level Regression\" and data_type.value == \"protein sequence\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_regression.jpg?raw=true' height='200px' width='700px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Residue-level Classification\" and data_type.value == \"protein sequence\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_token_classification.jpg?raw=true' height='200px' width='800px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-protein Classification\" and data_type.value == \"protein sequence\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_pair_classification.jpg?raw=true' height='200px' width='800px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-protein Regression\" and data_type.value == \"protein sequence\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/aa_pair_regression.jpg?raw=true' height='200px' width='800px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-level Classification\" and data_type.value == \"protein structure\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/struc_classification.jpg?raw=true' height='200px' width='550px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-level Regression\" and data_type.value == \"protein structure\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/struc_regression.jpg?raw=true' height='200px' width='550px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Residue-level Classification\" and data_type.value == \"protein structure\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/struc_token_classification.jpg?raw=true' height='200px' width='700px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-protein Classification\" and data_type.value == \"protein structure\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/struc_pair_classification.jpg?raw=true' height='200px' width='800px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    elif task_type.value == \"Protein-protein Regression\" and data_type.value == \"protein structure\":\n",
        "      upload_hint.value = markdown.markdown(\n",
        "              \"**Upload your training data:**\\n\\n\"\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/train/dataset/struc_pair_regression.jpg?raw=true' height='200px' width='800px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to train. Please strictly follow the format above **(column names are also needed in the csv file)**.\"\n",
        "              )\n",
        "\n",
        "    else:\n",
        "      raise\n",
        "\n",
        "  def click_advanced_setting(button):\n",
        "    if button.description == \"Show advanced setting‚¨áÔ∏è\":\n",
        "      button.description = \"Hide advanced setting‚¨áÔ∏è\"\n",
        "      for item in [lora_hint, lora_r, lora_dropout_box, lora_alpha_box, trainer_hint, limit_train_batches_box, limit_val_batches_box, limit_test_batches_box,]:\n",
        "        item.layout.display = None\n",
        "    else:\n",
        "      button.description = \"Show advanced setting‚¨áÔ∏è\"\n",
        "      for item in [lora_hint, lora_r, lora_dropout_box, lora_alpha_box, trainer_hint, limit_train_batches_box, limit_val_batches_box, limit_test_batches_box,]:\n",
        "        item.layout.display = \"none\"\n",
        "\n",
        "\n",
        "  def start_training(button):\n",
        "    print(\"Start training...\")\n",
        "\n",
        "    task_type_value = task_type.value\n",
        "    model_type_value = model_type.value\n",
        "    model_arg = items[model_arg_box_idx].value\n",
        "\n",
        "    # Check compatibility between chosen task and model\n",
        "    if model_type_value == \"Shared by peers on SaprotHub\" or model_type_value == \"Trained by yourself on ColabSaprot\" or model_type_value == \"Saved in your local computer\":\n",
        "      if model_type_value == \"Saved in your local computer\":\n",
        "        zip_path = get_upload_file_path(upload_model_items)\n",
        "        name = os.path.basename(zip_path)\n",
        "        save_dir = ADAPTER_HOME / \"Upload\" / name.rsplit('.', 1)[0]\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        # unzip model.zip\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(save_dir)\n",
        "        os.remove(zip_path)\n",
        "        model_arg = save_dir\n",
        "\n",
        "      model_task_type = load_task_type_from_model(model_type_value, model_arg)\n",
        "      assert task_type_value == model_task_type, f\"The model you choose was trained on {model_task_type} task, which is not suitable for the {task_type_value} task you choose!\"\n",
        "\n",
        "    ######################################################################\n",
        "    #            Start training              #\n",
        "    ######################################################################\n",
        "    from saprot.config.config_dict import Default_config\n",
        "    config = copy.deepcopy(Default_config)\n",
        "\n",
        "    # training config\n",
        "    GPU_batch_size = 0\n",
        "    accumulate_grad_batches = 0\n",
        "    num_workers = 2\n",
        "    seed = 20000812\n",
        "\n",
        "    # lora config\n",
        "    r = lora_r.value\n",
        "    lora_dropout = lora_dropout_box.value\n",
        "    lora_alpha = lora_alpha_box.value\n",
        "\n",
        "    # dataset config\n",
        "    val_check_interval = val_check_interval_box.value\n",
        "    limit_train_batches = limit_train_batches_box.value\n",
        "    limit_val_batches = limit_val_batches_box.value\n",
        "    limit_test_batches = limit_test_batches_box.value\n",
        "\n",
        "    mask_struc_ratio=None\n",
        "    if torch.cuda.is_available() is False:\n",
        "      raise BaseException(\"Please switch your Runtime to a GPU!\")\n",
        "\n",
        "    # Task config\n",
        "    task_name_value = task_name.value.replace(\" \", \"_\")\n",
        "    task_type_value = task_type.value\n",
        "    original_task_type = task_type_value\n",
        "    task_type_value = task_type_dict[task_type_value]\n",
        "\n",
        "    #####################################################################\n",
        "    #            Model config               #\n",
        "    #####################################################################\n",
        "    base_model = model_type.value\n",
        "    # continue learning\n",
        "    if base_model in [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]:\n",
        "      continue_learning = True\n",
        "    else:\n",
        "      continue_learning = False\n",
        "\n",
        "    # base_model\n",
        "    if continue_learning:\n",
        "      adapter_path = ADAPTER_HOME / model_arg\n",
        "      print(f\"Training on an existing model: {adapter_path}\")\n",
        "\n",
        "      if base_model == \"Shared by peers on SaprotHub\":\n",
        "        if not adapter_path.exists():\n",
        "          snapshot_download(repo_id=model_arg, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "      adapter_config_path = Path(adapter_path) / \"adapter_config.json\"\n",
        "      assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "      with open(adapter_config_path, 'r') as f:\n",
        "        adapter_config = json.load(f)\n",
        "        base_model = adapter_config['base_model_name_or_path']\n",
        "\n",
        "    elif base_model == \"Official SaProt (35M)\":\n",
        "      base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "\n",
        "    elif base_model == \"Official SaProt (650M)\":\n",
        "      base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "    # model size and model name\n",
        "    if base_model == \"westlake-repl/SaProt_650M_AF2\":\n",
        "      model_size = \"650M\"\n",
        "      model_name = f\"Model-{task_name_value}-{model_size}\"\n",
        "    elif base_model == \"westlake-repl/SaProt_35M_AF2\":\n",
        "      model_size = \"35M\"\n",
        "      model_name = f\"Model-{task_name_value}-{model_size}\"\n",
        "\n",
        "    config.setting.run_mode = \"train\"\n",
        "    config.setting.seed = seed\n",
        "\n",
        "    if task_type_value in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "      config.model.kwargs.num_labels = num_label.value\n",
        "\n",
        "    config.model.model_py_path = model_type_dict[task_type_value]\n",
        "    config.model.kwargs.config_path = base_model\n",
        "    config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "    config.model.save_path = str(ADAPTER_HOME / \"Local\" / f\"{task_type_value}\" / model_name)\n",
        "\n",
        "    if task_type_value in [\"regression\", \"pair_regression\"]:\n",
        "      config.model.kwargs.extra_config = {}\n",
        "      config.model.kwargs.extra_config.attention_probs_dropout_prob=0\n",
        "      config.model.kwargs.extra_config.hidden_dropout_prob=0\n",
        "\n",
        "    config.model.kwargs.lora_kwargs = EasyDict({\n",
        "      \"is_trainable\": True,\n",
        "      \"num_lora\": 1,\n",
        "      \"r\": r,\n",
        "      \"lora_dropout\": lora_dropout,\n",
        "      \"lora_alpha\": lora_alpha,\n",
        "      \"config_list\": []})\n",
        "    if continue_learning:\n",
        "      config.model.kwargs.lora_kwargs.config_list.append({\"lora_config_path\": adapter_path})\n",
        "\n",
        "    #####################################################################\n",
        "    #            Dataset config              #\n",
        "    #####################################################################\n",
        "    if data_src_type.value == \"SaprotHub in HuggingFace\":\n",
        "      data_src_type_value = \"SaprotHub Dataset\"\n",
        "      raw_data = saprothub_data_id\n",
        "      csv_dataset_path = get_SA_sequence_by_data_type(data_src_type_value, raw_data)\n",
        "\n",
        "      # Get the meta-data of the dataset\n",
        "      metadata_path = DATASET_HOME / saprothub_data_id.value / 'metadata.json'\n",
        "      metadata = json.load(open(metadata_path, \"r\"))\n",
        "\n",
        "      # Check compatibility between chosen task and data\n",
        "      assert metadata[\"training_task_type\"] == task_type.value, f\"This dataset is for '{metadata['training_task_type']}', which is not suitable for your task '{task_type.value}'\"\n",
        "\n",
        "      # Check compatibility between chosen data and model\n",
        "      if model_type_value == \"Shared by peers on SaprotHub\" or model_type_value == \"Trained by yourself on ColabSaprot\":\n",
        "        model_data_type = load_data_type_from_model(model_type_value, model_arg)\n",
        "\n",
        "        if model_data_type != metadata[\"training_data_type\"]:\n",
        "          if model_data_type == \"SA\":\n",
        "            raise Exception(\"Error: The model you choose should be trained on protein structures. So you have to choose protein structure dataset!\")\n",
        "          else:\n",
        "            raise Exception(\"Error: The model you choose should be trained on protein sequences. So you have to choose protein sequence dataset!\")\n",
        "\n",
        "    else:\n",
        "      data_type_value = data_type.value\n",
        "      metadata = {\n",
        "          \"training_task_type\": task_type.value,\n",
        "          \"training_data_type\": \"AA\" if data_type_value == \"protein sequence\" else \"SA\"\n",
        "      }\n",
        "\n",
        "      csv_path = get_upload_file_path(upload_items_1)\n",
        "      assert csv_path.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "\n",
        "      if data_type_value == \"protein sequence\":\n",
        "        if \"Protein-protein\" not in task_type.value:\n",
        "          processed_data_type = \"Multiple AA Sequences\"\n",
        "        else:\n",
        "          processed_data_type = \"Multiple pairs of AA Sequences\"\n",
        "\n",
        "        tmp_path = f\"{csv_path}.tmp\"\n",
        "        os.system(f\"cp {csv_path} {tmp_path}\")\n",
        "        csv_dataset_path = get_SA_sequence_by_data_type(processed_data_type, tmp_path)\n",
        "\n",
        "        # df = pd.read_csv(csv_dataset_path)\n",
        "        # print(df)\n",
        "        # raise\n",
        "\n",
        "      else:\n",
        "        zip_path = get_upload_file_path(upload_items_2)\n",
        "        zip_name = os.path.basename(zip_path)\n",
        "        assert zip_path.endswith(\".zip\"), \"Please upload file with correct format (.zip)!\"\n",
        "\n",
        "        # Unzip structures\n",
        "        upload_path = Path(UPLOAD_FILE_HOME)\n",
        "        upload_path.mkdir(parents=True, exist_ok=True)\n",
        "        prefix = zip_name.rsplit(\".\", 1)[0]\n",
        "        struc_dir = upload_path / prefix\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(struc_dir)\n",
        "\n",
        "        # If the unzipped file is a directory\n",
        "        unzipped_files = os.listdir(struc_dir)\n",
        "        if len(unzipped_files) == 1:\n",
        "          file_path = f\"{struc_dir}/{unzipped_files[0]}\"\n",
        "          if os.path.isdir(file_path):\n",
        "            struc_dir = file_path\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if \"Protein-protein\" not in task_type.value:\n",
        "          pdbs = []\n",
        "          for pdb_name, chain in df[[\"protein\", \"chain\"]].values:\n",
        "            pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "            pdbs.append([pdb_path, chain])\n",
        "\n",
        "          sa_seqs = pdb2sa(pdbs)\n",
        "          df[\"protein\"] = sa_seqs\n",
        "\n",
        "        else:\n",
        "          pdbs = []\n",
        "          for pdb_name, chain in df[[\"protein_1\", \"chain_1\"]].values:\n",
        "            pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "            pdbs.append([pdb_path, chain])\n",
        "\n",
        "          sa_seqs = pdb2sa(pdbs)\n",
        "          df[\"name_1\"] = df[\"protein_1\"]\n",
        "          df[\"protein_1\"] = sa_seqs\n",
        "\n",
        "          pdbs = []\n",
        "          for pdb_name, chain in df[[\"protein_2\", \"chain_2\"]].values:\n",
        "            pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "            pdbs.append([pdb_path, chain])\n",
        "\n",
        "          sa_seqs = pdb2sa(pdbs)\n",
        "          df[\"name_2\"] = df[\"protein_2\"]\n",
        "          df[\"protein_2\"] = sa_seqs\n",
        "\n",
        "        tmp_path = f\"{csv_path}.tmp\"\n",
        "        os.system(f\"cp {csv_path} {tmp_path}\")\n",
        "        df.to_csv(tmp_path, index=False)\n",
        "        csv_dataset_path = tmp_path\n",
        "\n",
        "    # Rename columns to match backend functions\n",
        "    df = pd.read_csv(csv_dataset_path)\n",
        "    if \"Protein-protein\" in task_type.value:\n",
        "      df = df.rename(columns={\"protein_1\": \"sequence_1\", \"protein_2\": \"sequence_2\"})\n",
        "    else:\n",
        "      df = df.rename(columns={\"protein\": \"sequence\"})\n",
        "\n",
        "    if \"stage\" not in df.columns:\n",
        "      print(\"The 'stage' column was not provided. We will randomly split the data into training, validation and test set.\")\n",
        "      train_len = int(0.8 * len(df))\n",
        "      valid_len = int(0.1 * len(df))\n",
        "      test_len = len(df) - train_len - valid_len\n",
        "      stage = [\"train\"] * train_len + [\"valid\"] * valid_len + [\"test\"] * test_len\n",
        "\n",
        "      # assert train_len > 0 and valid_len > 0 and test_len > 0, \"Please provide more data for splitting\"\n",
        "      if train_len == 0 or valid_len == 0 or test_len == 0:\n",
        "        red_print(\"Error: The size of your data is too small. Please provide more data for training (>= 10).\")\n",
        "        return\n",
        "\n",
        "\n",
        "      # Shuffle the stage labels\n",
        "      random.shuffle(stage)\n",
        "      df[\"stage\"] = stage\n",
        "\n",
        "    csv_for_lmdb_path = f\"{csv_dataset_path}.for_lmdb\"\n",
        "    df.to_csv(csv_for_lmdb_path, index=False)\n",
        "    check_column_label_and_stage(csv_for_lmdb_path)\n",
        "\n",
        "    # Check the number of labels for classification task\n",
        "    if task_type_value in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "      max_label = 0\n",
        "\n",
        "      try:\n",
        "        for label in df[\"label\"].values:\n",
        "          if task_type_value == \"token_classification\":\n",
        "            label_ids = [int(i.strip()) for i in label.strip().split(\",\")]\n",
        "            max_label = max([max_label] + label_ids)\n",
        "\n",
        "          else:\n",
        "            max_label = max(max_label, int(label))\n",
        "\n",
        "      except Exception:\n",
        "        raise Exception(\"Please make sure you upload the correct data according to the task type you choose!\")\n",
        "\n",
        "      assert num_label.value > max_label, f\"The number of category you set ({num_label.value}) should match the dataset you upload ({max_label+1})!\"\n",
        "\n",
        "    from saprot.utils.construct_lmdb import construct_lmdb\n",
        "    try:\n",
        "      construct_lmdb(csv_for_lmdb_path, LMDB_HOME, task_name_value, task_type_value)\n",
        "\n",
        "    except Exception:\n",
        "      raise Exception(\"Please make sure you upload the correct data according to the task type you choose!\")\n",
        "\n",
        "    lmdb_dataset_path = LMDB_HOME / task_name_value\n",
        "\n",
        "    config.dataset.dataset_py_path = dataset_type_dict[task_type_value]\n",
        "\n",
        "    config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "    config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "    config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "    # num_workers\n",
        "    config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "    def get_accumulate_grad_samples(num_samples):\n",
        "        if num_samples > 3200:\n",
        "            return 64\n",
        "        elif 1600 < num_samples <= 3200:\n",
        "            return 32\n",
        "        elif 800 < num_samples <= 1600:\n",
        "            return 16\n",
        "        elif 400 < num_samples <= 800:\n",
        "            return 8\n",
        "        elif 200 < num_samples <= 400:\n",
        "            return 4\n",
        "        elif 100 < num_samples <= 200:\n",
        "            return 2\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # advanced config\n",
        "    if (GPU_batch_size > 0) and (accumulate_grad_batches > 0):\n",
        "      config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "      config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "    elif (GPU_batch_size == 0) and (accumulate_grad_batches == 0):\n",
        "\n",
        "      # batch_size\n",
        "      # if base_model == \"westlake-repl/SaProt_650M_AF2\" and root_dir == \"/content\":\n",
        "      #   GPU_batch_size = 1\n",
        "      # else:\n",
        "      #   GPU_batch_size_dict = {\n",
        "      #     \"Tesla T4\": 2,\n",
        "      #     \"NVIDIA L4\": 2,\n",
        "      #     \"NVIDIA A100-SXM4-40GB\": 4,\n",
        "      #     }\n",
        "      #   GPU_name = torch.cuda.get_device_name(0)\n",
        "      #   GPU_batch_size = GPU_batch_size_dict[GPU_name] if GPU_name in GPU_batch_size_dict else 2\n",
        "\n",
        "      #   if task_type_value in [\"pair_classification\", \"pair_regression\"]:\n",
        "      #     GPU_batch_size = int(max(GPU_batch_size / 2, 1))\n",
        "      GPU_batch_size = 1\n",
        "      config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "\n",
        "      # accumulate_grad_batches\n",
        "      if batch_size.value == \"Adaptive\":\n",
        "        num_samples = get_length(config.dataset.train_lmdb)\n",
        "        accumulate_grad_samples = get_accumulate_grad_samples(num_samples)\n",
        "\n",
        "      else:\n",
        "        accumulate_grad_samples = int(batch_size.value)\n",
        "\n",
        "      accumulate_grad_batches = max(int(accumulate_grad_samples / GPU_batch_size), 1)\n",
        "\n",
        "      config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "      # For test\n",
        "      config.Trainer.accumulate_grad_batches= 64\n",
        "      config.dataset.dataloader_kwargs.batch_size = 1\n",
        "\n",
        "    else:\n",
        "      raise BaseException(f\"Please make sure `GPU_batch_size`({GPU_batch_size}) and `accumulate_grad_batches`({accumulate_grad_batches}) are both greater than zero!\")\n",
        "\n",
        "    #####################################################################\n",
        "    #          Hyper-parameter config            #\n",
        "    #####################################################################\n",
        "    epoch_value = epoch.value\n",
        "    lr_value = lr.value\n",
        "\n",
        "    #####################################################################\n",
        "    #               Trainer               #\n",
        "    #####################################################################\n",
        "    config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # epoch\n",
        "    config.Trainer.max_epochs = epoch_value\n",
        "    # test only: load the existing model\n",
        "    if config.Trainer.max_epochs == 0 and continue_learning:\n",
        "      config.model.save_path = config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']\n",
        "\n",
        "    # learning rate\n",
        "    config.model.lr_scheduler_kwargs.init_lr = lr_value\n",
        "\n",
        "    # trainer\n",
        "    config.Trainer.limit_train_batches=limit_train_batches\n",
        "    config.Trainer.limit_val_batches=limit_val_batches\n",
        "    config.Trainer.limit_test_batches=limit_test_batches\n",
        "    config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "    # strategy\n",
        "    strategy = {\n",
        "        # - deepspeed\n",
        "        # 'class': 'DeepSpeedStrategy',\n",
        "        # 'stage': 2\n",
        "\n",
        "        # - None\n",
        "        # 'class': None,\n",
        "\n",
        "        # - DP\n",
        "        # 'class': 'DataParallelStrategy',\n",
        "\n",
        "        # - DDP\n",
        "        # 'class': 'DDPStrategy',\n",
        "        # 'find_unused_parameter': True\n",
        "    }\n",
        "    config.Trainer.strategy = strategy\n",
        "\n",
        "    ####################################################################\n",
        "    #          Run the training task           #\n",
        "    ####################################################################\n",
        "    print('='*100)\n",
        "    print(Fore.BLUE+f\"Training task type: {task_type_value}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "    if continue_learning:\n",
        "      print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']}\"+Style.RESET_ALL)\n",
        "    print('='*100)\n",
        "    pprint.pprint(config)\n",
        "    print('='*100)\n",
        "\n",
        "    # Save the metadata file\n",
        "    def add_training_data_type_to_config(metadata_path, metadata):\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(metadata, file, indent=4)\n",
        "    metadata_path = Path(config.model.save_path) / \"metadata.json\"\n",
        "    os.makedirs(config.model.save_path, exist_ok=True)\n",
        "    add_training_data_type_to_config(metadata_path, metadata)\n",
        "\n",
        "    from saprot.scripts.training import finetune\n",
        "    finetune(config)\n",
        "\n",
        "\n",
        "    ####################################################################\n",
        "    #            Modify README              #\n",
        "    ####################################################################\n",
        "    name = model_name\n",
        "    description = '<slot name=\\'description\\'>'\n",
        "\n",
        "    with open(f'{config.model.save_path}/adapter_config.json', 'r') as f:\n",
        "      lora_config = json.load(f)\n",
        "\n",
        "    readme = f'''\n",
        "---\n",
        "\n",
        "base_model: {base_model} \\n\n",
        "library_name: peft\n",
        "\n",
        "---\n",
        "\\n\n",
        "\n",
        "# Model Card for {name}\n",
        "{description}\n",
        "\n",
        "## Task type\n",
        "{original_task_type}\n",
        "\n",
        "## Model input type\n",
        "{metadata[\"training_data_type\"]} Sequence\n",
        "\n",
        "## LoRA config\n",
        "\n",
        "- **r:** {lora_config['r']}\n",
        "- **lora_dropout:** {lora_config['lora_dropout']}\n",
        "- **lora_alpha:** {lora_config['lora_alpha']}\n",
        "- **target_modules:** {lora_config['target_modules']}\n",
        "- **modules_to_save:** {lora_config['modules_to_save']}\n",
        "\n",
        "## Training config\n",
        "\n",
        "- **optimizer:**\n",
        "  - **class:** AdamW\n",
        "  - **betas:** (0.9, 0.98)\n",
        "  - **weight_decay:** 0.01\n",
        "- **learning rate:** {config.model.lr_scheduler_kwargs.init_lr}\n",
        "- **epoch:** {config.Trainer.max_epochs}\n",
        "- **batch size:** {config.dataset.dataloader_kwargs.batch_size * config.Trainer.accumulate_grad_batches}\n",
        "- **precision:** 16-mixed \\n\n",
        "'''\n",
        "    # Write the markdown output to a file\n",
        "    with open(f\"{config.model.save_path}/README.md\", \"w\") as file:\n",
        "      file.write(readme)\n",
        "\n",
        "    ####################################################################\n",
        "    #            Save the adapter            #\n",
        "    ####################################################################\n",
        "    print(Fore.BLUE)\n",
        "    print(f\"Model is saved to \\\"{config.model.save_path}\\\" on Colab Server\")\n",
        "    print(Style.RESET_ALL)\n",
        "\n",
        "    adapter_zip = Path(config.model.save_path) / f\"{model_name}.zip\"\n",
        "    cmd = f\"cd {config.model.save_path} && zip -r {adapter_zip} 'adapter_config.json' 'adapter_model.safetensors' 'README.md' 'metadata.json'\"\n",
        "    os.system(cmd)\n",
        "    # !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\" \"metadata.json\"\n",
        "    # !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\" \"metadata.json\"\n",
        "    print(\"Click to download the model to your local computer\")\n",
        "    if adapter_zip.exists():\n",
        "      file_download(adapter_zip)\n",
        "\n",
        "    finish_hint = HTML(markdown.markdown(\n",
        "        f\"## The training is completed, you can then:\\n\\n\"\n",
        "        \"- **Train other models or continually train this model with more epochs:**\\n\\n\"\n",
        "        \"\\t1. Click the ``Refresh`` button.\\n\\n\"\n",
        "        \"\\t2. Reset your training configuration and start a new training task.\\n\\n\"\n",
        "        \"- **Use this model for prediction:**\\n\\n\"\n",
        "        \"\\t1. Click the ``Go back`` button.\\n\\n\"\n",
        "        \"\\t2. Choose ``I want to use existing models to make prediction`` module.\\n\\n\"\n",
        "        \"\\t3. Choose ``Protein property prediction`` module.\\n\\n\"\n",
        "        \"\\t4. Set the base model option to ``Trained by yourself on ColabSaprot`` and then choose the model you trained for prediction.\\n\\n\"\n",
        "        \"- **Upload this trained model to SaprotHub:**\\n\\n\"\n",
        "        \"\\t1. Click the ``Go back`` button.\\n\\n\"\n",
        "        \"\\t2. Choose ``I want to share my model publicly`` module.\\n\\n\"\n",
        "        \"\\t3. Complete a basic description of your model and upload it.\"\n",
        "        ))\n",
        "    display(finish_hint)\n",
        "\n",
        "  # Set click events\n",
        "  task_type.observe(change_task_type, names='value')\n",
        "  model_type.observe(change_model_type, names='value')\n",
        "  data_type.observe(change_data_type, names=\"value\")\n",
        "  data_src_type.observe(change_data_src_type, names='value')\n",
        "  advanced_setting.on_click(click_advanced_setting)\n",
        "\n",
        "  start_btn.on_click(\n",
        "      # disable_wrapper(\n",
        "      #     lambda btn: start_thread(start_training, (btn,))\n",
        "      #     )\n",
        "      lambda btn: start_thread(start_training, (btn,))\n",
        "      )\n",
        "\n",
        "  # Set default state\n",
        "  for item in [lora_hint, lora_r, lora_dropout_box, lora_alpha_box, trainer_hint, limit_train_batches_box, limit_val_batches_box, limit_test_batches_box,]:\n",
        "    item.layout.display = \"none\"\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#          Choose prediction task              #\n",
        "######################################################################\n",
        "def choose_pred_task():\n",
        "  global refresh_module\n",
        "  refresh_module = choose_pred_task\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "\n",
        "  question = HTML(markdown.markdown(\"## ColabSaprot supports multiple prediction tasks, which one would you like to choose?\"))\n",
        "  normal_pred = Button(description='Protein property prediction', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "  normal_intro = HTML(markdown.markdown(\n",
        "    f\"This section enables property prediction using well-trained models for various protein characteristics, \"\n",
        "    \"including stability, subcellular localization, and solubility, etc. Users can leverage either models \"\n",
        "    \"available in <a href='https://huggingface.co/SaProtHub' target='blank'>SaprotHub</a> or their previously trained models for predictions. The model input for \"\n",
        "    \"predicting should be consistent with its training format.\"), layout=Layout(width=WIDTH))\n",
        "\n",
        "  zeroshot_pred = Button(description='Mutational effect prediction', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "  zeroshot_intro = HTML(markdown.markdown(\n",
        "  f\"The Mutational Effect Prediction section utilizes the Saprot 650M model. By analyzing the predicted mutation \"\n",
        "  \"scores, users can quickly identify mutations that are likely to enhance specific protein functions, such as \"\n",
        "  \"enzyme activity.\"\n",
        "  ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  design_pred = Button(description='Protein sequence design', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "  design_intro = HTML(markdown.markdown(\n",
        "    f\"The Sequence Design section enables generation of diverse protein sequences compatible with a given \"\n",
        "    \"structural backbone. Users can input their desired backbone coordinates and obtain novel sequences \"\n",
        "    \"optimized for that scaffold.\"\n",
        "    ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  repr_pred = Button(description='Obtain protein-level embeddings', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "  repr_intro = HTML(markdown.markdown(\n",
        "    f\"The Protein Embedding section enables extraction of sequence or structure embeddings using either \"\n",
        "    \"standard Saprot models or custom fine-tuned models for specialized analysis tasks.\"\n",
        "    ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  back_btn = Button(description='Go back', layout=Layout(width='500px', height='30px'))\n",
        "\n",
        "  sep_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "\n",
        "  items = [\n",
        "      question,\n",
        "      sep_hint,\n",
        "      normal_pred,\n",
        "      normal_intro,\n",
        "      sep_hint,\n",
        "      zeroshot_pred,\n",
        "      zeroshot_intro,\n",
        "      sep_hint,\n",
        "      design_pred,\n",
        "      design_intro,\n",
        "      sep_hint,\n",
        "      repr_pred,\n",
        "      repr_intro,\n",
        "      sep_hint,\n",
        "      # back_btn\n",
        "      ]\n",
        "\n",
        "  # Set click events\n",
        "  normal_pred.on_click(partial(jump, next=protein_property_prediction))\n",
        "  zeroshot_pred.on_click(partial(jump, next=start_mut_pred))\n",
        "  design_pred.on_click(partial(jump, next=protein_sequence_design))\n",
        "  repr_pred.on_click(partial(jump, next=obtain_protein_embedding))\n",
        "  back_btn.on_click(partial(jump, next=train_or_pred))\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#          Protein property prediction          #\n",
        "######################################################################\n",
        "def protein_property_prediction():\n",
        "  global refresh_module\n",
        "  refresh_module = protein_property_prediction\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT = \"30px\"\n",
        "  hint = HTML(markdown.markdown(\"## Protein property prediction\"))\n",
        "\n",
        "  task_hint = HTML(markdown.markdown(\"### Choose the prediction task:\"))\n",
        "  task_type_box = ipywidgets.Dropdown(\n",
        "            options=['Protein-level Classification', 'Protein-level Regression', 'Residue-level Classification', \"Protein-protein Classification\", \"Protein-protein Regression\"],\n",
        "            value='Protein-level Classification',\n",
        "            description='Task type:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "          )\n",
        "\n",
        "  task_intro_dict = {\n",
        "      'Protein-level Classification': \"<font color=red>What is **Protein-level Classification:** Given a protein, you have some categories and you want to predict which category the protein belongs to.</font>\",\n",
        "      \"Protein-level Regression\": \"<font color=red>What is **Protein-level Regression:** Given a protein, you want to predict a score about its property such as stability or enzyme activity.</font>\",\n",
        "      \"Residue-level Classification\": \"<font color=red>What is **Residue-level Classification:** Given a protein, you have some categories and for every amino acid you want to predict which category it belongs to.</font>\",\n",
        "      \"Protein-protein Classification\": \"<font color=red>What is **Protein-protein Classification:** Given a pair of proteins, you have some categories and you want to predict which category the pair belongs to.</font>\",\n",
        "      \"Protein-protein Regression\": \"<font color=red>What is **Protein-protein Regression:** Given a pair of proteins, you want to predict a score about its property such as binding affinity.</font>\"\n",
        "  }\n",
        "\n",
        "  task_intro = HTML(markdown.markdown(\n",
        "    \"<font color=red>What is **Protein-level Classification:** Given a protein, you have some categories and you want to predict which category the protein belongs to.</font>\"\n",
        "    ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  num_label_box = ipywidgets.BoundedIntText(\n",
        "        value=2,\n",
        "        min=2,\n",
        "        max=100000000,\n",
        "        step=1,\n",
        "        description='Number of categories:',\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "      )\n",
        "\n",
        "  model_hint = HTML(markdown.markdown(\"### Choose the model for prediction:\"))\n",
        "  model_type_box = ipywidgets.Dropdown(\n",
        "            options=[\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\", \"Multi-models on SaprotHub\"],\n",
        "            value='Trained by yourself on ColabSaprot',\n",
        "            description='Base model:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "          )\n",
        "  model_arg_box = select_adapter_from(None, use_model_from=model_type_box.value)\n",
        "  model_arg_box.layout.width = WIDTH\n",
        "  model_arg_box.description = \"Select your local model:\"\n",
        "  model_arg_box.style = {'description_width': 'initial'}\n",
        "\n",
        "  upload_model_items = get_upload_box(btn_desc=\"Upload Model-<task_name>-<model_size>.zip file\")\n",
        "  set_upload_visibility(upload_model_items, mode=\"none\")\n",
        "\n",
        "  saprothub_link = HTML(markdown.markdown(\n",
        "      \"<font color=red>You could search models using our <a href='https://huggingface.co/spaces/SaProtHub/SaprotHub-search' target='blank'>search engine</a> or from <a href='https://huggingface.co/SaProtHub' target='blank'>SaprotHub</a>\\n\\n\"\n",
        "      \"A model id example: <a href='https://huggingface.co/SaProtHub/Model-Binary_Localization-650M' target='blank'>SaProtHub/Model-Binary_Localization-650M</a>\\n\\n\"\n",
        "      ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  data_type_hint = HTML(markdown.markdown(\"### Uploaded data type:\"))\n",
        "  data_type_box = ipywidgets.RadioButtons(\n",
        "      options=['protein sequence', 'protein structure'],\n",
        "      value=\"protein structure\",\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      )\n",
        "  saprothub_data_type_hint = HTML(markdown.markdown(\n",
        "    \"<font color=red>Note: For models with input type '(Structure-aware) sequence' in SaprotHub, protein structure input is required.</font>\",\n",
        "    ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  upload_type_hint = HTML(markdown.markdown(\"### Choose the number of protein:\"))\n",
        "  upload_type_box = ipywidgets.RadioButtons(\n",
        "        options=['Single file', 'Multiple files'],\n",
        "        layout={'width': 'max-content'}, # If the items' names are long\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        )\n",
        "\n",
        "  input_seq_hint = HTML(markdown.markdown(\"### Input proteins:\"))\n",
        "  input_seq_box_1 = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Input protein sequence',\n",
        "                disabled=False,\n",
        "                description=\"Protein sequence:\",\n",
        "                layout=Layout(width=WIDTH, height=HEIGHT, display=\"none\"),\n",
        "                style={'description_width': 'initial'},\n",
        "                )\n",
        "  input_seq_box_2 = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Input protein sequence',\n",
        "                disabled=False,\n",
        "                description=\"Protein sequence:\",\n",
        "                layout=Layout(width=WIDTH, height=HEIGHT, display=\"none\"),\n",
        "                style={'description_width': 'initial'},\n",
        "                )\n",
        "\n",
        "  chain_hint_1 = HTML(markdown.markdown(\"Chain (to be extracted from the structure):\"))\n",
        "  input_chain_1 = ipywidgets.Text(value=\"A\",placeholder=f'Enter the name of chain here', layout=Layout(width=\"250px\", height=HEIGHT))\n",
        "  upload_hint_1 = HTML(markdown.markdown(\"**Upload protein structure (.pdb / .cif file):**\"))\n",
        "  upload_items_1 = get_upload_box()\n",
        "\n",
        "  chain_hint_2 = HTML(markdown.markdown(\"Chain (to be extracted from the structure):\"), layout=Layout(display=\"none\"))\n",
        "  input_chain_2 = ipywidgets.Text(value=\"A\",placeholder=f'Enter the name of chain here', layout=Layout(width=\"250px\", height=HEIGHT, display=\"none\"))\n",
        "  upload_hint_2 = HTML(markdown.markdown(\"**Upload the second protein structure (.pdb / .cif file):**\"), layout=Layout(display=\"none\"))\n",
        "  upload_items_2 = get_upload_box()\n",
        "  set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "\n",
        "  save_path_hint = HTML(layout={\"display\": \"none\"})\n",
        "  start_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "  start_btn = Button(description='Start prediction', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "  items = [\n",
        "      hint,\n",
        "      task_hint,\n",
        "      task_type_box,\n",
        "      num_label_box,\n",
        "      task_intro,\n",
        "      model_hint,\n",
        "      model_type_box,\n",
        "      model_arg_box,\n",
        "      saprothub_link,\n",
        "      data_type_hint,\n",
        "      data_type_box,\n",
        "      saprothub_data_type_hint,\n",
        "      upload_type_hint,\n",
        "      upload_type_box,\n",
        "      input_seq_hint,\n",
        "      input_seq_box_1,\n",
        "      input_seq_box_2,\n",
        "      upload_hint_1,\n",
        "      ipywidgets.HBox([chain_hint_1, input_chain_1]),\n",
        "      *upload_items_1,\n",
        "      upload_hint_2,\n",
        "      ipywidgets.HBox([chain_hint_2, input_chain_2]),\n",
        "      *upload_items_2,\n",
        "      save_path_hint,\n",
        "      start_hint,\n",
        "      start_btn\n",
        "      ]\n",
        "\n",
        "  # Used to replace input box based on model type\n",
        "  model_arg_box_idx = items.index(model_arg_box)\n",
        "\n",
        "  # Set click events\n",
        "  def change_task_type(change):\n",
        "    set_input_format()\n",
        "    now_type = change[\"new\"]\n",
        "    task_intro.value = markdown.markdown(task_intro_dict[now_type])\n",
        "\n",
        "    if \"Classification\" in now_type:\n",
        "      num_label_box.layout.display = None\n",
        "    else:\n",
        "      num_label_box.layout.display = \"none\"\n",
        "\n",
        "  def change_model_type(change):\n",
        "      model_type_value = change[\"new\"]\n",
        "      if model_type_value == \"Shared by peers on SaprotHub\" or model_type_value == \"Multi-models on SaprotHub\":\n",
        "        saprothub_data_type_hint.layout.display = None\n",
        "      else:\n",
        "        saprothub_data_type_hint.layout.display = \"none\"\n",
        "\n",
        "      if model_type_value == \"Trained by yourself on ColabSaprot\":\n",
        "        new_model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "        new_model_arg_box.layout.width = WIDTH\n",
        "        new_model_arg_box.description = \"Select your local model:\"\n",
        "        new_model_arg_box.style = {'description_width': 'initial'}\n",
        "        saprothub_link.layout.display = \"none\"\n",
        "\n",
        "      elif model_type_value == \"Shared by peers on SaprotHub\":\n",
        "        new_model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "        new_model_arg_box.layout.width = WIDTH\n",
        "        new_model_arg_box.placeholder = \"Enter SaprotHub model id in HuggingFace\"\n",
        "        new_model_arg_box.description = \"Model id:\"\n",
        "        saprothub_link.layout.display = None\n",
        "\n",
        "      elif model_type_value == \"Saved in your local computer\":\n",
        "        set_upload_visibility(upload_model_items, \"default\")\n",
        "        new_model_arg_box = upload_model_items\n",
        "        saprothub_link.layout.display = \"none\"\n",
        "\n",
        "      elif model_type_value == \"Multi-models on SaprotHub\":\n",
        "        new_model_arg_box = ipywidgets.Textarea(\n",
        "                        value=None,\n",
        "                        placeholder='Enter SaprotHub model ids, one line for one model.\\ne.g.:\\nSaprotHub/model_1\\nSaprotHub/model_2\\n\\nNote: These models should have the same model size, i.e. both are 650M or 35M.',\n",
        "                        description='SaprotHub model ids:',\n",
        "                        disabled=False,\n",
        "                        style={'description_width': 'initial'},\n",
        "                        layout=Layout(width=WIDTH, height=\"140px\"),\n",
        "                        )\n",
        "        saprothub_link.layout.display = \"none\"\n",
        "\n",
        "      else:\n",
        "        new_model_arg_box = HTML(markdown.markdown(\"dummy box\"))\n",
        "        new_model_arg_box.layout.display = \"none\"\n",
        "        saprothub_link.layout.display = \"none\"\n",
        "\n",
        "      if isinstance(new_model_arg_box, list):\n",
        "        new_items = items[:model_arg_box_idx] + new_model_arg_box + items[model_arg_box_idx+1:]\n",
        "        custom_display(*new_items)\n",
        "\n",
        "      else:\n",
        "        items[model_arg_box_idx] = new_model_arg_box\n",
        "        custom_display(*items)\n",
        "\n",
        "  def set_input_format():\n",
        "    data_type = data_type_box.value\n",
        "    upload_type = upload_type_box.value\n",
        "    task_type = task_type_box.value\n",
        "    if \"Protein-protein\" not in task_type:\n",
        "      chain_hint_2.layout.display = \"none\"\n",
        "      input_chain_2.layout.display = \"none\"\n",
        "      upload_hint_2.layout.display = \"none\"\n",
        "      if data_type == \"protein sequence\":\n",
        "        if upload_type == \"Single file\":\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          upload_hint_1.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_1, mode=\"none\")\n",
        "          chain_hint_2.layout.display = \"none\"\n",
        "          input_chain_2.layout.display = \"none\"\n",
        "          upload_hint_2.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "          input_seq_box_1.layout.display = None\n",
        "          input_seq_box_1.description = \"Protein sequence:\"\n",
        "          input_seq_box_2.layout.display = \"none\"\n",
        "\n",
        "        else:\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          chain_hint_2.layout.display = \"none\"\n",
        "          input_chain_2.layout.display = \"none\"\n",
        "          upload_hint_2.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "          upload_hint_1.layout.display = None\n",
        "          set_upload_visibility(upload_items_1, mode=\"default\")\n",
        "          input_seq_box_1.layout.display = \"none\"\n",
        "          input_seq_box_2.layout.display = \"none\"\n",
        "          upload_hint_1.value = markdown.markdown(\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/prediction/dataset/new_aa.jpg?raw=true' height='200px' width='300px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to predict. Please strictly follow the format above **(column names are also needed in the csv file)**. \"\n",
        "              \"Then click the start button.\"\n",
        "              )\n",
        "\n",
        "      else:\n",
        "        upload_hint_1.layout.display = None\n",
        "        set_upload_visibility(upload_items_1, mode=\"default\")\n",
        "        input_seq_box_1.layout.display = \"none\"\n",
        "        input_seq_box_2.layout.display = \"none\"\n",
        "        if upload_type == \"Single file\":\n",
        "          set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "          chain_hint_1.layout.display = None\n",
        "          input_chain_1.layout.display = None\n",
        "          upload_hint_1.value = markdown.markdown(\"**Upload protein structure:**\")\n",
        "\n",
        "        else:\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"default\")\n",
        "          upload_hint_1.value = markdown.markdown(\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/prediction/dataset/new_pdb.jpg?raw=true' height='200px' width='500px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to predict. Please strictly follow the format above **(column names are also needed in the csv file)**. \"\n",
        "              \"Then click the start button.\\n\\n**Upload .csv file:**\"\n",
        "              )\n",
        "          upload_hint_2.layout.display = None\n",
        "          upload_hint_2.value = markdown.markdown(\"**Upload .zip file:**\")\n",
        "    else:\n",
        "      if data_type == \"protein sequence\":\n",
        "        if upload_type == \"Single file\":\n",
        "          upload_hint_1.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_1, mode=\"none\")\n",
        "          upload_hint_2.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          chain_hint_2.layout.display = \"none\"\n",
        "          input_chain_2.layout.display = \"none\"\n",
        "          input_seq_box_1.layout.display = None\n",
        "          input_seq_box_2.layout.display = None\n",
        "          input_seq_box_1.description = \"Protein sequence 1:\"\n",
        "          input_seq_box_2.description = \"Protein sequence 2:\"\n",
        "\n",
        "        else:\n",
        "          upload_hint_1.layout.display = None\n",
        "          set_upload_visibility(upload_items_1, mode=\"default\")\n",
        "          upload_hint_2.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"none\")\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          chain_hint_2.layout.display = \"none\"\n",
        "          input_chain_2.layout.display = \"none\"\n",
        "          input_seq_box_1.layout.display = \"none\"\n",
        "          input_seq_box_2.layout.display = \"none\"\n",
        "          upload_hint_1.value = markdown.markdown(\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/prediction/dataset/pair_aa.jpg?raw=true' height='240px' width='300px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all protein sequences to predict. Please strictly follow the format above **(column names are also needed in the csv file)**. \"\n",
        "              \"Then click the start button.\"\n",
        "              )\n",
        "\n",
        "      else:\n",
        "        upload_hint_1.layout.display = None\n",
        "        set_upload_visibility(upload_items_1, mode=\"default\")\n",
        "        input_seq_box_1.layout.display = \"none\"\n",
        "        input_seq_box_2.layout.display = \"none\"\n",
        "        if upload_type == \"Single file\":\n",
        "          upload_hint_2.layout.display = None\n",
        "          set_upload_visibility(upload_items_2, mode=\"default\")\n",
        "          chain_hint_1.layout.display = None\n",
        "          input_chain_1.layout.display = None\n",
        "          chain_hint_2.layout.display = None\n",
        "          input_chain_2.layout.display = None\n",
        "          upload_hint_1.value = markdown.markdown(\"**Upload protein structure 1:**\")\n",
        "          upload_hint_2.value = markdown.markdown(\"**Upload protein structure 2:**\")\n",
        "\n",
        "        else:\n",
        "          chain_hint_1.layout.display = \"none\"\n",
        "          input_chain_1.layout.display = \"none\"\n",
        "          chain_hint_2.layout.display = \"none\"\n",
        "          input_chain_2.layout.display = \"none\"\n",
        "          set_upload_visibility(upload_items_2, mode=\"default\")\n",
        "          upload_hint_1.value = markdown.markdown(\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/prediction/dataset/pair_pdb.jpg?raw=true' height='200px' width='900px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file and a ``.zip`` file that stores all protein structures to predict. Please strictly follow the format above **(column names are also needed in the csv file)**. \"\n",
        "              \"Then click the start button.\\n\\n**Upload .csv file:**\"\n",
        "              )\n",
        "          upload_hint_2.layout.display = None\n",
        "          upload_hint_2.value = markdown.markdown(\"**Upload .zip file:**\")\n",
        "\n",
        "  def change_data_type(change):\n",
        "    set_input_format()\n",
        "\n",
        "  def change_upload_type(change):\n",
        "    set_input_format()\n",
        "\n",
        "  def start_prediction(button):\n",
        "    print(\"Start prediction....\")\n",
        "\n",
        "    task_type = task_type_box.value\n",
        "    model_type = model_type_box.value\n",
        "    model_arg = items[model_arg_box_idx].value\n",
        "\n",
        "    if model_type == \"Saved in your local computer\":\n",
        "      zip_path = get_upload_file_path(upload_model_items)\n",
        "      name = os.path.basename(zip_path)\n",
        "      save_dir = ADAPTER_HOME / \"Upload\" / name.rsplit('.', 1)[0]\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "      # unzip model.zip\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(save_dir)\n",
        "      os.remove(zip_path)\n",
        "      model_arg = save_dir\n",
        "\n",
        "    if model_type == \"Multi-models on SaprotHub\":\n",
        "      model_arg_list = model_arg.strip().split(\"\\n\")\n",
        "      for arg in model_arg_list:\n",
        "        model_task_type = load_task_type_from_model(model_type, arg.strip())\n",
        "        # Check compatibility between chosen task and model\n",
        "        assert task_type == model_task_type, f\"The model {arg.strip()} was trained on {model_task_type} task, which is not suitable for the {task_type} task you choose!\"\n",
        "\n",
        "    else:\n",
        "      model_task_type = load_task_type_from_model(model_type, model_arg)\n",
        "      # Check compatibility between chosen task and model\n",
        "      assert task_type == model_task_type, f\"The model you choose was trained on {model_task_type} task, which is not suitable for the {task_type} task you choose!\"\n",
        "\n",
        "    # Check data type compatibility\n",
        "    input_data_type = \"AA\" if data_type_box.value == \"protein sequence\" else \"SA\"\n",
        "\n",
        "    if model_type == \"Multi-models on SaprotHub\":\n",
        "      model_arg_list = model_arg.strip().split(\"\\n\")\n",
        "      for arg in model_arg_list:\n",
        "        model_data_type = load_data_type_from_model(model_type, arg.strip())\n",
        "        if model_data_type != input_data_type:\n",
        "          if model_data_type == \"SA\":\n",
        "            raise Exception(f\"Error: The model {arg.strip()} was trained on protein structures. So you have to upload protein structures!\")\n",
        "          else:\n",
        "            raise Exception(f\"Error: The model {arg.strip()} was trained on protein sequences. So you have to upload protein sequences!\")\n",
        "\n",
        "    else:\n",
        "      model_data_type = load_data_type_from_model(model_type, model_arg)\n",
        "      if model_data_type != input_data_type:\n",
        "        if model_data_type == \"SA\":\n",
        "          raise Exception(\"Error: The model you choose was trained on protein structures. So you have to upload protein structures!\")\n",
        "        else:\n",
        "          raise Exception(\"Error: The model you choose was trained on protein sequences. So you have to upload protein sequences!\")\n",
        "\n",
        "    # Process input\n",
        "    data_type = data_type_box.value\n",
        "    upload_type = upload_type_box.value\n",
        "    rows = []\n",
        "    if \"Protein-protein\" not in task_type and data_type == \"protein sequence\" and upload_type == \"Single file\":\n",
        "      df = pd.DataFrame(columns=[\"protein\"])\n",
        "      aa_seq = input_seq_box_1.value\n",
        "      df = df._append({\"protein\": aa_seq}, ignore_index=True)\n",
        "\n",
        "      sa_seq = \"\".join(aa + \"#\" for aa in aa_seq)\n",
        "      rows.append(sa_seq)\n",
        "\n",
        "    elif \"Protein-protein\" not in task_type and data_type == \"protein sequence\" and upload_type == \"Multiple files\":\n",
        "      csv_path = get_upload_file_path(upload_items_1)\n",
        "      assert csv_path.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "\n",
        "      df = pd.read_csv(csv_path)\n",
        "      rows = []\n",
        "      for aa_seq in df[\"protein\"].values:\n",
        "        sa_seq = \"\".join(aa + \"#\" for aa in aa_seq)\n",
        "        rows.append(sa_seq)\n",
        "\n",
        "    elif \"Protein-protein\" not in task_type and data_type == \"protein structure\" and upload_type == \"Single file\":\n",
        "      pdb_path = get_upload_file_path(upload_items_1)\n",
        "      name = os.path.basename(pdb_path)\n",
        "      assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "      chain = input_chain_1.value\n",
        "      sa_seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, chains=[chain], plddt_mask=True)[chain][-1]\n",
        "      rows.append(sa_seq)\n",
        "\n",
        "      df = pd.DataFrame(columns=[\"protein\"])\n",
        "      df = df._append({\"protein\": name}, ignore_index=True)\n",
        "\n",
        "    elif \"Protein-protein\" not in task_type and data_type == \"protein structure\" and upload_type == \"Multiple files\":\n",
        "      csv_path = get_upload_file_path(upload_items_1)\n",
        "      csv_name = os.path.basename(csv_path)\n",
        "      zip_path = get_upload_file_path(upload_items_2)\n",
        "      zip_name = os.path.basename(zip_path)\n",
        "      assert csv_path.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "      assert zip_path.endswith(\".zip\"), \"Please upload file with correct format (.zip)!\"\n",
        "\n",
        "      # Unzip structures\n",
        "      upload_path = Path(UPLOAD_FILE_HOME)\n",
        "      upload_path.mkdir(parents=True, exist_ok=True)\n",
        "      prefix = zip_name.rsplit(\".\", 1)[0]\n",
        "      struc_dir = upload_path / prefix\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(struc_dir)\n",
        "\n",
        "      # If the unzipped file is a directory\n",
        "      unzipped_files = os.listdir(struc_dir)\n",
        "      if len(unzipped_files) == 1:\n",
        "        file_path = f\"{struc_dir}/{unzipped_files[0]}\"\n",
        "        if os.path.isdir(file_path):\n",
        "          struc_dir = file_path\n",
        "\n",
        "      df = pd.read_csv(csv_path)\n",
        "      pdbs = []\n",
        "      for pdb_name, chain in df[[\"protein\", \"chain\"]].values:\n",
        "        pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "        pdbs.append([pdb_path, chain])\n",
        "      rows = pdb2sa(pdbs)\n",
        "\n",
        "    elif \"Protein-protein\" in task_type and data_type == \"protein sequence\" and upload_type == \"Single file\":\n",
        "      df = pd.DataFrame(columns=[\"protein_1\", \"protein_2\"])\n",
        "      aa_seq_1 = input_seq_box_1.value\n",
        "      aa_seq_2 = input_seq_box_2.value\n",
        "      df = df._append({\"protein_1\": aa_seq_1, \"protein_2\": aa_seq_2}, ignore_index=True)\n",
        "\n",
        "      sa_seq_1 = \"\".join(aa + \"#\" for aa in aa_seq_1)\n",
        "      sa_seq_2 = \"\".join(aa + \"#\" for aa in aa_seq_2)\n",
        "      rows.append([sa_seq_1, sa_seq_2])\n",
        "\n",
        "    elif \"Protein-protein\" in task_type and data_type == \"protein sequence\" and upload_type == \"Multiple files\":\n",
        "      csv_path = get_upload_file_path(upload_items_1)\n",
        "      assert csv_path.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "\n",
        "      df = pd.read_csv(csv_path)\n",
        "      rows = []\n",
        "      for aa_seq_1, aa_seq_2 in df[[\"protein_1\", \"protein_2\"]].values:\n",
        "        sa_seq_1 = \"\".join(aa + \"#\" for aa in aa_seq_1)\n",
        "        sa_seq_2 = \"\".join(aa + \"#\" for aa in aa_seq_2)\n",
        "        rows.append([sa_seq_1, sa_seq_2])\n",
        "\n",
        "    elif \"Protein-protein\" in task_type and data_type == \"protein structure\" and upload_type == \"Single file\":\n",
        "      pdb_path_1 = get_upload_file_path(upload_items_1)\n",
        "      name_1 = os.path.basename(pdb_path_1)\n",
        "      pdb_path_2 = get_upload_file_path(upload_items_2)\n",
        "      name_2 = os.path.basename(pdb_path_2)\n",
        "      assert name_1.endswith(\".pdb\") or name_1.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "      assert name_2.endswith(\".pdb\") or name_2.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "      chain_1 = input_chain_1.value\n",
        "      sa_seq_1 = get_struc_seq(FOLDSEEK_PATH, pdb_path_1, chains=[chain_1], plddt_mask=True)[chain_1][-1]\n",
        "\n",
        "      chain_2 = input_chain_2.value\n",
        "      sa_seq_2 = get_struc_seq(FOLDSEEK_PATH, pdb_path_2, chains=[chain_2], plddt_mask=True)[chain_2][-1]\n",
        "\n",
        "      rows.append([sa_seq_1, sa_seq_2])\n",
        "\n",
        "      df = pd.DataFrame(columns=[\"protein_1\", \"protein_2\"])\n",
        "      df = df._append({\"protein_1\": name_1, \"protein_2\": name_2}, ignore_index=True)\n",
        "\n",
        "    elif \"Protein-protein\" in task_type and data_type == \"protein structure\" and upload_type == \"Multiple files\":\n",
        "      csv_path = get_upload_file_path(upload_items_1)\n",
        "      csv_name = os.path.basename(csv_path)\n",
        "      zip_path = get_upload_file_path(upload_items_2)\n",
        "      zip_name = os.path.basename(zip_path)\n",
        "      assert csv_path.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "      assert zip_path.endswith(\".zip\"), \"Please upload file with correct format (.zip)!\"\n",
        "\n",
        "      # Unzip structures\n",
        "      upload_path = Path(UPLOAD_FILE_HOME)\n",
        "      upload_path.mkdir(parents=True, exist_ok=True)\n",
        "      prefix = zip_name.rsplit(\".\", 1)[0]\n",
        "      struc_dir = upload_path / prefix\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(struc_dir)\n",
        "\n",
        "      # If the unzipped file is a directory\n",
        "      unzipped_files = os.listdir(struc_dir)\n",
        "      if len(unzipped_files) == 1:\n",
        "        file_path = f\"{struc_dir}/{unzipped_files[0]}\"\n",
        "        if os.path.isdir(file_path):\n",
        "          struc_dir = file_path\n",
        "\n",
        "      df = pd.read_csv(csv_path)\n",
        "      pdbs_1 = []\n",
        "      for pdb_name, chain in df[[\"protein_1\", \"chain_1\"]].values:\n",
        "        pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "        pdbs_1.append([pdb_path, chain])\n",
        "\n",
        "      sa_seqs_1 = pdb2sa(pdbs_1)\n",
        "\n",
        "      pdbs_2 = []\n",
        "      for pdb_name, chain in df[[\"protein_2\", \"chain_2\"]].values:\n",
        "        pdb_path = f\"{struc_dir}/{pdb_name}\"\n",
        "        pdbs_2.append([pdb_path, chain])\n",
        "\n",
        "      sa_seqs_2 = pdb2sa(pdbs_2)\n",
        "\n",
        "      rows = list(zip(sa_seqs_1, sa_seqs_2))\n",
        "\n",
        "    else:\n",
        "      raise\n",
        "\n",
        "    return_label, output_file = make_predictions(df, rows, num_label_box.value, model_type, model_arg)\n",
        "\n",
        "    report_str = \"\"\n",
        "    if return_label is not None:\n",
        "      report_str = f\"The predicted label is **{return_label}**\"\n",
        "\n",
        "    dividing_line = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "    pred_hint = HTML(markdown.markdown(\n",
        "      f\"{report_str}\\n\\n\"\n",
        "      f\"All prediction results have been saved to ``{output_file}``. You can click to download the results.\\n\\n\"\n",
        "      )\n",
        "    )\n",
        "    download_btn = generate_download_btn(output_file)\n",
        "\n",
        "    new_items = items + [dividing_line, pred_hint, download_btn]\n",
        "    custom_display(*new_items)\n",
        "\n",
        "\n",
        "  task_type_box.observe(change_task_type, names=\"value\")\n",
        "  model_type_box.observe(change_model_type, names=\"value\")\n",
        "  data_type_box.observe(change_data_type, names=\"value\")\n",
        "  upload_type_box.observe(change_upload_type, names='value')\n",
        "  start_btn.on_click(\n",
        "      lambda btn: start_thread(start_prediction, (btn,))\n",
        "      )\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#       Start mutational effect predction         #\n",
        "######################################################################\n",
        "def start_mut_pred():\n",
        "  global refresh_module\n",
        "  refresh_module = start_mut_pred\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "  question = HTML(markdown.markdown(\"## Please choose the mutation task you want to perform\"))\n",
        "  sep_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "\n",
        "  single_btn = Button(description='Single-site or Multi-site mutagenesis', layout=Layout(width=WIDTH, height='30px'), button_style=\"info\")\n",
        "  single_intro = HTML(markdown.markdown(\n",
        "      f\"You have to specify a mutation mannually and the model will output a score to predict whether this mutation \"\n",
        "      \"is good or not.\"\n",
        "      ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  all_btn = Button(description='Single-site saturation mutagenesis', layout=Layout(width=WIDTH, height='30px'), button_style=\"info\")\n",
        "  all_intro = HTML(markdown.markdown(\n",
        "      f\"You don't have to specify mutation mannually. The model will predict scores for all single-site mutations given a protein, \"\n",
        "      \"e.g. given a protein with the length of 200 the model will output 200 * 20 scores, each for a single-site mutation.\"\n",
        "      ), layout=Layout(width=WIDTH))\n",
        "\n",
        "  items = [\n",
        "      question,\n",
        "      sep_hint,\n",
        "      single_btn,\n",
        "      single_intro,\n",
        "      sep_hint,\n",
        "      all_btn,\n",
        "      all_intro,\n",
        "      sep_hint,\n",
        "  ]\n",
        "\n",
        "  # Set click events\n",
        "  single_btn.on_click(partial(jump, next=single_mut_pred))\n",
        "  all_btn.on_click(partial(jump, next=saturation_mut_pred))\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#      Single-site or Multi-site mutagenesis        #\n",
        "######################################################################\n",
        "def single_mut_pred():\n",
        "  global refresh_module\n",
        "  refresh_module = single_mut_pred\n",
        "\n",
        "  hint = HTML(markdown.markdown(\"# Single-site or Multi-site mutagenesis\\n\\n## Please upload the protein structure\\n If you only have protein sequence, you could use <a href='https://alphafoldserver.com' target='blank'>AlphaFold server</a> to predict its structure and upload it here.\"))\n",
        "\n",
        "  chain_hint = HTML(markdown.markdown(\"Chain (to be extracted from the structure):\"))\n",
        "  input_chain = ipywidgets.Text(value=\"A\",placeholder=f'Enter the name of chain here', layout=Layout(width='500px', height='30px'))\n",
        "  upload_hint = HTML(markdown.markdown(\"Upload the protein structure:\"))\n",
        "  # upload_btn = ipywidgets.FileUpload(accept='',multiple=False, description=\"Upload protein structure (.pdb / .cif file)\", layout=Layout(width='500px', height='30px'))\n",
        "  # upload_btn = ipywidgets.Button(description=\"Upload protein structure (.pdb / .cif file)\", layout=Layout(width='500px', height='30px'))\n",
        "  upload_items = get_upload_box()\n",
        "  upload_ok_btn = ipywidgets.Button(\n",
        "      description=\"Submit\",\n",
        "      layout=Layout(width='500px', height='30px'),\n",
        "      button_style=\"info\",\n",
        "      )\n",
        "\n",
        "  items = [\n",
        "      hint,\n",
        "      chain_hint,\n",
        "      input_chain,\n",
        "      upload_hint,\n",
        "      *upload_items,\n",
        "      upload_ok_btn,\n",
        "  ]\n",
        "\n",
        "  # Set click events\n",
        "  def on_upload_file(button):\n",
        "    save_path = get_upload_file_path(upload_items)\n",
        "    name = os.path.basename(save_path)\n",
        "    assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "    chain = input_chain.value\n",
        "    protein_list = [(save_path, chain)]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True, verbose=False)\n",
        "    seqs = mprs.run()\n",
        "\n",
        "    assert len(seqs) != 0, f\"The specified chain '{chain}' does not exist in the structure!\"\n",
        "\n",
        "    sa_seq = seqs[0].split(\"\\t\")[-1]\n",
        "    aa_seq = sa_seq[0::2]\n",
        "    struc_seq = sa_seq[1::2].replace(\"#\", \"\\#\")\n",
        "\n",
        "    seq_info = HTML(markdown.markdown(f\"**{name}**\\n\\n**Foldseek sequence (\\\"#\\\" means low pLDDT positions that are masked):**\\n\\n{struc_seq}\\n\\n**Amino acid sequence:**\\n\\n{aa_seq}\"))\n",
        "\n",
        "    # Mutation information box\n",
        "    input_hint = HTML(markdown.markdown(\n",
        "        \"**Please input the mutation information:**\"\n",
        "        )\n",
        "    )\n",
        "    pred_num_box = ipywidgets.RadioButtons(\n",
        "      options=['Single variant', 'multiple variants'],\n",
        "      disabled=False,\n",
        "      layout={'width': 'max-content'}, # If the items' names are long\n",
        "      description=\"How many variants do you want to predict?\",\n",
        "      style={'description_width': 'initial'},\n",
        "      )\n",
        "\n",
        "    mut_hint =  HTML(markdown.markdown(\n",
        "        \"For single-site mutation, e.g. M1E means mutating the amino acid M to E at first position. \"\n",
        "        \"For multi-site mutation, you are expected to separate each position by ':', e.g. M1E:P2V\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    upload_csv_hint = HTML(markdown.markdown(\n",
        "              \"<img src='https://github.com/westlake-repl/SaProtHub/blob/dev/Figure/prediction/dataset/mut.jpg?raw=true' height='200px' width='400px' align='center'>\\n\\n\"\n",
        "              \"Please upload a ``.csv`` file that contains all mutations to predict. Please strictly follow the format above **(column names are also needed in the csv file)**. \"\n",
        "              \"Then click the start button.\"\n",
        "              ), layout=Layout(display=\"none\"))\n",
        "\n",
        "    upload_csv_items = get_upload_box()\n",
        "    for item in upload_csv_items[:4]:\n",
        "      item.layout.display = \"none\"\n",
        "\n",
        "    input_mut = ipywidgets.Text(\n",
        "        placeholder=\"Enter mutation information here, e.g. M1E\", layout=Layout(width='1000px', height='30px'))\n",
        "    submit_btn = Button(description='Calculate mutation score', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "\n",
        "\n",
        "    new_items = items + [\n",
        "        seq_info,\n",
        "        input_hint,\n",
        "        pred_num_box,\n",
        "        mut_hint,\n",
        "        input_mut,\n",
        "        upload_csv_hint,\n",
        "        *upload_csv_items,\n",
        "        submit_btn\n",
        "        ]\n",
        "\n",
        "    def change_pred_num(change):\n",
        "      now_type = change[\"new\"]\n",
        "      if now_type == \"Single variant\":\n",
        "        upload_csv_hint.layout.display = \"none\"\n",
        "        set_upload_visibility(upload_csv_items, \"none\")\n",
        "        input_mut.layout.display = None\n",
        "\n",
        "      else:\n",
        "        upload_csv_hint.layout.display = None\n",
        "        set_upload_visibility(upload_csv_items, \"default\")\n",
        "        input_mut.layout.display = \"none\"\n",
        "\n",
        "    # Click to calculate mutation score\n",
        "    def calc_mut_score(button):\n",
        "      if pred_num_box.value == \"Single variant\":\n",
        "        mut_info = input_mut.value\n",
        "        # Skip empty input\n",
        "        if mut_info.strip() == \"\":\n",
        "          return\n",
        "\n",
        "        for single in mut_info.split(\":\"):\n",
        "          # Assert position valid\n",
        "          pos = int(single[1:-1])\n",
        "          assert 0 < pos <= len(aa_seq), f\"The mutation position should be greater than 0 and not greater than the length of sequence ({len(aa_seq)})\"\n",
        "\n",
        "          # Assert amino acid valid\n",
        "          ori_aa = single[0]\n",
        "          assert ori_aa == aa_seq[pos-1], f\"The amino acid at position {pos} is not {ori_aa}! Please refer to the amino acid sequence above to find correct position (the counting starts from 1).\"\n",
        "\n",
        "        print(f\"Predict the mutation score for {mut_info}...\")\n",
        "        score = predict_mut(sa_seq, mut_info)\n",
        "        score_hint = HTML(markdown.markdown(\n",
        "            f\"The score for {mut_info} is <font color=red>{score.item()}</font>. **Please check <a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#introduction-of-saprot-mutation-score' target='blank'>here</a> for the description of the mutation score.**\"))\n",
        "        new_items = items + [seq_info, input_hint, pred_num_box, input_mut, upload_csv_hint, mut_hint, *upload_csv_items, submit_btn, score_hint]\n",
        "        custom_display(*new_items)\n",
        "\n",
        "      else:\n",
        "        save_path = get_upload_file_path(upload_csv_items)\n",
        "        name = os.path.basename(save_path)\n",
        "        assert name.endswith(\".csv\"), \"Please upload file with correct format (.csv)!\"\n",
        "\n",
        "        df = pd.read_csv(save_path)\n",
        "        print(f\"Predicting...\")\n",
        "        zeroshot_model = load_zeroshot_model()\n",
        "        scores = []\n",
        "        for mut_info in tqdm(df[\"mutation\"].values):\n",
        "          for single in mut_info.split(\":\"):\n",
        "            # Assert position valid\n",
        "            pos = int(single[1:-1])\n",
        "            assert 0 < pos <= len(aa_seq), f\"For {mut_info}, The mutation position should be greater than 0 and not greater than the length of sequence ({len(aa_seq)})\"\n",
        "\n",
        "            # Assert amino acid valid\n",
        "            ori_aa = single[0]\n",
        "            assert ori_aa == aa_seq[pos-1], f\"For {mut_info}, The amino acid at position {pos} is not {ori_aa}! Please refer to the amino acid sequence above to find correct position (the counting starts from 1).\"\n",
        "\n",
        "          score = zeroshot_model.predict_mut(sa_seq, mut_info)\n",
        "          scores.append(score.item())\n",
        "\n",
        "        df[\"score\"] = scores\n",
        "        save_path = OUTPUT_HOME / \"mutation_results.csv\"\n",
        "        df.to_csv(save_path, index=False)\n",
        "        download_btn = generate_download_btn(save_path)\n",
        "\n",
        "        pred_hint = HTML(markdown.markdown(\n",
        "          f\"Prediction results have been saved to ``{save_path}``. You can click to download the results.\"\n",
        "          )\n",
        "        )\n",
        "\n",
        "        new_items = items + [seq_info, input_hint, pred_num_box, input_mut, upload_csv_hint, mut_hint, *upload_csv_items, submit_btn, pred_hint, download_btn]\n",
        "        custom_display(*new_items)\n",
        "\n",
        "    pred_num_box.observe(change_pred_num, names=\"value\")\n",
        "    submit_btn.on_click(\n",
        "        lambda btn: start_thread(calc_mut_score, (btn,))\n",
        "        )\n",
        "    custom_display(*new_items)\n",
        "\n",
        "  upload_ok_btn.on_click(on_upload_file)\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#       Single-site saturation mutagenesis         #\n",
        "######################################################################\n",
        "def saturation_mut_pred():\n",
        "  global refresh_module\n",
        "  refresh_module = saturation_mut_pred\n",
        "\n",
        "  hint = HTML(markdown.markdown(\"# Single-site saturation mutagenesis\\n\\n## Please upload the protein structure\\n If you only have protein sequence, you could use <a href='https://alphafoldserver.com' target='blank'>AlphaFold server</a> to predict its structure and upload it here.\"))\n",
        "\n",
        "  chain_hint = HTML(markdown.markdown(\"Chain (to be extracted from the structure):\"))\n",
        "  input_chain = ipywidgets.Text(value=\"A\",placeholder=f'Enter the name of chain here', layout=Layout(width='500px', height='30px'))\n",
        "  upload_hint = HTML(markdown.markdown(\"Upload the protein structure:\"))\n",
        "  upload_items = get_upload_box()\n",
        "  upload_ok_btn = ipywidgets.Button(\n",
        "      description=\"Submit\",\n",
        "      layout=Layout(width='500px', height='30px'),\n",
        "      button_style=\"info\",\n",
        "      )\n",
        "\n",
        "  items = [\n",
        "      hint,\n",
        "      chain_hint,\n",
        "      input_chain,\n",
        "      upload_hint,\n",
        "      *upload_items,\n",
        "      upload_ok_btn,\n",
        "  ]\n",
        "\n",
        "  # Set click events\n",
        "  def on_upload_file(change):\n",
        "    save_path = get_upload_file_path(upload_items)\n",
        "    name = os.path.basename(save_path)\n",
        "    assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "    chain = input_chain.value\n",
        "    protein_list = [(save_path, chain)]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True, verbose=False)\n",
        "    seqs = mprs.run()\n",
        "\n",
        "    assert len(seqs) != 0, f\"The specified chain '{chain}' does not exist in the structure!\"\n",
        "\n",
        "    sa_seq = seqs[0].split(\"\\t\")[-1]\n",
        "    aa_seq = sa_seq[0::2]\n",
        "    struc_seq = sa_seq[1::2].replace(\"#\", \"\\#\")\n",
        "\n",
        "    seq_info = HTML(markdown.markdown(f\"**{name}**\\n\\n**Foldseek sequence (\\\"#\\\" means low pLDDT positions that are masked):**\\n\\n{struc_seq}\\n\\n**Amino acid sequence:**\\n\\n{aa_seq}\"))\n",
        "    submit_btn = Button(description='Calculate mutation score for all single-site mutations', layout=Layout(width='500px', height='30px'), button_style=\"info\")\n",
        "\n",
        "\n",
        "    new_items = items + [seq_info, submit_btn]\n",
        "\n",
        "    # Click to calculate mutation score\n",
        "    def calc_mut_score(button):\n",
        "      print(f\"Predicting mutation scores for all single-site mutations...\")\n",
        "      save_path = predict_all_mut(sa_seq)\n",
        "      score_hint = HTML(markdown.markdown(f\"The result has been saved to {save_path}. You can click to download the file.\\n\\n\"\n",
        "      \"**Please check <a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#introduction-of-saprot-mutation-score' target='blank'>here</a> for the description of the mutation score.**\"))\n",
        "      download_btn = generate_download_btn(save_path)\n",
        "\n",
        "      new_items = items + [seq_info, submit_btn, score_hint, download_btn]\n",
        "      custom_display(*new_items)\n",
        "\n",
        "    submit_btn.on_click(\n",
        "        # disable_wrapper(\n",
        "        #     lambda btn: start_thread(calc_mut_score, (btn,))\n",
        "        #     )\n",
        "        lambda btn: start_thread(calc_mut_score, (btn,))\n",
        "          )\n",
        "    custom_display(*new_items)\n",
        "\n",
        "  upload_ok_btn.on_click(on_upload_file)\n",
        "\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#          Protein sequence design           #\n",
        "#####################################################################\n",
        "def protein_sequence_design():\n",
        "  global refresh_module\n",
        "  refresh_module = protein_sequence_design\n",
        "\n",
        "  title = HTML(markdown.markdown(\"## Protein sequence design\"))\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT= \"30px\"\n",
        "\n",
        "  task_hint = HTML(markdown.markdown(\"### Please upload the structure backbone:\"))\n",
        "  chain_hint = HTML(markdown.markdown(\"Chain (to be extracted from the structure):\"))\n",
        "  input_chain = ipywidgets.Text(value=\"A\",placeholder=f'Enter the name of chain here', layout=Layout(width=WIDTH, height=HEIGHT))\n",
        "  upload_hint = HTML(markdown.markdown(\"Upload the protein structure:\"))\n",
        "  upload_items = get_upload_box()\n",
        "  upload_ok_btn = ipywidgets.Button(\n",
        "      description=\"Submit\",\n",
        "      layout=Layout(width='500px', height='30px'),\n",
        "      button_style=\"info\",\n",
        "      )\n",
        "\n",
        "  items = [\n",
        "      title,\n",
        "      task_hint,\n",
        "      chain_hint,\n",
        "      input_chain,\n",
        "      upload_hint,\n",
        "      *upload_items,\n",
        "      upload_ok_btn,\n",
        "  ]\n",
        "\n",
        "  # Set click events\n",
        "  def parse_structure(change):\n",
        "    save_path = get_upload_file_path(upload_items)\n",
        "    name = os.path.basename(save_path)\n",
        "    assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "    chain = input_chain.value\n",
        "    protein_list = [(save_path, chain)]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True, verbose=False)\n",
        "    seqs = mprs.run()\n",
        "\n",
        "    assert len(seqs) != 0, f\"The specified chain '{chain}' does not exist in the structure!\"\n",
        "\n",
        "    sa_seq = seqs[0].split(\"\\t\")[-1]\n",
        "    aa_seq = sa_seq[0::2]\n",
        "    struc_seq = sa_seq[1::2].replace(\"#\", \"\\#\")\n",
        "\n",
        "    seq_info = HTML(markdown.markdown(\n",
        "        f\"**Foldseek sequence of chain {chain} (\\\"#\\\" means low pLDDT positions that are masked):**\\n\\n{struc_seq}\\n\\n**Backbone visualization of {name} (chains are displayed with different colors):**\"\n",
        "        ), layout=Layout(width=WIDTH))\n",
        "\n",
        "    # Sampling config\n",
        "    sampling_method_box = ipywidgets.Dropdown(\n",
        "              options=['argmax', 'multinomial'],\n",
        "              value='argmax',\n",
        "              description='Sampling methodology:',\n",
        "              disabled=False,\n",
        "              layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "              style={'description_width': 'initial'},\n",
        "            )\n",
        "    sampling_num = ipywidgets.BoundedIntText(\n",
        "          value=1,\n",
        "          min=1,\n",
        "          max=100,\n",
        "          step=1,\n",
        "          description='Sampling num:',\n",
        "          disabled=False,\n",
        "          style={'description_width': 'initial'},\n",
        "          layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "        )\n",
        "    sampling_intro = HTML(markdown.markdown(\n",
        "        f\"*For each position, Saprot will predict the probability of every amino acid occuring at this position. ***argmax*** means the model \"\n",
        "        \"will choose the amino acid with highest probability as its prediction. ***multinomial*** means the model will sample amino acids based on \"\n",
        "        \"the probability distribution.*\"\n",
        "        ), layout=Layout(width=WIDTH))\n",
        "\n",
        "    save_name = ipywidgets.Text(\n",
        "          value=\"predicted_seq\",\n",
        "          description='Save name:',\n",
        "          disabled=False,\n",
        "          style={'description_width': 'initial'},\n",
        "          layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "        )\n",
        "\n",
        "\n",
        "    pred_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "    pred_btn = Button(description='Predict protein sequence', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "    esmfold_hint = HTML(markdown.markdown(\n",
        "        f\"### You could use ESMFold to evaluate the quality of the predicted sequence\\n\\n\"\n",
        "        \"#### <font color=red>Warning: If you are using Google T4 GPU, it will run out of memory!</font>\"\n",
        "        ))\n",
        "    esmfold_input_box = ipywidgets.Text(\n",
        "          value=\"\",\n",
        "          placeholder = \"Copy the predicted sequence and paste it here\",\n",
        "          disabled=False,\n",
        "          style={'description_width': 'initial'},\n",
        "          layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "        )\n",
        "    esmfold_pred_btn = Button(description='Predict structure', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "    def change_sampling_method(change):\n",
        "      sampling_method = change[\"new\"]\n",
        "      if sampling_method == \"multinomial\":\n",
        "        sampling_num.layout.display = None\n",
        "      else:\n",
        "        sampling_num.layout.display = \"none\"\n",
        "\n",
        "    # Predict protein sequence\n",
        "    def predict(button):\n",
        "      print(\"Predicting sequence...\")\n",
        "\n",
        "      struc_seq = sa_seq[1::2]\n",
        "      masked_aa_seq = \"#\" * len(struc_seq)\n",
        "      methods = sampling_method_box.value\n",
        "      num_samples = sampling_num.value\n",
        "\n",
        "      pred_seqs = inverse_folding(masked_aa_seq, struc_seq, methods, num_samples)\n",
        "      print(pred_seqs)\n",
        "\n",
        "      save_seq_path = f\"{root_dir}/SaprotHub/output/{save_name.value}.fasta\"\n",
        "      pred_seq_hint = \"**Predicted sequences are listed below:**\\n\\n\"\n",
        "      with open(save_seq_path, \"w\") as w:\n",
        "        for i, aa_seq in enumerate(pred_seqs):\n",
        "          pred_seq_hint += f\"{i+1}. {aa_seq}\\n\\n\"\n",
        "          w.write(f\">predicted_seq_{i}\\n{aa_seq}\\n\")\n",
        "\n",
        "      pred_seq_hint += \"You can click the button to dowanload the predictions.\"\n",
        "      pred_seq_hint = HTML(markdown.markdown(pred_seq_hint))\n",
        "      download_btn = generate_download_btn(save_seq_path)\n",
        "\n",
        "      # Update the display\n",
        "      clear_output()\n",
        "      display(*items)\n",
        "      display(seq_info)\n",
        "      show_pdb(save_path, color=\"chain\").show()\n",
        "      display(sampling_method_box, sampling_num, sampling_intro, pred_hint, pred_btn, pred_seq_hint, download_btn, esmfold_hint, esmfold_input_box, esmfold_pred_btn)\n",
        "      display(*system_widgets)\n",
        "\n",
        "      def esmfold_pred(button):\n",
        "        print(\"Predicting structure...\")\n",
        "        aa_seq = esmfold_input_box.value\n",
        "        pred_pdb_path = predict_structure(aa_seq)\n",
        "\n",
        "        # Update the display after using ESMFold\n",
        "        clear_output()\n",
        "        display(*items)\n",
        "        display(seq_info)\n",
        "        show_pdb(save_path, color=\"chain\").show()\n",
        "        display(sampling_method_box, sampling_num, sampling_intro, pred_hint, pred_btn, pred_seq_hint, download_btn, esmfold_hint, esmfold_input_box, esmfold_pred_btn)\n",
        "        print(\"Predicted structure by ESMFold:\")\n",
        "\n",
        "        # Display predicted structure\n",
        "        color = \"lDDT\"\n",
        "        show_sidechains = False\n",
        "        show_mainchains = False\n",
        "        show_pdb(pred_pdb_path, show_sidechains, show_mainchains, color).show()\n",
        "        if color == \"lDDT\":\n",
        "          plot_plddt_legend().show()\n",
        "        display(*system_widgets)\n",
        "\n",
        "\n",
        "      esmfold_pred_btn.on_click(\n",
        "          # disable_wrapper(\n",
        "          #     lambda btn: start_thread(esmfold_pred, (btn,))\n",
        "          #     )\n",
        "          lambda btn: start_thread(esmfold_pred, (btn,))\n",
        "          )\n",
        "\n",
        "\n",
        "    # Set click events\n",
        "    sampling_method_box.observe(change_sampling_method, names=\"value\")\n",
        "    pred_btn.on_click(\n",
        "        # disable_wrapper(\n",
        "            # lambda btn: start_thread(predict, (btn,))\n",
        "            # )\n",
        "            lambda btn: start_thread(predict, (btn,))\n",
        "        )\n",
        "\n",
        "    # Set default state\n",
        "    sampling_num.layout.display = \"none\"\n",
        "\n",
        "    # Update the display\n",
        "    clear_output()\n",
        "    display(*items)\n",
        "    display(seq_info)\n",
        "    show_pdb(save_path, color=\"chain\").show()\n",
        "    display(sampling_method_box, sampling_num, sampling_intro, pred_hint, pred_btn)\n",
        "    display(*system_widgets)\n",
        "\n",
        "  upload_ok_btn.on_click(parse_structure)\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "def obtain_protein_embedding():\n",
        "  global refresh_module\n",
        "  refresh_module = obtain_protein_embedding\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT = \"30px\"\n",
        "  hint = HTML(markdown.markdown(\"## Obtain protein-level embeddings\"))\n",
        "\n",
        "  data_type_hint = HTML(markdown.markdown(\"### Uploaded data type:\"))\n",
        "  data_type_box = ipywidgets.RadioButtons(\n",
        "      options=['protein sequence', 'protein structure'],\n",
        "      value=\"protein structure\",\n",
        "      disabled=False,\n",
        "      style={'description_width': 'initial'},\n",
        "      )\n",
        "  saprothub_data_type_hint = HTML(markdown.markdown(\n",
        "    \"<font color=red>Note: For models with input type '(Structure-aware) sequence' in SaprotHub, protein structure input is required.</font>\",\n",
        "    ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  model_hint = HTML(markdown.markdown(\"### Choose the model for embedding generation:\"))\n",
        "  model_type_box = ipywidgets.Dropdown(\n",
        "            options=['Official SaProt (35M)', \"Official SaProt (650M)\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"],\n",
        "            value='Official SaProt (35M)',\n",
        "            description='Base model:',\n",
        "            disabled=False,\n",
        "            layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "          )\n",
        "  model_arg_box = HTML(markdown.markdown(\"dummy box\"))\n",
        "  model_arg_box.layout.display = \"none\"\n",
        "\n",
        "  upload_model_items = get_upload_box(btn_desc=\"Upload Model-<task_name>-<model_size>.zip file\")\n",
        "  set_upload_visibility(upload_model_items, mode=\"none\")\n",
        "\n",
        "  saprothub_link = HTML(markdown.markdown(\n",
        "    \"<font color=red>You could search models using our <a href='https://huggingface.co/spaces/SaProtHub/SaprotHub-search' target='blank'>search engine</a> or from <a href='https://huggingface.co/SaProtHub' target='blank'>SaprotHub</a>\\n\\n\"\n",
        "    \"A model id example: <a href='https://huggingface.co/SaProtHub/Model-Binary_Localization-650M' target='blank'>SaProtHub/Model-Binary_Localization-650M</a></font>\"\n",
        "    ), layout=Layout(display=\"none\"))\n",
        "\n",
        "  upload_type_hint = HTML(markdown.markdown(\"### Choose the number of protein structure:\"))\n",
        "  upload_type_box = ipywidgets.RadioButtons(\n",
        "        options=['Single file', 'Multiple files'],\n",
        "        layout={'width': 'max-content'}, # If the items' names are long\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'},\n",
        "        )\n",
        "\n",
        "  upload_hint = HTML(markdown.markdown(\"**Upload the protein structure (.pdb / .cif file):**\"))\n",
        "  upload_items = get_upload_box()\n",
        "  upload_ok_btn = ipywidgets.Button(\n",
        "      description=\"Submit\",\n",
        "      layout=Layout(width='500px', height='30px'),\n",
        "      button_style=\"info\",\n",
        "      )\n",
        "\n",
        "  save_path_hint = HTML(layout={\"display\": \"none\"})\n",
        "  start_hint = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "  start_btn = Button(description='Start embedding generation', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "  # Set items to display\n",
        "  items = [\n",
        "      hint,\n",
        "      model_hint,\n",
        "      model_type_box,\n",
        "      model_arg_box,\n",
        "      saprothub_link,\n",
        "      data_type_hint,\n",
        "      data_type_box,\n",
        "      saprothub_data_type_hint,\n",
        "      upload_type_hint,\n",
        "      upload_type_box,\n",
        "      upload_hint,\n",
        "      *upload_items,\n",
        "      # upload_ok_btn,\n",
        "      save_path_hint,\n",
        "      start_hint,\n",
        "      start_btn\n",
        "  ]\n",
        "\n",
        "  # Used to replace input box based on model type\n",
        "  model_arg_box_idx = items.index(model_arg_box)\n",
        "\n",
        "  # Set click events\n",
        "  def change_model_type(change):\n",
        "    model_type_value = change[\"new\"]\n",
        "    if model_type_value == \"Shared by peers on SaprotHub\":\n",
        "      saprothub_data_type_hint.layout.display = None\n",
        "    else:\n",
        "      saprothub_data_type_hint.layout.display = \"none\"\n",
        "\n",
        "    if model_type_value == \"Trained by yourself on ColabSaprot\":\n",
        "      new_model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "      new_model_arg_box.layout.width = WIDTH\n",
        "      new_model_arg_box.description = \"Select your local model:\"\n",
        "      new_model_arg_box.style = {'description_width': 'initial'}\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "\n",
        "    elif model_type_value == \"Shared by peers on SaprotHub\":\n",
        "      new_model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "      new_model_arg_box.layout.width = WIDTH\n",
        "      new_model_arg_box.placeholder = \"Enter SaprotHub model id in HuggingFace\"\n",
        "      new_model_arg_box.description = \"Model id:\"\n",
        "      saprothub_link.layout.display = None\n",
        "\n",
        "    elif model_type_value == \"Saved in your local computer\":\n",
        "      set_upload_visibility(upload_model_items, \"default\")\n",
        "      new_model_arg_box = upload_model_items\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "\n",
        "    else:\n",
        "      new_model_arg_box = HTML(markdown.markdown(\"dummy box\"))\n",
        "      new_model_arg_box.layout.display = \"none\"\n",
        "      saprothub_link.layout.display = \"none\"\n",
        "\n",
        "    if isinstance(new_model_arg_box, list):\n",
        "      new_items = items[:model_arg_box_idx] + new_model_arg_box + items[model_arg_box_idx+1:]\n",
        "      custom_display(*new_items)\n",
        "\n",
        "    else:\n",
        "      items[model_arg_box_idx] = new_model_arg_box\n",
        "      custom_display(*items)\n",
        "\n",
        "  def change_data_type(change):\n",
        "    data_type_value = change[\"new\"]\n",
        "    if data_type_value == \"protein sequence\":\n",
        "      upload_type_hint.layout.display = \"none\"\n",
        "      upload_type_box.layout.display = \"none\"\n",
        "      upload_hint.value = markdown.markdown(\"**Upload the protein sequences (.fasta file):**\")\n",
        "\n",
        "    else:\n",
        "      upload_type_hint.layout.display = None\n",
        "      upload_type_box.layout.display = None\n",
        "      if upload_type_box.value == \"Single file\":\n",
        "        upload_hint.value = markdown.markdown(\"**Upload the protein structure (.pdb / .cif file):**\")\n",
        "      else:\n",
        "        upload_hint.value = markdown.markdown(\"**Upload the protein structure (a .zip file that contains all structure files):**\")\n",
        "\n",
        "  def change_upload_type(change):\n",
        "    now_type = change[\"new\"]\n",
        "    if now_type == \"Single file\":\n",
        "      upload_hint.value = markdown.markdown(\"**Upload the protein structure (.pdb / .cif file):**\")\n",
        "    else:\n",
        "      upload_hint.value = markdown.markdown(\"**Upload the protein structure (a .zip file that contains all structure files):**\")\n",
        "\n",
        "  # def on_upload_file(button):\n",
        "  #   save_path = get_upload_file_path(upload_items)\n",
        "  #   name = os.path.basename(save_path)\n",
        "\n",
        "  #   # Check the file format\n",
        "  #   if data_type_box.value == \"protein sequence\":\n",
        "  #     assert name.endswith(\".fasta\"), \"Please upload file with correct format (.fasta)!\"\n",
        "\n",
        "  #   elif upload_type_box.value == \"Single file\":\n",
        "  #     assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "  #   else:\n",
        "  #     assert name.endswith(\".zip\"), \"Please upload file with correct format (.zip)!\"\n",
        "\n",
        "  #   save_path_hint.value = markdown.markdown(f\"The uploaded file is saved to '{save_path}'\")\n",
        "  #   for item in [save_path_hint, start_hint, start_btn]:\n",
        "  #     item.layout.display = None\n",
        "\n",
        "  #   if name.endswith(\".fasta\"):\n",
        "  #     pdb_list = save_path\n",
        "\n",
        "  #   elif name.endswith(\".zip\"):\n",
        "  #     save_dir = save_path.rsplit(\".\", 1)[0]\n",
        "  #     with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "  #         zip_ref.extractall(save_dir)\n",
        "\n",
        "  #     # If the unzipped file is a directory\n",
        "  #     unzipped_files = os.listdir(save_dir)\n",
        "  #     if len(unzipped_files) == 1:\n",
        "  #       file_path = f\"{save_dir}/{unzipped_files[0]}\"\n",
        "  #       if os.path.isdir(file_path):\n",
        "  #         save_dir = file_path\n",
        "\n",
        "  #     pdb_list = [f\"{save_dir}/{file}\" for file in os.listdir(save_dir)]\n",
        "\n",
        "  #   else:\n",
        "  #     pdb_list = [str(save_path)]\n",
        "\n",
        "  #   # Enable click for embedding generation\n",
        "  #   start_btn.on_click(\n",
        "  #       lambda btn: start_thread(partial(start_generation, pdb_list=pdb_list), (btn,))\n",
        "  #       )\n",
        "\n",
        "  def start_generation(button):\n",
        "    save_path = get_upload_file_path(upload_items)\n",
        "    name = os.path.basename(save_path)\n",
        "\n",
        "    # Check the file format\n",
        "    if data_type_box.value == \"protein sequence\":\n",
        "      assert name.endswith(\".fasta\"), \"Please upload file with correct format (.fasta)!\"\n",
        "\n",
        "    elif upload_type_box.value == \"Single file\":\n",
        "      assert name.endswith(\".pdb\") or name.endswith(\".cif\"), \"Please upload file with correct format (.pdb / .cif)!\"\n",
        "\n",
        "    else:\n",
        "      assert name.endswith(\".zip\"), \"Please upload file with correct format (.zip)!\"\n",
        "\n",
        "    if name.endswith(\".fasta\"):\n",
        "      pdb_list = save_path\n",
        "\n",
        "    elif name.endswith(\".zip\"):\n",
        "      save_dir = save_path.rsplit(\".\", 1)[0]\n",
        "      with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(save_dir)\n",
        "\n",
        "      # If the unzipped file is a directory\n",
        "      unzipped_files = os.listdir(save_dir)\n",
        "      if len(unzipped_files) == 1:\n",
        "        file_path = f\"{save_dir}/{unzipped_files[0]}\"\n",
        "        if os.path.isdir(file_path):\n",
        "          save_dir = file_path\n",
        "\n",
        "      pdb_list = [f\"{save_dir}/{file}\" for file in os.listdir(save_dir)]\n",
        "\n",
        "    else:\n",
        "      pdb_list = [str(save_path)]\n",
        "\n",
        "    # Check data type compatibility\n",
        "    upload_data_type = \"AA\" if data_type_box.value == \"protein sequence\" else \"SA\"\n",
        "\n",
        "    if model_type_box.value == \"Saved in your local computer\":\n",
        "      zip_path = get_upload_file_path(upload_model_items)\n",
        "      name = os.path.basename(zip_path)\n",
        "      save_dir = ADAPTER_HOME / \"Upload\" / name.rsplit('.', 1)[0]\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "      # unzip model.zip\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(save_dir)\n",
        "      os.remove(zip_path)\n",
        "      model_arg = save_dir\n",
        "\n",
        "    else:\n",
        "      model_arg = items[model_arg_box_idx].value\n",
        "\n",
        "    model_data_type = load_data_type_from_model(model_type_box.value, model_arg)\n",
        "    if model_data_type != upload_data_type:\n",
        "      if model_data_type == \"SA\":\n",
        "        raise Exception(\"Error: The model you choose was trained on protein structures. So you have to upload protein structures!\")\n",
        "      else:\n",
        "        raise Exception(\"Error: The model you choose was trained on protein sequences. So you have to upload protein sequences!\")\n",
        "\n",
        "    print(\"Start generation...\")\n",
        "    zip_path = generate_embeddings(pdb_list, model_type_box.value, model_arg)\n",
        "\n",
        "    dividing_line = HTML(markdown.markdown(f\"### {'-'*75}\"))\n",
        "    zip_hint = HTML(markdown.markdown(\n",
        "      f\"The embedding files have been saved to ``{zip_path}``. You can click to download the file.\\n\\n\"\n",
        "      \"There are two files in the zip file, ``embedding_seqs.fasta`` and ``embeddings.pt``.\\n\\n\"\n",
        "      \"``embedding_seqs.fasta`` contains proteins used for generating embeddings.\\n\\n\"\n",
        "      \"``embeddings.pt`` is the generated embeddings with the shape ``[N, D]``, where ``N`` is the number of sequences and ``D`` is the hidden dimension.\\n\\n\"\n",
        "      \"The order of sequences in ``fasta`` file is consistent with the order of embeddings in ``pt`` file.\"\n",
        "      ))\n",
        "    download_btn = generate_download_btn(zip_path)\n",
        "\n",
        "    new_items = items + [dividing_line, zip_hint, download_btn]\n",
        "    custom_display(*new_items)\n",
        "\n",
        "\n",
        "  model_type_box.observe(change_model_type, names=\"value\")\n",
        "  data_type_box.observe(change_data_type, names=\"value\")\n",
        "  upload_type_box.observe(change_upload_type, names='value')\n",
        "  start_btn.on_click(lambda btn: start_thread(start_generation, (btn,)))\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "def share_model():\n",
        "  global refresh_module\n",
        "  refresh_module = share_model\n",
        "\n",
        "  from huggingface_hub import notebook_login\n",
        "  notebook_login()\n",
        "\n",
        "  WIDTH = \"500px\"\n",
        "  HEIGHT = \"30px\"\n",
        "\n",
        "  sep_hint = HTML(markdown.markdown(f\"### {'-'*75}\\n\\n## Please first login to HuggingFace (see above)\"))\n",
        "\n",
        "  upload_hint = HTML(markdown.markdown(f\"### Choose the model yout want to share\"))\n",
        "\n",
        "  share_model_type_box = ipywidgets.Dropdown(\n",
        "              options=[\"Trained by yourself on ColabSaprot\", \"Saved in your local computer\"],\n",
        "              value='Trained by yourself on ColabSaprot',\n",
        "              description='Base model:',\n",
        "              disabled=False,\n",
        "              layout=Layout(width=WIDTH, height=HEIGHT)\n",
        "            )\n",
        "  model_arg_box = select_adapter_from(None, use_model_from=share_model_type_box.value)\n",
        "  model_arg_box.layout.width = WIDTH\n",
        "  model_arg_box.description = \"Select your local model:\"\n",
        "  model_arg_box.style = {'description_width': 'initial'}\n",
        "\n",
        "  upload_model_items = get_upload_box(btn_desc=\"Upload Model-<task_name>-<model_size>.zip file\")\n",
        "  set_upload_visibility(upload_model_items, mode=\"none\")\n",
        "\n",
        "\n",
        "  desc_hint = HTML(markdown.markdown(\n",
        "      f\"### Describe your model\\n\\n\"\n",
        "      \"<font color=red>A more accurate model description enables easier retrieval by other researchers on SaprotHub.</font>\"\n",
        "      ))\n",
        "\n",
        "  task_id_box = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Input the name of your task, e.g. Subcellular_Localization.',\n",
        "                disabled=False,\n",
        "                description=\"Task name:\",\n",
        "                style={'description_width': 'initial'},\n",
        "                layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "                )\n",
        "\n",
        "  task_desc_box = ipywidgets.Textarea(\n",
        "                placeholder=f'Briefly introduce your task.\\n\\nExample:\\n\\nThe task is to predict the catalytic activity of laccase enzymes in lignin degradation based on their protein sequences and structures.',\n",
        "                disabled=False,\n",
        "                description=\"Task description:\",\n",
        "                layout=Layout(width=WIDTH, height=\"200px\"),\n",
        "                style={'description_width': 'initial'},\n",
        "                )\n",
        "  label_meaning_box = ipywidgets.Textarea(\n",
        "                placeholder=f'Briefly introduce the label range/categories predicted by your model.\\n\\nExamples:\\n\\n- Label \"1\" indicates soluble proteins, while \"0\" indicates membrane-bound proteins.\\n\\n- The labels represent fluorescence intensity values ranging from 0 to 1.',\n",
        "                disabled=False,\n",
        "                description=\"Label meaning:\",\n",
        "                layout=Layout(width=WIDTH, height=\"200px\"),\n",
        "                style={'description_width': 'initial'},\n",
        "                )\n",
        "\n",
        "  data_desc_box = ipywidgets.Textarea(\n",
        "                placeholder=f\"Briefly introduce your training data. How was it created or collected? How many training examples? Other details?\",\n",
        "                disabled=False,\n",
        "                description=\"Data description (optional):\",\n",
        "                layout=Layout(width=WIDTH, height=\"200px\"),\n",
        "                style={'description_width': 'initial'},\n",
        "                )\n",
        "\n",
        "  data_src_box = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Training data URL/link or referenced literature.',\n",
        "                disabled=False,\n",
        "                description=\"Data source (optional):\",\n",
        "                style={'description_width': 'initial'},\n",
        "                layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "                )\n",
        "\n",
        "  model_performance_box = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Prediction accuracy on your test set.',\n",
        "                disabled=False,\n",
        "                description=\"Model performance (optional):\",\n",
        "                style={'description_width': 'initial'},\n",
        "                layout=Layout(width=WIDTH, height=HEIGHT),\n",
        "                )\n",
        "\n",
        "  citation_box = ipywidgets.Text(\n",
        "                value=\"\",\n",
        "                placeholder=f'Add your preferred citation here for users to cite.',\n",
        "                disabled=False,\n",
        "                description=\"Citation (optional):\",\n",
        "                style={'description_width': 'initial'},\n",
        "                layout=Layout(width=WIDTH, height=\"HEIGHT\"),\n",
        "                )\n",
        "\n",
        "  upload_hint = HTML(markdown.markdown(\n",
        "    \"<font color=red>Note: Model input type and task type will be detected automatically.</font>\"\n",
        "    ))\n",
        "\n",
        "  upload_to_hf_btn = Button(description='Upload to HuggingFace', layout=Layout(width=WIDTH, height=HEIGHT), button_style=\"info\")\n",
        "\n",
        "\n",
        "  items = [\n",
        "      sep_hint,\n",
        "      upload_hint,\n",
        "      share_model_type_box,\n",
        "      model_arg_box,\n",
        "      desc_hint,\n",
        "      task_id_box,\n",
        "      task_desc_box,\n",
        "      label_meaning_box,\n",
        "      data_desc_box,\n",
        "      data_src_box,\n",
        "      model_performance_box,\n",
        "      citation_box,\n",
        "      upload_hint,\n",
        "      upload_to_hf_btn,\n",
        "  ]\n",
        "\n",
        "  # Used to replace input box based on model type\n",
        "  model_arg_box_idx = items.index(model_arg_box)\n",
        "\n",
        "  # Set click events\n",
        "  def change_model_type(change):\n",
        "    model_type_value = change[\"new\"]\n",
        "\n",
        "    if model_type_value == \"Trained by yourself on ColabSaprot\":\n",
        "      new_model_arg_box = select_adapter_from(None, use_model_from=model_type_value)\n",
        "      new_model_arg_box.layout.width = WIDTH\n",
        "      new_model_arg_box.description = \"Select your local model:\"\n",
        "      new_model_arg_box.style = {'description_width': 'initial'}\n",
        "\n",
        "    elif model_type_value == \"Saved in your local computer\":\n",
        "      set_upload_visibility(upload_model_items, \"default\")\n",
        "      new_model_arg_box = upload_model_items\n",
        "\n",
        "    else:\n",
        "      raise KeyError(model_type_value)\n",
        "\n",
        "    clear_output()\n",
        "    notebook_login()\n",
        "    if isinstance(new_model_arg_box, list):\n",
        "      new_items = items[:model_arg_box_idx] + new_model_arg_box + items[model_arg_box_idx+1:]\n",
        "      display(*new_items)\n",
        "\n",
        "    else:\n",
        "      items[model_arg_box_idx] = new_model_arg_box\n",
        "      display(*items)\n",
        "\n",
        "    display(*system_widgets)\n",
        "\n",
        "  def upload_to_huggingface(button):\n",
        "    from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "    api = HfApi()\n",
        "    user = api.whoami()\n",
        "\n",
        "    name = task_id_box.value.replace(\" \", \"_\")\n",
        "    assert name != \"\", \"Please input your task name!\"\n",
        "\n",
        "    task_desc = task_desc_box.value\n",
        "    assert task_desc != \"\", \"Please introduce your task!\"\n",
        "\n",
        "    label_meaning = label_meaning_box.value\n",
        "    assert label_meaning != \"\", \"Please introduce the meaning of the label predicted by your model!\"\n",
        "\n",
        "    model_performance = model_performance_box.value\n",
        "    data_desc = data_desc_box.value\n",
        "    data_src = data_src_box.value\n",
        "    citation = citation_box.value\n",
        "\n",
        "    repo_name = user['name'] + '/' + name\n",
        "    local_dir = Path(f\"{root_dir}/SaprotHub/model_to_push\") / repo_name\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "    if repo_name not in repo_list:\n",
        "      api.create_repo(repo_name, private=False)\n",
        "\n",
        "    repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "    model_arg = items[model_arg_box_idx].value\n",
        "    if share_model_type_box.value == \"Saved in your local computer\":\n",
        "      zip_path = get_upload_file_path(upload_model_items)\n",
        "      zip_name = os.path.basename(zip_path)\n",
        "      save_dir = ADAPTER_HOME / \"Upload\" / zip_name.rsplit('.', 1)[0]\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "      # unzip model.zip\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(save_dir)\n",
        "      model_arg = f\"Upload/{zip_name.rsplit('.', 1)[0]}\"\n",
        "\n",
        "    command = f\"cp '{ADAPTER_HOME}/{model_arg}'/* '{local_dir}/'\"\n",
        "    subprocess.run(command, shell=True)\n",
        "\n",
        "    ######################################################################\n",
        "    #             Modify README              #\n",
        "    ######################################################################\n",
        "    import json\n",
        "\n",
        "    md_path = local_dir / \"README.md\"\n",
        "\n",
        "    # Obtain base model\n",
        "    adapter_config_path = local_dir / \"adapter_config.json\"\n",
        "    with open(adapter_config_path, \"r\") as r:\n",
        "      config_dict = json.load(r)\n",
        "      base_model = config_dict[\"base_model_name_or_path\"]\n",
        "\n",
        "    model_desc = \"## Base model\\n\\n\" \\\n",
        "            f\"[{base_model}](https://huggingface.co/{base_model})\\n\\n\" \\\n",
        "            \"## Task description\\n\\n\" \\\n",
        "            f\"{task_desc}\\n\\n\" \\\n",
        "            \"## Label meaning\\n\\n\" \\\n",
        "            f\"{label_meaning}\\n\\n\"\n",
        "\n",
        "    if data_desc != \"\":\n",
        "      model_desc += \"## Data description\\n\\n\" \\\n",
        "              f\"{data_desc}\\n\\n\" \\\n",
        "\n",
        "    if data_src != \"\":\n",
        "      model_desc += \"## Data source\\n\\n\" \\\n",
        "              f\"{data_src}\\n\\n\" \\\n",
        "\n",
        "    if model_performance != \"\":\n",
        "      model_desc += \"## Model performance (on test set)\\n\\n\" \\\n",
        "              f\"{model_performance}\\n\\n\" \\\n",
        "\n",
        "    if citation != \"\":\n",
        "      model_desc += \"## Citation\\n\\n\" \\\n",
        "              \"Please consider citing the following work if you find the model useful:\\n\\n\" \\\n",
        "              f\"{citation}\\n\\n\" \\\n",
        "\n",
        "    replace_data = {\n",
        "        '<slot name=\\'description\\'>': model_desc,\n",
        "    }\n",
        "\n",
        "    with open(md_path, \"r\") as file:\n",
        "        content = file.read()\n",
        "\n",
        "    for key, value in replace_data.items():\n",
        "        if value != \"\":\n",
        "            content = content.replace(key, value)\n",
        "\n",
        "    with open(md_path, \"w\") as file:\n",
        "        file.write(content)\n",
        "\n",
        "    ######################################################################\n",
        "    #          Push to HuggingFace             #\n",
        "    ######################################################################\n",
        "    repo.push_to_hub(commit_message=\"Upload adapter model\")\n",
        "    url = f\"https://huggingface.co/{user['name']}/{name}\"\n",
        "    finish_hint = HTML(markdown.markdown(\n",
        "        f\"**You model has been successfully uploaded to <a href='{url}' target='blank'>{url}</a>!**\\n\\n\"\n",
        "        \"**You can further move your model from personal repository to official SaprotHub, see <a href='https://github.com/westlake-repl/SaprotHub/wiki/SaprotHub-v2-(latest)#share-your-model-to-official-saprothub' target='blank'>here</a>!**\\n\\n\"\n",
        "        ))\n",
        "    display(finish_hint)\n",
        "\n",
        "  share_model_type_box.observe(change_model_type, names=\"value\")\n",
        "  upload_to_hf_btn.on_click(upload_to_huggingface)\n",
        "  display(*items)\n",
        "\n",
        "\n",
        "# Stop the running task\n",
        "def stop_thread(button):\n",
        "  global now_thread\n",
        "  if now_thread:\n",
        "    print(\"\\033[31mInterupt the running task...\\033[0m\")\n",
        "    while now_thread.is_alive():\n",
        "      import inspect\n",
        "      import ctypes\n",
        "\n",
        "      # Stop the thread\n",
        "      exctype = SystemExit\n",
        "      tid = ctypes.c_long(now_thread.ident)\n",
        "      if not inspect.isclass(exctype):\n",
        "          exctype = type(exctype)\n",
        "      res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
        "      if res == 0:\n",
        "          # raise ValueError(\"invalid thread id\")\n",
        "          continue\n",
        "      elif res != 1:\n",
        "          # \"\"\"if it returns a number greater than one, you're in trouble,\n",
        "          # and you should call it again with exc=NULL to revert the effect\"\"\"\n",
        "          ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
        "          # raise SystemError(\"PyThreadState_SetAsyncExc failed\")\n",
        "          continue\n",
        "\n",
        "    print(\"\\033[31m\\nTask interrupted by user!\\033[0m\")\n",
        "    now_thread = None\n",
        "\n",
        "  else:\n",
        "    print(\"\\033[31m\\nNo running task to be stopped!\\033[0m\")\n",
        "\n",
        "\n",
        "# Start a new thread when executing a task\n",
        "def start_thread(func, args=tuple()):\n",
        "  # def test(button):\n",
        "  #   save_path = \"/sujin/temp/test.tsv\"\n",
        "  #   with open(save_path, \"w\") as w:\n",
        "  #     for i in range(1000):\n",
        "  #       print(i)\n",
        "  #       w.write(f\"{i}\\n\")\n",
        "  #       w.flush()\n",
        "  #       time.sleep(0.1)\n",
        "\n",
        "  # disable_wrapper(func)(*args)\n",
        "  thread = threading.Thread(target=disable_wrapper(func), args=args)\n",
        "  thread.start()\n",
        "\n",
        "  global now_thread\n",
        "  now_thread = thread\n",
        "\n",
        "\n",
        "# Jump to next page\n",
        "def jump(button, next=None):\n",
        "  # Stop the running task\n",
        "  stop_thread(None)\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  if next is None:\n",
        "    # Refresh current module\n",
        "    refresh_module()\n",
        "\n",
        "  else:\n",
        "    # Jump to next module\n",
        "    next()\n",
        "\n",
        "  display(*system_widgets)\n",
        "\n",
        "\n",
        "def custom_display(*args, **kwargs):\n",
        "  clear_output()\n",
        "  display(*args, **kwargs)\n",
        "  display(*system_widgets)\n",
        "\n",
        "\n",
        "# Current running thread\n",
        "now_thread = None\n",
        "\n",
        "HINT = HTML(markdown.markdown(\"**<font color=red>Note: At any time you can click below buttons to stop and restart</font>**\"))\n",
        "stop_btn = Button(description=\"Stop\", button_style=\"danger\")\n",
        "back_btn = Button(description=\"Go back\", button_style=\"success\")\n",
        "refresh_btn = Button(description=\"Refresh\", button_style=\"success\")\n",
        "system_hint = HTML(markdown.markdown(\n",
        "    \"**Go back:** Stop the running task and go back to the first interface\\n\\n\"\n",
        "    \"**Refresh**: Stop the running task and refresh the current interface\\n\\n\"\n",
        "    \"**Stop**: Stop the running task\"\n",
        "    ))\n",
        "\n",
        "refresh_module = None\n",
        "back_btn.on_click(partial(jump, next=train_or_pred))\n",
        "refresh_btn.on_click(jump)\n",
        "stop_btn.on_click(stop_thread)\n",
        "\n",
        "train_or_pred()\n",
        "system_widgets = [HINT, ipywidgets.HBox([back_btn, refresh_btn, stop_btn]), system_hint]\n",
        "display(*system_widgets)\n",
        "\n",
        "with ui_events() as poll:\n",
        "  try:\n",
        "    # start_thread(None, (None,))\n",
        "    while True:\n",
        "      poll(10)\n",
        "      # print(now_thread)\n",
        "      time.sleep(0.1)\n",
        "\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "  finally:\n",
        "    stop_thread(None)\n",
        "    clear_output()\n",
        "    end_hint = HTML(markdown.markdown(\"## The program is interrupted. You could click the run-button to restart.\"))\n",
        "    display(end_hint)"
      ],
      "metadata": {
        "id": "LHKi8xI-aU4E",
        "cellView": "form"
      },
      "id": "LHKi8xI-aU4E",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}