{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdjG4-XsE0I"
      },
      "source": [
        "# **SaprotHub: Making Protein Modeling Accessible to All Biologists**\n",
        "\n",
        "This is the Colab version of [SaProt](https://github.com/westlake-repl/SaProt), a pre-trained protein language model designed for various downstream protein tasks.\n",
        "\n",
        "**ColabSaprot** is a platform where **Protein Language Models(PLMs)** are more accessible and user-friendly for biologists, enabling effortless model training and sharing within the scientific community.\n",
        "\n",
        "We've established the [SaprotHub](https://huggingface.co/SaProtHub) for storing and sharing models and datasets, where you can explore extensive collections for specific protein prediction tasks.\n",
        "\n",
        "We hope ColabSaprot and SaprotHub can contribute to advancing biological research, fostering collaboration, and accelerating discoveries in the field. You can access [our paper](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v2) for further details.\n",
        "\n",
        "For detailed steps of each section, please refer to the <a href=\"#manual\">manual</a>.\n",
        "\n",
        "Check this [video](https://www.youtube.com/watch?v=r42z1hvYKfw) to see how to train your model using ColabSaprot.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQ6vaQTjYO3"
      },
      "source": [
        "## SaprotHub\n",
        "\n",
        "Find awesome models and datasets for specific protein task on [SaprotHub](https://huggingface.co/SaprotHub)!\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SaProtHub.png?raw=true\" height=\"500\" width=\"800px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLb_im9sJWw"
      },
      "source": [
        "## ColabSaprot Content\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/content.png\n",
        "?raw=true\" height=\"400\" width=\"600px\" align=\"center\">\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnTxW38rBI7d"
      },
      "source": [
        "# 0: Instruction\n",
        "\n",
        "Before you begin training and utilizing your model, here are some important details about **Task**, **Dataset** and **Basic Colab knowledge** that you need to be aware of.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bygnx_yiBI7d"
      },
      "source": [
        "## 0.1: Task\n",
        "\n",
        "Different models are designed for different tasks, so it's essential to understand **which type your task belongs to**.\n",
        "\n",
        "You can recognize your task type based on your task description and objectives.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!-- ### Task Type\n",
        "\n",
        "- **Classification Task**: classify protein sequences.\n",
        "- **Regression Task**: predict the value of some property of a protein sequence.\n",
        "- **Amino Acid Classification Task**: classify the amino acids in a protein sequence.  -->\n",
        "\n",
        "\n",
        "| Task Description                                                                        | Task Type                          | Use SaProt to predict                         |\n",
        "| --------------------------------------------------------------------------------------- | ---------------------------------- | --------------------------------------------- |\n",
        "| Classify protein sequences.                                                             | **Classification Task**            | <a href=\"#classification_regression\">here</a> |\n",
        "| Predict the value of some property of a protein sequence.                               | **Regression Task**                | <a href=\"#classification_regression\">here</a> |\n",
        "| Classify the amino acids in a protein sequence.                                         | **Amino Acid Classification Task** | <a href=\"#classification_regression\">here</a> |\n",
        "| Predict the mutational effect based on the wild type sequence and mutation information. | **Mutational Effect Prediction**   | <a href=\"#mutational_effect\">here</a>         |\n",
        "| Predict the residue sequence given the structure backbone.                              | **Inverse Folding Prediction**     | <a href=\"#inverse_folding\">here</a>           |\n",
        "| Predict if there is interaction between the two proteins.                               | **Pair Classification Task**       | <a href=\"#classification_regression\">here</a> |\n",
        "| Predict the ability of interaction between the two proteins.                            | **Pair Regression Task**           | <a href=\"#classification_regression\">here</a> |\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are some example tasks and their task type:\n",
        "\n",
        "| Task Type | Example |\n",
        "| --- | --- |\n",
        "| **Classification Task** | **Subcellular Location Prediction**: predict which location category the protein belong to. |\n",
        "| **Classification Task** | **Metal Ion Binding Detection**: predict whether there are metal ion–binding sites in the protein. |\n",
        "| **Regression Task** | **Thermostability Prediction**: predict the thermostability value of a protein. |\n",
        "| **Amino Acid Classification Task** | **Binding Site Detection**: predict whether the amino acid is a binding site or not. |\n",
        "\n",
        "<!-- <br>\n",
        "\n",
        "### Use your models or shared models on SaProtHub\n",
        "\n",
        "You can use\n",
        "\n",
        "- your trained model\n",
        "- or shared models on SaProtHub\n",
        "- pre-trained protein language model\n",
        "\n",
        "to make some prediction -->\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "To view the full list of tasks supported by ColabSaprot, please refer to [task_list.md](https://github.com/westlake-repl/SaProtHub/blob/main/task_list.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acS6TsJCBI7d"
      },
      "source": [
        "## 0.2: Dataset <a name=\"data_format\"></a>\n",
        "\n",
        "You can use your private data to train and predict. Below are the various data formats corresponding to different **data types**.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### SA(Structure-aware) Sequence\n",
        "\n",
        "We combine the residue and structure tokens at each residue site to create a **Structure-aware sequence** (SA sequence), merging both residue and structural information.\n",
        "\n",
        "The structure tokens are generated by encoding the 3D structure of proteins using Foldseek.\n",
        "\n",
        "<a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/SA_Sequence.png?raw=true\" height=\"300\" width=\"600px\" align=\"center\">\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Data Type\n",
        "\n",
        "1. Single AA Sequence\n",
        "2. Single SA Sequence\n",
        "3. Single UniProt ID\n",
        "4. Single PDB/CIF Structure\n",
        "5. Multiple AA Sequences\n",
        "6. Multiple SA Sequences\n",
        "7. Multiple UniProt IDs\n",
        "8. Multiple PDB/CIF Structures\n",
        "9. SaprotHub Dataset\n",
        "\n",
        "For tasks that require **two protein sequences as input** (pair classification & pair regression) :\n",
        "\n",
        "10. A pair of AA Sequences\n",
        "11. A pair of SA Sequences\n",
        "12. A pair of UniProt IDs\n",
        "13. A pair of PDB/CIF Structures\n",
        "14. Multiple pairs of AA Sequences\n",
        "15. Multiple pairs of SA Sequences\n",
        "16. Multiple pairs of UniProt IDs\n",
        "17. Multiple pairs of PDB/CIF Structures\n",
        "\n",
        "<br>\n",
        "\n",
        "### Data Format <a name='data_format'></a>\n",
        "\n",
        "#### For `Single AA Sequence`, `Single SA Sequence`, and `Single UniProt ID` (first three data types)\n",
        "An input box will appear after running the cell. Please enter the protein sequence in the required format.\n",
        "\n",
        "<br>\n",
        "\n",
        "####  For `Single PDB/CIF Structure` (fourth data type)\n",
        "A file upload button will appear after running the cell. Please upload a .pdb or .cif file.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### For `Multiple AA Sequences`, `Multiple SA Sequences`, `Multiple UniProt IDs` (fifth to seventh data types)\n",
        "A file upload button will appear after running the cell. Please upload a .csv file and ensure that the column name in the .csv file is `Sequence`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_Sequences_data_format.png?raw=true\" height=\"200\" width=\"800px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#### For `Multiple PDB/CIF Structures`\n",
        "A file upload button will appear after running the cell. Please upload a .csv file containing three columns: `Sqeuence`, `type` and `chain`;\n",
        "\n",
        "- `type`: Indicate whether the structure file is a real PDB structure or an AlphaFold 2 predicted structure. For AF2 (AlphaFold 2) structures, we will apply pLDDT masking. The value must be either \"PDB\" or \"AF2\".\n",
        "- `chain`: For real PDB structures, since multiple chains may exist in one .pdb file, it is necessary to specify which chain is used. For AF2 structures, the chain is assumed to be A by default.\n",
        "\n",
        "After successfully uploading the .csv file, a second file upload button will appear. Please upload a zip file containing all corresponding pdb/cif files.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_PDB_CIF_Structures_data_format.png?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#### For `SaprotHub Dataset`\n",
        "An input box will appear after running the cell. Please enter the the ID of the SaprotHub Dataset. Find some datasets in [Official SaProtHub Repository](https://huggingface.co/SaProtHub).\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Huggingface_ID.png?raw=true\" height=\"200\" width=\"700px\" align=\"center\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6TiuSc4avN"
      },
      "source": [
        "## 0.3: Basic Colab Knowledge\n",
        "\n",
        "<br>\n",
        "\n",
        "### Cell Running status\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Cell_status.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dw1U1uBI7d"
      },
      "source": [
        "# **1: Installation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RQmyJm-3Qoii"
      },
      "outputs": [],
      "source": [
        "#@title 1.1: ⚠️ Switch your Runtime type to <font color=red>**GPU!!!**</font>\n",
        "\n",
        "#@markdown You can check the current runtime type in <font color=red>**the upper right corner of the page**</font>. If the current runtime type is CPU, you need to <font color=red>**switch it to GPU (either the free T4 or the paid A100)**</font> for a better training experience.\n",
        "\n",
        "#@markdown <img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Runtime.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "#@markdown #### Please follow the steps below to switch the runtime to GPU:\n",
        "\n",
        "#@markdown 1. Click the dropdown button\n",
        "#@markdown 2. Select option \"Change runtime type\"\n",
        "#@markdown 3. Select a GPU\n",
        "#@markdown 4. Click \"Save\" button\n",
        "#@markdown 5. <font color=red>Each time you switch the runtime, all code blocks need to be **re-executed**.</font>\n",
        "\n",
        "\n",
        "#@markdown <img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Switch_Runtime.png?raw=true\" height=\"400\" width=\"800px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgvb8ibwBI7d",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 1.2: ▶️ Click the run button to install SaProt\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "# Check whether the server is local or from google cloud\n",
        "root_dir = os.getcwd()\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "try:\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "  import saprot\n",
        "  print(\"SaProt is installed successfully!\")\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "\n",
        "except ImportError:\n",
        "  print(\"Installing SaProt...\")\n",
        "  os.system(f\"rm -rf {root_dir}/SaprotHub\")\n",
        "  # !rm -rf /content/SaprotHub/\n",
        "\n",
        "  !git clone https://github.com/westlake-repl/SaprotHub.git\n",
        "\n",
        "  # !pip install /content/SaprotHub/saprot-0.4.7-py3-none-any.whl\n",
        "  os.system(f\"pip install -r {root_dir}/SaprotHub/requirements.txt\")\n",
        "  # !pip install -r /content/SaprotHub/requirements.txt\n",
        "\n",
        "  os.system(f\"pip install {root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/LMDB\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/bin\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/output\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/datasets\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/token_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/structures\")\n",
        "  # !mkdir -p /content/SaprotHub/LMDB\n",
        "  # !mkdir -p /content/SaprotHub/bin\n",
        "  # !mkdir -p /content/SaprotHub/output\n",
        "  # !mkdir -p /content/SaprotHub/datasets\n",
        "  # !mkdir -p /content/SaprotHub/adapters/classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/token_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/structures\n",
        "\n",
        "  # !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  # os.system(\n",
        "  #   f\"wget 'https://drive.usercontent.google.com/download?id=1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re&export=download&authuser=0' -O {root_dir}/SaprotHub/bin/foldseek\"\n",
        "  # )\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "  # !chmod +x /content/SaprotHub/bin/foldseek\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "\n",
        "  # !mv /content/SaprotHub/ColabSaprotSetup/foldseek /content/SaprotHub/bin/\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import lmdb\n",
        "import base64\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import yaml\n",
        "import argparse\n",
        "import pprint\n",
        "import subprocess\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from loguru import logger\n",
        "from easydict import EasyDict\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from huggingface_hub import snapshot_download\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from transformers import AutoTokenizer, EsmForProteinFolding\n",
        "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "\n",
        "DATASET_HOME = Path(f'{root_dir}/SaprotHub/datasets')\n",
        "ADAPTER_HOME = Path(f'{root_dir}/SaprotHub/adapters')\n",
        "STRUCTURE_HOME = Path(f\"{root_dir}/SaprotHub/structures\")\n",
        "LMDB_HOME = Path(f'{root_dir}/SaprotHub/LMDB')\n",
        "OUTPUT_HOME = Path(f'{root_dir}/SaprotHub/output')\n",
        "UPLOAD_FILE_HOME = Path(f'{root_dir}/SaprotHub/upload_files')\n",
        "FOLDSEEK_PATH = Path(f\"{root_dir}/SaprotHub/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"SaprotHub Dataset\",\n",
        "                  \"A pair of AA Sequences\",\n",
        "                  \"A pair of SA Sequences\",\n",
        "                  \"A pair of UniProt IDs\",\n",
        "                  \"A pair of PDB/CIF Structures\",\n",
        "                  \"Multiple pairs of AA Sequences\",\n",
        "                  \"Multiple pairs of SA Sequences\",\n",
        "                  \"Multiple pairs of UniProt IDs\",\n",
        "                  \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Classify protein sequences (classification)\" : \"classification\",\n",
        "  \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\" : \"token_classification\",\n",
        "  \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" : \"regression\",\n",
        "  \"Predict protein-protein interaction (pair classification)\":\"pair_classification\",\n",
        "  \"Predict protein-protein interaction (pair regression)\":\"pair_regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_model\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_dataset\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_dataset\",\n",
        "}\n",
        "training_data_type_dict = {\n",
        "  \"Single AA Sequence\": \"AA\",\n",
        "  \"Single SA Sequence\": \"SA\",\n",
        "  \"Single UniProt ID\": \"SA\",\n",
        "  \"Single PDB/CIF Structure\": \"SA\",\n",
        "  \"Multiple AA Sequences\": \"AA\",\n",
        "  \"Multiple SA Sequences\": \"SA\",\n",
        "  \"Multiple UniProt IDs\": \"SA\",\n",
        "  \"Multiple PDB/CIF Structures\": \"SA\",\n",
        "  \"SaprotHub Dataset\": \"SA\",\n",
        "  \"A pair of AA Sequences\": \"AA\",\n",
        "  \"A pair of SA Sequences\": \"SA\",\n",
        "  \"A pair of UniProt IDs\": \"SA\",\n",
        "  \"A pair of PDB/CIF Structures\": \"SA\",\n",
        "  \"Multiple pairs of AA Sequences\": \"AA\",\n",
        "  \"Multiple pairs of SA Sequences\": \"SA\",\n",
        "  \"Multiple pairs of UniProt IDs\": \"SA\",\n",
        "  \"Multiple pairs of PDB/CIF Structures\": \"SA\",\n",
        "}\n",
        "\n",
        "\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### adapters #######################################\n",
        "################################################################################\n",
        "def get_adapters_list(task_type=None):\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    if task_type:\n",
        "      for file_path in (ADAPTER_HOME / task_type).glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "    else:\n",
        "      for file_path in ADAPTER_HOME.glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "\n",
        "def show_adapters_info(adapters_list):\n",
        "  grid = widgets.Grid(len(adapters_list)+1, 2, header_row=True, header_column=True)\n",
        "\n",
        "  with grid.output_to(0, 0):\n",
        "    print(\"ID\")\n",
        "\n",
        "  with grid.output_to(0, 1):\n",
        "    print(\"Local Model\")\n",
        "\n",
        "  # with grid.output_to(0, 2):\n",
        "  #   print(\"Adapter Path\")\n",
        "\n",
        "  for i in range(len(adapters_list)):\n",
        "    with grid.output_to(i+1, 0):\n",
        "      print(i)\n",
        "    with grid.output_to(i+1, 1):\n",
        "      print(adapters_list[i].stem)\n",
        "    # with grid.output_to(i+1, 2):\n",
        "    #   print(adapters_list[i])\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "  display(dropdown)\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter SaprotHub Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def select_adapter():\n",
        "  adapters_list = get_adapters_list()\n",
        "  print(Fore.BLUE+\"Existing Models:\"+Style.RESET_ALL)\n",
        "  # print(\"=\"*100)\n",
        "  # show_adapters_info(adapters_list)\n",
        "  # print(\"=\"*100)\n",
        "  return adapters_combobox(adapters_list)\n",
        "\n",
        "def adapters_selectmultiple(adapters_list):\n",
        "  selectmulitiple = ipywidgets.SelectMultiple(\n",
        "  options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "  value=[],\n",
        "  #rows=10,\n",
        "  placeholder='Select multiple models',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(selectmulitiple)\n",
        "\n",
        "  return selectmulitiple\n",
        "\n",
        "def adapters_textmultiple(adapters_list):\n",
        "  textmultiple = ipywidgets.Text(\n",
        "  value=None,\n",
        "  placeholder='Enter multiple SaprotHub Model IDs, separated by commas.',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(textmultiple)\n",
        "\n",
        "  return textmultiple\n",
        "\n",
        "# def select_adapter_from(use_model_from):\n",
        "#   adapters_list = get_adapters_list()\n",
        "\n",
        "#   if use_model_from == 'Trained by yourself on ColabSaprot':\n",
        "#     print(Fore.BLUE+\"Local Model:\"+Style.RESET_ALL)\n",
        "#     return adapters_dropdown(adapters_list)\n",
        "#   elif use_model_from == 'Shared by peers on SaprotHub':\n",
        "#     print(Fore.BLUE+\"SaprotHub Model:\"+Style.RESET_ALL)\n",
        "#     return adapters_text(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "def select_adapter_from(task_type, use_model_from):\n",
        "  adapters_list = get_adapters_list(task_type)\n",
        "\n",
        "  if use_model_from == 'Trained by yourself on ColabSaprot':\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_dropdown(adapters_list)\n",
        "\n",
        "  elif use_model_from == 'Shared by peers on SaprotHub':\n",
        "    print(Fore.BLUE+\"SaprotHub Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Saved in your local computer\":\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "\n",
        "    return EasyDict({\"value\":  f\"Local/{adapter_zip_path.stem}\"})\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ColabSaprot\":\n",
        "    # 1. select the list of adapters\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Multiple values can be selected with \\\"shift\\\" and/or \\\"ctrl\\\" (or \\\"command\\\") pressed and mouse clicks or arrow keys.\"+Style.RESET_ALL)\n",
        "    return adapters_selectmultiple(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "    # 1. enter the list of adapters\n",
        "    print(Fore.BLUE+f\"SaprotHub Model IDs, separated by commas ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_textmultiple(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  import shutil\n",
        "  import os\n",
        "  from pathlib import Path\n",
        "  import sys\n",
        "\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "  print(Fore.BLUE+\"Dataset: \"+Style.RESET_ALL, end='')\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == data_type_list[3]:\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type)\n",
        "\n",
        "    input_chain = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain here',\n",
        "      disabled=False)\n",
        "    input_chain.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "    display(input_chain)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    return pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "  # 4-7 & 13-16. Multiple Sequences\n",
        "  elif data_type in data_type_list[4:8] or data_type in data_type_list[13:17]:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type == data_type_list[7] or data_type == data_type_list[16]:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == data_type_list[8]:\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Copy and paste the SaprotHub Dataset ID here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "  # 9-11. A pair of seq\n",
        "  elif data_type in [\"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\"]:\n",
        "    print()\n",
        "\n",
        "    seq_type = data_type[len(\"A pair of \"):-1]\n",
        "\n",
        "    input_seq1 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 1 here',\n",
        "      disabled=False)\n",
        "    input_seq1.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 1:\"+Style.RESET_ALL)\n",
        "    display(input_seq1)\n",
        "\n",
        "    input_seq2 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 2 here',\n",
        "      disabled=False)\n",
        "    input_seq2.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 2:\"+Style.RESET_ALL)\n",
        "    display(input_seq2)\n",
        "\n",
        "    return (input_seq1, input_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  elif data_type == data_type_list[12]:\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type1 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The first structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type1)\n",
        "\n",
        "    input_chain1 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the first structure here',\n",
        "      disabled=False)\n",
        "    input_chain1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the first structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain1)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path1 = upload_file(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    dropdown_type2 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The second structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type2)\n",
        "\n",
        "    input_chain2 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the second structure here',\n",
        "      disabled=False)\n",
        "    input_chain2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the second structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain2)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path2 = upload_file(STRUCTURE_HOME)\n",
        "    return (pdb_file_path1.stem, dropdown_type1, input_chain1, pdb_file_path2.stem, dropdown_type2, input_chain2)\n",
        "\n",
        "\n",
        "  # elif data_type == \"Multiple pairs of PDB/CIF Structures\":\n",
        "  #   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "  #   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "  #   print(\"=\"*100)\n",
        "\n",
        "  #   if data_type == data_type_list[7]:\n",
        "  #     # upload and unzip PDB files\n",
        "  #     print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "  #     pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #     if pdb_zip_path.suffix != \".zip\":\n",
        "  #       logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "  #       raise RuntimeError(\"The data type does not match.\")\n",
        "  #     print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "  #     print(\"=\"*100)\n",
        "\n",
        "  #     import zipfile\n",
        "  #     with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "  #       zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "  #   return uploaded_csv_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # 0. Single AA Sequence\n",
        "  if data_type == data_type_list[0]:\n",
        "    input_seq = raw_data\n",
        "    aa_seq = input_seq.value\n",
        "\n",
        "    sa_seq = ''\n",
        "    for aa in aa_seq:\n",
        "        sa_seq += aa + '#'\n",
        "    return sa_seq\n",
        "\n",
        "  # 1. Single SA Sequence\n",
        "  if data_type == data_type_list[1]:\n",
        "    input_seq = raw_data\n",
        "    sa_seq = input_seq.value\n",
        "\n",
        "    return sa_seq\n",
        "\n",
        "  # 2. Single UniProt ID\n",
        "  if data_type == data_type_list[2]:\n",
        "    input_seq = raw_data\n",
        "    uniprot_id = input_seq.value\n",
        "\n",
        "\n",
        "    protein_list = [(uniprot_id, \"AF2\", \"A\")]\n",
        "    uniprot2pdb([protein_list[0][0]])\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  if data_type == data_type_list[3]:\n",
        "    uniprot_id = raw_data[0]\n",
        "    struc_type = raw_data[1].value\n",
        "    chain = raw_data[2].value\n",
        "\n",
        "    protein_list = [(uniprot_id, struc_type, chain)]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "    return sa_seq\n",
        "\n",
        "  # Multiple sequences\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "  if data_type in data_type_list[4:8] or data_type in data_type_list[13:17]:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "\n",
        "  # 4. Multiple AA Sequences\n",
        "  if data_type == data_type_list[4]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    for index, value in protein_df['Sequence'].items():\n",
        "      sa_seq = ''\n",
        "      for aa in value:\n",
        "        sa_seq += aa + '#'\n",
        "      protein_df.at[index, 'Sequence'] = sa_seq\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 5. Multiple SA Sequences\n",
        "  if data_type == data_type_list[5]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 6. Multiple UniProt IDs\n",
        "  if data_type == data_type_list[6]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_list = protein_df.iloc[:, 0].tolist()\n",
        "    uniprot2pdb(protein_list)\n",
        "    protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 7. Multiple PDB/CIF Structures\n",
        "  if data_type == data_type_list[7]:\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    # protein_list = [(uniprot_id, type, chain), ...]\n",
        "    # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "    # uniprot2pdb(protein_list)\n",
        "    protein_list = []\n",
        "    for row_tuple in protein_df.itertuples(index=False):\n",
        "      assert row_tuple.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "      protein_list.append(row_tuple)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == data_type_list[8]:\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=LMDB_HOME/REPO_ID)\n",
        "\n",
        "    return LMDB_HOME/REPO_ID\n",
        "\n",
        "  # 9. Pair Single AA Sequences\n",
        "  elif data_type == \"A pair of AA Sequences\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[0], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[0], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 10. Pair Single SA Sequences\n",
        "  elif data_type ==  \"A pair of SA Sequences\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[1], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[1], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 11. Pair Single UniProt IDs\n",
        "  elif data_type ==  \"A pair of UniProt IDs\":\n",
        "    input_seq_1, input_seq_2 = raw_data\n",
        "    sa_seq1 = get_SA_sequence_by_data_type(data_type_list[2], input_seq_1)\n",
        "    sa_seq2 = get_SA_sequence_by_data_type(data_type_list[2], input_seq_2)\n",
        "\n",
        "    return (sa_seq1, sa_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  if data_type == \"A pair of PDB/CIF Structures\":\n",
        "    uniprot_id1 = raw_data[0]\n",
        "    struc_type1 = raw_data[1].value\n",
        "    chain1 = raw_data[2].value\n",
        "\n",
        "    protein_list1 = [(uniprot_id1, struc_type1, chain1)]\n",
        "    mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs1 = mprs1.run()\n",
        "    sa_seq1 = seqs1[0].split('\\t')[1]\n",
        "\n",
        "    uniprot_id2 = raw_data[3]\n",
        "    struc_type2 = raw_data[4].value\n",
        "    chain2 = raw_data[5].value\n",
        "\n",
        "    protein_list2 = [(uniprot_id2, struc_type2, chain2)]\n",
        "    mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "    seqs2 = mprs2.run()\n",
        "    sa_seq2 = seqs2[0].split('\\t')[1]\n",
        "    return sa_seq1, sa_seq2\n",
        "\n",
        "  # # Pair raw_data = upload_files/xxx.csv\n",
        "  # if data_type in data_type_list[12:16]:\n",
        "  #   uploaded_csv_path = raw_data\n",
        "  #   csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "\n",
        "  # 13. Pair Multiple AA Sequences\n",
        "  if data_type == \"Multiple pairs of AA Sequences\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    for index, value in protein_df['seq_1'].items():\n",
        "      sa_seq1 = ''\n",
        "      for aa in value:\n",
        "        sa_seq1 += aa + '#'\n",
        "      protein_df.at[index, 'seq_1'] = sa_seq1\n",
        "\n",
        "    protein_df['name_1'] = 'name_1'\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "    for index, value in protein_df['seq_2'].items():\n",
        "      sa_seq2 = ''\n",
        "      for aa in value:\n",
        "        sa_seq2 += aa + '#'\n",
        "      protein_df.at[index, 'seq_2'] = sa_seq2\n",
        "\n",
        "    protein_df['name_2'] = 'name_2'\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 14. Pair Multiple SA Sequences\n",
        "  if data_type == \"Multiple pairs of SA Sequences\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "\n",
        "    protein_df['name_1'] = 'name_1'\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "\n",
        "    protein_df['name_2'] = 'name_2'\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "  # 15. Pair Multiple UniProt IDs\n",
        "  if data_type == \"Multiple pairs of UniProt IDs\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    protein_list1 = protein_df.loc[:, \"seq_1\"].tolist()\n",
        "    uniprot2pdb(protein_list1)\n",
        "    protein_df['name_1'] = protein_list1\n",
        "    protein_list1 = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list1]\n",
        "    mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs1 = mprs1.run()\n",
        "\n",
        "    protein_df['seq_1'] = [output.split(\"\\t\")[1] for output in outputs1]\n",
        "    protein_df['chain_1'] = 'A'\n",
        "\n",
        "    protein_list2 = protein_df.loc[:, \"seq_2\"].tolist()\n",
        "    uniprot2pdb(protein_list2)\n",
        "    protein_df['name_2'] = protein_list2\n",
        "    protein_list2 = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list2]\n",
        "    mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "    outputs2 = mprs2.run()\n",
        "\n",
        "    protein_df['seq_2'] = [output.split(\"\\t\")[1] for output in outputs2]\n",
        "    protein_df['chain_2'] = 'A'\n",
        "\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "\n",
        "  # # 13-16. Pair Multiple Sequences\n",
        "  # elif data_type in data_type_list[12:16]:\n",
        "  #   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "  #   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "  #   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "  #   print(\"=\"*100)\n",
        "\n",
        "  elif data_type ==  \"Multiple pairs of PDB/CIF Structures\":\n",
        "    protein_df = pd.read_csv(uploaded_csv_path)\n",
        "    # columns: seq_1, seq_2, type_1, type_2, chain_1, chain_2, label, stage\n",
        "\n",
        "    # protein_list = [(uniprot_id, type, chain), ...]\n",
        "    # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "    # uniprot2pdb(protein_list)\n",
        "\n",
        "    for i in range(1, 3):\n",
        "      protein_list = []\n",
        "      for index, row in protein_df.iterrows():\n",
        "        assert row[f\"type_{i}\"] in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "        row_tuple = (row[f\"seq_{i}\"], row[f\"type_{i}\"], row[f\"chain_{i}\"])\n",
        "        protein_list.append(row_tuple)\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      # add name column, del type column\n",
        "      protein_df[f'name_{i}'] = protein_df[f'seq_{i}'].apply(lambda x: x.split('.')[0])\n",
        "      protein_df.drop(f\"type_{i}\", axis=1, inplace=True)\n",
        "      print(outputs)\n",
        "      protein_df[f'seq_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "\n",
        "    # columns: name_1, name_2, chain_1, chain_2, seq_1, seq_2, label, stage\n",
        "    protein_df.to_csv(csv_dataset_path, index=None)\n",
        "    return csv_dataset_path\n",
        "\n",
        "\n",
        "'''\n",
        "  elif data_type == \"A pair of AA Sequences\",\n",
        "  elif data_type == \"A pair of SA Sequences\",\n",
        "  elif data_type == \"A pair of UniProt IDs\",\n",
        "  elif data_type == \"A pair of PDB/CIF Structures\",\n",
        "  elif data_type == \"Multiple pairs of AA Sequences\",\n",
        "  elif data_type == \"Multiple pairs of SA Sequences\",\n",
        "  elif data_type == \"Multiple pairs of UniProt IDs\",\n",
        "  elif data_type == \"Multiple pairs of PDB/CIF Structures\",\n",
        "'''\n",
        "\n",
        "\n",
        "# # return a SA Sequence or a csv dataset path\n",
        "# def get_raw_dataset(data_type, raw_data):\n",
        "#   if data_type in data_type_list[:3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "#   elif data_type == data_type_list[3]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[4:8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "#   elif data_type in data_type_list[8]:\n",
        "#     raw_dataset = get_SA_sequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "#   return raw_dataset\n",
        "\n",
        "# def upload_dataset(data_type):\n",
        "#   print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "#   uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "#   print(\"=\"*100)\n",
        "\n",
        "#   # selected_csv_dataset = DATASET_HOME / f\"[DATASET]{Path(uploaded_csv_path).stem}.csv\"\n",
        "#   # get_SASequence_by_data_type(data_type, uploaded_csv_path, selected_csv_dataset)\n",
        "#   # get_SA_sequence_by_data_type(data_type, uploaded_csv_path)\n",
        "#   # print()\n",
        "#   # print(\"=\"*100)\n",
        "#   # print(Fore.BLUE+\"Successfully upload your dataset!\"+Style.RESET_ALL)\n",
        "\n",
        "#   return uploaded_csv_path\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "\n",
        "  # print(\"=\"*100)\n",
        "  # print(row_tuple)\n",
        "  # print(\"=\"*100)\n",
        "  uniprot_id = row_tuple[0].split('.')[0]     #\n",
        "  struc_type = row_tuple[1]                   # PDB or AF2\n",
        "  chain = row_tuple[2]\n",
        "\n",
        "  if struc_type==\"AF2\":\n",
        "    plddt_mask= True\n",
        "    chain = 'A'\n",
        "  else:\n",
        "    plddt_mask= False\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "    if Path(pdb_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    elif Path(cif_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    else:\n",
        "      raise BaseException(f\"The {uniprot_id}.pdb/{uniprot_id}.cif file doesn't exists!\")\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "          \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "          \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "          \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "          \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "def convert_outputs_to_pdb(outputs):\n",
        "\tfinal_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
        "\toutputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
        "\tfinal_atom_positions = final_atom_positions.cpu().numpy()\n",
        "\tfinal_atom_mask = outputs[\"atom37_atom_exists\"]\n",
        "\tpdbs = []\n",
        "\toutputs[\"plddt\"] *= 100\n",
        "\n",
        "\tfor i in range(outputs[\"aatype\"].shape[0]):\n",
        "\t\taa = outputs[\"aatype\"][i]\n",
        "\t\tpred_pos = final_atom_positions[i]\n",
        "\t\tmask = final_atom_mask[i]\n",
        "\t\tresid = outputs[\"residue_index\"][i] + 1\n",
        "\t\tpred = OFProtein(\n",
        "\t\t    aatype=aa,\n",
        "\t\t    atom_positions=pred_pos,\n",
        "\t\t    atom_mask=mask,\n",
        "\t\t    residue_index=resid,\n",
        "\t\t    b_factors=outputs[\"plddt\"][i],\n",
        "\t\t    chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
        "\t\t)\n",
        "\t\tpdbs.append(to_pdb(pred))\n",
        "\treturn pdbs\n",
        "\n",
        "\n",
        "# This function is copied from ColabFold!\n",
        "def show_pdb(path, show_sidechains=False, show_mainchains=False, color=\"lddt\"):\n",
        "  file_type = str(path).split(\".\")[-1]\n",
        "  if file_type == \"cif\":\n",
        "    file_type == \"mmcif\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(path,'r').read(),file_type)\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = 1\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###############   Download file to local computer   ##################\n",
        "################################################################################\n",
        "def file_download(path: str):\n",
        "  with open(path, \"rb\") as r:\n",
        "    res = r.read()\n",
        "\n",
        "  #FILE\n",
        "  filename = os.path.basename(path)\n",
        "  b64 = base64.b64encode(res)\n",
        "  payload = b64.decode()\n",
        "\n",
        "  #BUTTONS\n",
        "  html_buttons = '''<html>\n",
        "  <head>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "  </head>\n",
        "  <body>\n",
        "  <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "  <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "  </a>\n",
        "  </body>\n",
        "  </html>\n",
        "  '''\n",
        "\n",
        "  html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "  display(HTML(html_button))\n",
        "\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uxag_RSBI7e"
      },
      "source": [
        "# **2: Train and Share your Protein Model**\n",
        "\n",
        "## Training Dataset\n",
        "\n",
        "For the training dataset, **two additional columns** are required in the CSV file: `label` and `stage`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_training.png\n",
        "?raw=true\" height=\"200\" width=\"400px\" align=\"center\">\n",
        "\n",
        "### Column `label`\n",
        "\n",
        "The content of column `label` depends on your **task type**:\n",
        "\n",
        "| Task Type                         | Content in the Column                          |\n",
        "|-----------------------------------|------------------------------------------------|\n",
        "| Classification tasks              | Category index starting from zero              |\n",
        "| Amino Acid Classification tasks   | A list of category indices for each amino acid |\n",
        "| Regression tasks                  | Numerical values                               |\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/label_format.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "<br>\n",
        "\n",
        "\n",
        "### Column `stage`\n",
        "\n",
        "The column `stage` indicate whether the sample is used for training, validation, or testing. Ensure your dataset includes samples for all three stages. The values are: `train`, `valid`, `test`.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Note:**\n",
        "\n",
        "1. **Examples are available** at /content/SaprotHub/upload_files (if you connect to your local server, then the path is /SaprotHub/upload_files). Download to review their format, and then upload them for a trial.\n",
        "\n",
        "2.  <a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "3. <a href=\"#fa2csv\">Here</a> you can **convert your .fa/.fasta file to a .csv file**, which corresponds to the data format for Multiple AA Sequences.\n",
        "\n",
        "4. <a href=\"#split_dataset\">Here</a> you can **randomly split your .csv dataset**, which means to add a `stage` column, where the ratio of `train`:`valid`:`test` is 8:1:1.\n",
        "\n",
        "<!-- 4. The maximum input length of the model is 1024, and protein sequences exceeding this length will only retain the first 1024 amino acids. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vqdmLslQBI7e"
      },
      "outputs": [],
      "source": [
        "#@title 2.1: Task Config\n",
        "\n",
        "################################################################################\n",
        "################################## TASK CONFIG #################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "task_name = \"demo\" # @param {type:\"string\"}\n",
        "task_objective = \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\", \"Predict protein-protein interaction (pair classification)\", \"Predict protein-protein interaction (pair regression)\"]\n",
        "task_type = task_type_dict[task_objective]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'Enter the number of category in your training dataset here:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#################################### MODEL #####################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "##@markdown We use Parameter-Efficient Fine-Tuning Technique for model training. It enables us to store model weights in a small **adapter** without changing the original model weights during training. After training, you can get an adapter specific to your task.\n",
        "##@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "##@markdown\n",
        "##@markdown 1. Select a **base model** from the dropdown box `model_path` below.\n",
        "##@markdown\n",
        "##@markdown 2. If you want to **train on existing adapters**, check the box `continue_learning` below. By running this cell, you will see an **adapter combobox**. We provide two ways to select your adapter:\n",
        "##@markdown  - Select a **Trained by yourself on ColabSaprot** from the combobox.\n",
        "##@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtAdapters/DeepLoc_cls10_35M\")\n",
        "##@markdown\n",
        "##@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtAdapters)\n",
        "base_model = \"Official pretrained SaProt (35M)\" # @param [\"Official pretrained SaProt (35M)\", \"Official pretrained SaProt (650M)\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\" # @param [\"westlake-repl/SaProt_35M_AF2\", \"westlake-repl/SaProt_650M_AF2\", \"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\"]\n",
        "# print(Fore.BLUE+f\"Model: {base_model}\"+Style.RESET_ALL)\n",
        "\n",
        "# continue_learning = True # @param {type:\"boolean\"}\n",
        "\n",
        "# base_model\n",
        "if base_model == \"Official pretrained SaProt (35M)\":\n",
        "  base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "if base_model == \"Official pretrained SaProt (650M)\":\n",
        "  base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "# continue learning\n",
        "if base_model in [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\"]:\n",
        "  continue_learning = True\n",
        "else:\n",
        "  continue_learning = False\n",
        "\n",
        "def upload_local_adapter(task_type):\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "    adapter_combobox = {\"value\":  f\"Local/{adapter_zip_path.stem}\"}\n",
        "\n",
        "    return adapter_combobox\n",
        "\n",
        "if continue_learning:\n",
        "  if base_model == \"Trained by yourself on ColabSaprot\":\n",
        "    adapter_combobox = select_adapter_from(task_type, use_model_from='Trained by yourself on ColabSaprot')\n",
        "\n",
        "  elif base_model == \"Shared by peers on SaprotHub\":\n",
        "    adapter_combobox = select_adapter_from(task_type, use_model_from='Shared by peers on SaprotHub')\n",
        "\n",
        "  elif base_model == 'Saved in your local computer':\n",
        "    adapter_combobox = select_adapter_from(task_type, use_model_from='Saved in your local computer')\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET ####################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "\n",
        "data_type = \"Multiple AA Sequences\" # @param [\"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"SaprotHub Dataset\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "# lmdb_dataset_path=''\n",
        "\n",
        "# if mode == \"Multiple Sequences\":\n",
        "#   csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "#   from saprot.utils.construct_lmdb import construct_lmdb\n",
        "#   construct_lmdb(csv_dataset_path, LMDB_HOME, task_name, task_type)\n",
        "#   lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "# Hub Dataset\n",
        "if data_type == data_type_list[8]:\n",
        "  def apply(button):\n",
        "    global lmdb_dataset_path\n",
        "    button.disabled = True\n",
        "    button.description = 'Clicked'\n",
        "    button.button_style = ''\n",
        "    lmdb_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "  button_apply = ipywidgets.Button(\n",
        "      description='Click this button to download SaprotHub Dataset',\n",
        "      disabled=False,\n",
        "      button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "      tooltip='Apply',\n",
        "      icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "      )\n",
        "  button_apply.on_click(apply)\n",
        "  button_apply.layout.width = '500px'\n",
        "  display(button_apply)\n",
        "else:\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  from saprot.utils.construct_lmdb import construct_lmdb\n",
        "  construct_lmdb(csv_dataset_path, LMDB_HOME, task_name, task_type)\n",
        "\n",
        "  lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##@markdown Complete some task configs and run this cell to Finetune SaProt on your dataset. <br>\n",
        "\n",
        "# def get_num_of_labels(selected_csv_dataset):\n",
        "#   df = pd.read_csv(selected_csv_dataset)\n",
        "#   num_of_labels = len(df['label'].unique())\n",
        "\n",
        "#   return num_of_labels\n",
        "\n",
        "\n",
        "##@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "############################### custom config ##################################\n",
        "################################################################################\n",
        "\n",
        "##@markdown ---\n",
        "##@markdown # <center>Training Task Config</center>\n",
        "\n",
        "\n",
        "# num_of_categories = 10 # @param {type:\"number\"}\n",
        "# #@markdown <font face=\"Consolas\" size=2 color='gray'>(Ignoring `num_of_categories` if predicting a value)\n",
        "\n",
        "\n",
        "  # print(Fore.BLUE+'It\\'s normal not to receive feedback once inputting is finished. Let\\'s move on to the next step.'+Style.RESET_ALL)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#@title 2.3: Select Model\n",
        "################################################################################\n",
        "\n",
        "# #@markdown We utilize **LoRA** (A Parameter-Efficient Fine-Tuning Technique), which allows us to store model weights into an small adapter without adjusting the original model weights during training.\n",
        "# #@markdown\n",
        "\n",
        "# #@markdown After training, you can obtain an adapter for your task.\n",
        "\n",
        "##@markdown ---\n",
        "##@markdown # <center>Model</center>\n",
        "\n",
        "\n",
        "\n",
        "# if continue_learning:\n",
        "#   print(Fore.BLUE+f\"Loaded Adapter: {adapter_combobox.value}\"+Style.RESET_ALL)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fOcuVy_cBI7f"
      },
      "outputs": [],
      "source": [
        "#@title 2.2: Train your Model\n",
        "\n",
        "batch_size = \"Adaptive\" # @param [\"Adaptive\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]\n",
        "max_epochs = 2 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-3 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "################################################################################\n",
        "############################## advance config ##################################\n",
        "################################################################################\n",
        "\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "val_check_interval=0.5\n",
        "seed = 20000812\n",
        "\n",
        "# use_lora = True\n",
        "num_workers = 2\n",
        "\n",
        "mask_struc_ratio=None\n",
        "# mask_struc_ratio=1.0\n",
        "\n",
        "download_adapter_to_your_computer = True\n",
        "\n",
        "################################################################################\n",
        "################################# MARKDOWM #####################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'> `batch_size` depends on the number of training samples. \"Adaptive\" (default choice) refers to automatic batch size according to your data size. If your training data set is large enough, you can use 32, 64, 128, 256, ..., others can be set to 8, 4, 2 (Note that you can not use a larger batch size if you use the Colab default T4 GPU. Strongly suggest you subscribe to Colab Pro for an A100 GPU.).\n",
        "# #@markdown |  Recommended batch size   | T4  |  A100   |\n",
        "# #@markdown | ---                       | --- |  ---    |\n",
        "# #@markdown | SaProt_35M_AF2            |  4  |    16   |\n",
        "# #@markdown | SaProt_650M_AF2           |  -  |    8    |\n",
        "\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`max_epochs` refers to the maximum number of training iterations. A larger value needs more training time. The best model will be saved after each iteration.\n",
        "#@markdown You can adjust `max_epochs` to control training duration. (Note that the max running time of colab is 12hrs for unsubscribed user or 24hrs for Colab Pro+ user) <br>\n",
        "#@markdown\n",
        "\n",
        "# download_adapter_to_your_computer = True #@param {type:\"boolean\"}\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`learning_rate` affects the convergence speed of the model.\n",
        "#@markdown Through experimentation, we have found that `5.0e-4` is a good default value for base model `Official pretrained SaProt (650M)` and `1.0e-3` for `Official pretrained SaProt (35M)`.\n",
        "\n",
        "################################################################################\n",
        "################################# CONFIG #######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "config.setting.run_mode = \"train\"\n",
        "config.setting.seed = seed\n",
        "\n",
        "################################################################################\n",
        "################################# ADAPTER ######################################\n",
        "################################################################################\n",
        "# config.model.kwargs.use_lora = use_lora\n",
        "\n",
        "# base model and lora path\n",
        "if continue_learning:\n",
        "  adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "  print(f\"Training on an existing model: {adapter_path}\")\n",
        "\n",
        "  if base_model == \"Shared by peers on SaprotHub\":\n",
        "    if not adapter_path.exists():\n",
        "      snapshot_download(repo_id=adapter_combobox.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "  adapter_config_path = Path(adapter_path) / \"adapter_config.json\"\n",
        "  assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "  with open(adapter_config_path, 'r') as f:\n",
        "    adapter_config = json.load(f)\n",
        "    base_model = adapter_config['base_model_name_or_path']\n",
        "\n",
        "  # config.model.kwargs.lora_config_path = adapter_path\n",
        "  config.model.kwargs.lora_kwargs = EasyDict({\n",
        "    \"is_trainable\": True,\n",
        "    \"num_lora\": 1,\n",
        "    \"config_list\": [{\"lora_config_path\": adapter_path}]})\n",
        "\n",
        "else:\n",
        "  # config.model.kwargs.lora_config_path = None\n",
        "  config.model.kwargs.lora_kwargs = EasyDict({\n",
        "    \"num_lora\": 1,\n",
        "    \"config_list\": []})\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# MODEL ########################################\n",
        "################################################################################\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "  # config.model.kwargs.num_labels = get_num_of_labels(selected_csv_dataset)\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "\n",
        "config.model.kwargs.config_path = base_model\n",
        "config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "if base_model == \"westlake-repl/SaProt_650M_AF2\":\n",
        "  model_size = \"650M\"\n",
        "  model_name = f\"Model-{task_name}-{model_size}\"\n",
        "elif base_model == \"westlake-repl/SaProt_35M_AF2\":\n",
        "  model_size = \"35M\"\n",
        "  model_name = f\"Model-{task_name}-{model_size}\"\n",
        "\n",
        "config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / \"Local\" / model_name)\n",
        "\n",
        "if task_type in [\"regression\", \"pair_regression\", \"pair_classification\"]:\n",
        "  config.model.kwargs.extra_config = {}\n",
        "  config.model.kwargs.extra_config.attention_probs_dropout_prob=0\n",
        "  config.model.kwargs.extra_config.hidden_dropout_prob=0\n",
        "\n",
        "# config.model.kwargs.lora_kwargs = {\n",
        "#     \"num_lora\": 6,\n",
        "#     \"config_list\": [\n",
        "#         { \"lora_config_path\": \"/content/subcellular/SaProt_650M_AF2_lora_splitNum5_rank0\",},\n",
        "#         { \"lora_config_path\": \"/content/subcellular/SaProt_650M_AF2_lora_splitNum5_rank1\",},\n",
        "#         { \"lora_config_path\": \"/content/subcellular/SaProt_650M_AF2_lora_splitNum5_rank2\",},\n",
        "#         { \"lora_config_path\": \"/content/subcellular/SaProt_650M_AF2_lora_splitNum5_rank3\",},\n",
        "#         { \"lora_config_path\": \"/content/subcellular/SaProt_650M_AF2_lora_splitNum5_rank4\",},\n",
        "#         { \"lora_config_path\": \"/content/SaprotHub/adapters/classification/SaProtHub/Model-Subcellular_Localization-650M\",},\n",
        "\n",
        "#         ]\n",
        "# }\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "\n",
        "config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "# num_workers\n",
        "config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "# mask_struc\n",
        "# config.dataset.kwargs.mask_struc_ratio= mask_struc_ratio\n",
        "\n",
        "################################################################################\n",
        "######################## batch size ############################################\n",
        "################################################################################\n",
        "def get_accumulate_grad_samples(num_samples):\n",
        "    if num_samples > 3200:\n",
        "        return 64\n",
        "    elif 1600 < num_samples <= 3200:\n",
        "        return 32\n",
        "    elif 800 < num_samples <= 1600:\n",
        "        return 16\n",
        "    elif 400 < num_samples <= 800:\n",
        "        return 8\n",
        "    elif 200 < num_samples <= 400:\n",
        "        return 4\n",
        "    elif 100 < num_samples <= 200:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# batch_size\n",
        "GPU_batch_size_dict = {\n",
        "    \"Tesla T4\": 2,\n",
        "    \"NVIDIA L4\": 2,\n",
        "    \"NVIDIA A100-SXM4-40GB\": 4,\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "  GPU_name = torch.cuda.get_device_name(0)\n",
        "  if base_model == \"westlake-repl/SaProt_650M_AF2\" and root_dir == \"/content\":\n",
        "    assert GPU_name == \"NVIDIA A100-SXM4-40GB\", \"If you want to train on SaProt 650M, please refer to Section 1.1 to switch your Runtime to GPU A100.\"\n",
        "  GPU_batch_size = GPU_batch_size_dict[GPU_name] if GPU_name in GPU_batch_size_dict else 2\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    GPU_batch_size = int(max(GPU_batch_size / 2, 1))\n",
        "else:\n",
        "  raise BaseException(\"Please refer to Section 1.1 to switch your Runtime to a GPU!\")\n",
        "config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "\n",
        "# accumulate_grad_batches\n",
        "if batch_size == \"Adaptive\":\n",
        "\n",
        "  env = lmdb.open(config.dataset.train_lmdb, readonly=True)\n",
        "\n",
        "  with env.begin() as txn:\n",
        "    stat = txn.stat()\n",
        "    num_samples = stat['entries']\n",
        "\n",
        "  accumulate_grad_samples = get_accumulate_grad_samples(num_samples)\n",
        "\n",
        "else:\n",
        "  accumulate_grad_samples = int(batch_size)\n",
        "\n",
        "config.Trainer.accumulate_grad_batches= max(int(accumulate_grad_samples / GPU_batch_size), 1)\n",
        "\n",
        "# config.dataset.dataloader_kwargs.batch_size = 2\n",
        "# config.Trainer.accumulate_grad_batches= 1\n",
        "\n",
        "################################################################################\n",
        "############################## TRAINER #########################################\n",
        "################################################################################\n",
        "\n",
        "config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# epoch\n",
        "config.Trainer.max_epochs = max_epochs\n",
        "# test only: load the existing model\n",
        "if config.Trainer.max_epochs == 0:\n",
        "  config.model.save_path = config.model.kwargs.lora_kwargs.config_list[0].lora_config_path\n",
        "\n",
        "# learning rate\n",
        "config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "# trainer\n",
        "config.Trainer.limit_train_batches=limit_train_batches\n",
        "config.Trainer.limit_val_batches=limit_val_batches\n",
        "config.Trainer.limit_test_batches=limit_test_batches\n",
        "config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "# strategy\n",
        "strategy = {\n",
        "    # - deepspeed\n",
        "    # 'class': 'DeepSpeedStrategy',\n",
        "    # 'stage': 2\n",
        "\n",
        "    # - None\n",
        "    # 'class': None,\n",
        "\n",
        "    # - DP\n",
        "    # 'class': 'DataParallelStrategy',\n",
        "\n",
        "    # - DDP\n",
        "    # 'class': 'DDPStrategy',\n",
        "    # 'find_unused_parameter': True\n",
        "}\n",
        "config.Trainer.strategy = strategy\n",
        "\n",
        "################################################################################\n",
        "############################## CONFIG ##########################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Run the task ####################################\n",
        "################################################################################\n",
        "\n",
        "print('='*100)\n",
        "print(Fore.BLUE+f\"Training task type: {task_type}\"+Style.RESET_ALL)\n",
        "print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "if continue_learning:\n",
        "  print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_kwargs.config_list[0].lora_config_path}\"+Style.RESET_ALL)\n",
        "print('='*100)\n",
        "pprint.pprint(config)\n",
        "print('='*100)\n",
        "\n",
        "from saprot.scripts.training import finetune\n",
        "finetune(config)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Save the adapter ################################\n",
        "################################################################################\n",
        "\n",
        "def add_training_data_type_to_config(metadata_path, training_data_type):\n",
        "  if metadata_path.exists() is False:\n",
        "    config_data = {\n",
        "        'training_data_type': training_data_type\n",
        "        }\n",
        "    with open(metadata_path, 'w') as file:\n",
        "        json.dump(config_data, file, indent=4)\n",
        "\n",
        "  else:\n",
        "    with open(metadata_path, 'r') as file:\n",
        "        config_data = json.load(file)\n",
        "\n",
        "    config_data['training_data_type'] = training_data_type\n",
        "\n",
        "    with open(metadata_path, 'w') as file:\n",
        "        json.dump(config_data, file, indent=4)\n",
        "\n",
        "\n",
        "metadata_path = Path(config.model.save_path) / \"metadata.json\"\n",
        "training_data_type = training_data_type_dict[data_type]\n",
        "\n",
        "add_training_data_type_to_config(metadata_path, training_data_type)\n",
        "\n",
        "print(Fore.BLUE)\n",
        "print(f\"Model is saved to \\\"{config.model.save_path}\\\" on Colab Server\")\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "if download_adapter_to_your_computer:\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{model_name}.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\" \"metadata.json\"\n",
        "  # with zipfile.ZipFile(adapter_zip, 'w') as zipf:\n",
        "  #   zip_files = [str(file_path) for file_path in Path(config.model.save_path).glob(\"*\")]\n",
        "  #   print(zip_files)\n",
        "  #   for file in zip_files:\n",
        "  #     zipf.write(file, Path(file).name)\n",
        "\n",
        "  print(\"Click to download the model to your local computer\")\n",
        "  if adapter_zip.exists():\n",
        "    # files.download(adapter_zip)\n",
        "    file_download(adapter_zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-06T03:20:40.127185Z",
          "start_time": "2024-07-06T03:20:40.084507100Z"
        },
        "cellView": "form",
        "id": "UnKX1BTZBI7f"
      },
      "outputs": [],
      "source": [
        "#@title **2.3: Login HuggingFace to upload your model (Optional)**\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6XlluTsPBI7m"
      },
      "outputs": [],
      "source": [
        "#@title **2.4: Upload Model (Optional)**\n",
        "\n",
        "# #@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "\n",
        "################################################################################\n",
        "########################## Metadata  ###########################################\n",
        "################################################################################\n",
        "#@markdown You can add some description to your model.\n",
        "name = \"demo_cls\" # @param {type:\"string\"}\n",
        "description = \"This model is used for a demo classification task\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown For the classification model, please provide detailed information about the meanings of all labels.\n",
        "\n",
        "#@markdown For example, in a Subcellular Localization Classification Task with 10 categories, label=0 means the protein is located in the Nucleus, label=1 means the protein is located in the Cytoplasm, and so on. The information should be provided as follows:\n",
        "\n",
        "#@markdown `Nucleus, Cytoplasm, Extracellular, Mitochondrion, Cell.membrane, Endoplasmic.reticulum, Plastid, Golgi.apparatus, Lysosome/Vacuole, Peroxisome`\n",
        "\n",
        "\n",
        "# #@markdown > 0: Nucleus <br>\n",
        "# #@markdown > 1: Cytoplasm <br>\n",
        "# #@markdown > 2: Extracellular <br>\n",
        "# #@markdown > ... <br>\n",
        "# #@markdown > 9: Peroxisome <br>\n",
        "\n",
        "label_meanings = \"A, B\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### Move Files  ########################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "if name == \"\":\n",
        "  name = model_name\n",
        "repo_name = user['name'] + '/' + name\n",
        "local_dir = Path(\"/content/SaprotHub/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "################################################################################\n",
        "########################## Modify README  ######################################\n",
        "################################################################################\n",
        "import json\n",
        "\n",
        "md_path = local_dir / \"README.md\"\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "  label_meanings_md = ''\n",
        "  for index, label in enumerate(label_meanings.split(', ')):\n",
        "    label_meanings_md += f\"{index}: {label} <br> \"\n",
        "\n",
        "  # print(label_meanings_md)\n",
        "  description = description + \"<br><br> The digital label means: <br>\" + label_meanings_md\n",
        "\n",
        "replace_data = {\n",
        "    \"<!-- Provide a quick summary of what the model is/does. -->\": description\n",
        "}\n",
        "\n",
        "with open(md_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "for key, value in replace_data.items():\n",
        "    if value != \"\":\n",
        "        content = content.replace(key, value)\n",
        "\n",
        "# new_md_path = \"README.md\"\n",
        "with open(md_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1JgmrsBI7m"
      },
      "source": [
        "# **3: Use SaProt to Predict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04hq3Z6QBI7m"
      },
      "source": [
        "## 3.1: Classification&Regression Prediction <a name=\"classification_regression\"></a>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "For the prediction dataset, **only** `Sequence` column is required in the CSV file.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_Sequences_data_format.png?raw=true\" height=\"200\" width=\"800px\" align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_PDB_CIF_Structures_data_format.png?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "You can refer to the <a href='#data_format'>instruction</a> for detailed data formats.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m9oB-TRJBI7m"
      },
      "outputs": [],
      "source": [
        "#@title 3.1.1: Task Config\n",
        "\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "################################################################################\n",
        "################################# TASK #########################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "\n",
        "task_objective = \"Classify protein sequences (classification)\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (amino acid classification), e.g. Binding site detection\", \"Predict protein-protein interaction (pair classification)\", \"Predict protein-protein interaction (pair regression)\"]\n",
        "task_type = task_type_dict[task_objective]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'The number of categories in your classification task:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################## MODEL #######################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "##@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "##@markdown\n",
        "##@markdown 1. Select a **base model**\n",
        "##@markdown\n",
        "##@markdown 2. By running this cell, you will see an **model combobox**. We provide two ways to select your adapter:\n",
        "##@markdown  - Select a **local model** from the combobox.\n",
        "##@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtHub/DeepLoc_cls10_35M\")\n",
        "##@markdown\n",
        "##@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtHub)\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\" #@param ['westlake-repl/SaProt_35M_AF2', 'westlake-repl/SaProt_650M_AF2'] {allow-input:true}\n",
        "use_model_from = \"Shared by peers on SaprotHub\" # @param [\"Trained by yourself on ColabSaprot\", \"Shared by peers on SaprotHub\", \"Saved in your local computer\", \"Multi-models on SaprotHub\"]\n",
        "if use_model_from == \"Multi-models on SaprotHub\":\n",
        "  multi_lora = True\n",
        "else:\n",
        "  multi_lora = False\n",
        "\n",
        "# use_existing_model = True # @param {type:\"boolean\"}\n",
        "# use_existing_model = True\n",
        "# if use_existing_model:\n",
        "#   adapter_combobox = select_adapter()\n",
        "\n",
        "adapter_input = select_adapter_from(task_type, use_model_from)\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "# # @markdown Please ensure that the selected task type aligns with the training task type of the model you intend to utilize.\n",
        "\n",
        "## @markdown If you are conducting inference on a classification task, please ensure that the `num_of_category` matches the number of categories in the training dataset. Otherwise, you do not need to assign `num_of_category`.\n",
        "\n",
        "\n",
        "##@markdown You have two options to provide your protein sequences:\n",
        "##@markdown - **Single Sequence: Enter a single SA sequence** into the input box, you can get a SA Sequence by clicking <a href=\"#get_SA_seq\">here</a>\n",
        "##@markdown - **Multiple Sequences: Select a dataset**, you can upload a dataset from <a href=\"#upload_dataset\">here</a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(Fore.BLUE+f\"Data type: {data_type}\"+Style.RESET_ALL)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################ DATASET #######################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\", \"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\", \"A pair of PDB/CIF Structures\", \"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\", \"Multiple pairs of UniProt IDs\", \"Multiple pairs of PDB/CIF Structures\"]\n",
        "\n",
        "mode = \"Multiple Sequences\" if (data_type in data_type_list[4:8] or data_type in data_type_list[13:17]) else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "# [\"Single AA Sequence\",\"Single SA Sequence\",\"Single UniProt ID\",\"Single PDB/CIF Structure\",\"Multiple AA Sequences\",\"Multiple SA Sequences\",\"Multiple UniProt IDs\",\"Multiple PDB/CIF Structures\",\"SaprotHub Dataset\",\"A pair of AA Sequences\",\"A pair of SA Sequences\",\"A pair of UniProt IDs\",\"A pair of PDB/CIF Structures\",\"Multiple pairs of AA Sequences\",\"Multiple pairs of SA Sequences\",\"Multiple pairs of UniProt IDs\",\"Multiple pairs of PDB/CIF Structures\"]\n",
        "if mode == \"Multiple Sequences\":\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  def apply(button):\n",
        "    global single_sa_seq\n",
        "    # button.disabled = True\n",
        "    # button.description = 'Clicked'\n",
        "    # button.button_style = ''\n",
        "\n",
        "    # print(Fore.BLUE+'Construct dataset...'+Style.RESET_ALL)\n",
        "    single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    print()\n",
        "    print('='*100)\n",
        "    print(Fore.BLUE+f'Current Model ({use_model_from}): {adapter_input.value}'+Style.RESET_ALL)\n",
        "    if data_type == \"A pair of PDB/CIF Structures\":\n",
        "      print(Fore.BLUE+f'Current Dataset ({data_type}): Sequence 1: {raw_data[0]}, Sequence 2: {raw_data[1]}'+Style.RESET_ALL)\n",
        "    elif data_type in [\"A pair of AA Sequences\",\"A pair of SA Sequences\",\"A pair of UniProt IDs\"]:\n",
        "      print(Fore.BLUE+f'Current Dataset ({data_type}): Sequence 1: {raw_data[0].value}, Sequence 2: {raw_data[1].value}'+Style.RESET_ALL)\n",
        "    elif data_type == \"Single PDB/CIF Structure\":\n",
        "      print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data[0]}' +Style.RESET_ALL)\n",
        "    else:\n",
        "      print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data.value}'+Style.RESET_ALL)\n",
        "\n",
        "  button_apply = ipywidgets.Button(\n",
        "      description='Apply',\n",
        "      disabled=False,\n",
        "      button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "      tooltip='Apply',\n",
        "      icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "      )\n",
        "  button_apply.on_click(apply)\n",
        "  # button_apply.layout.width = '500px'\n",
        "  display(button_apply)\n",
        "\n",
        "\n",
        "# def apply(button):\n",
        "#     global single_sa_seq\n",
        "#     # button.disabled = True\n",
        "#     # button.description = 'Clicked'\n",
        "#     # button.button_style = ''\n",
        "\n",
        "#     # print(Fore.BLUE+'Construct dataset...'+Style.RESET_ALL)\n",
        "#     if mode == \"Multiple Sequences\":\n",
        "#       raw_data = input_raw_data_by_data_type(data_type)\n",
        "#       csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "#     elif mode == \"Single Sequence\":\n",
        "#       single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "\n",
        "#     print()\n",
        "#     print('='*100)\n",
        "#     print(Fore.BLUE+f'Current Model ({use_model_from}): {adapter_input.value}'+Style.RESET_ALL)\n",
        "#     print(Fore.BLUE+f'Current Dataset ({data_type}): {raw_data.value}'+Style.RESET_ALL)\n",
        "\n",
        "# button_apply = ipywidgets.Button(\n",
        "#     description='Apply',\n",
        "#     disabled=False,\n",
        "#     button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "#     tooltip='Apply',\n",
        "#     icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "#     )\n",
        "# button_apply.on_click(apply)\n",
        "# # button_apply.layout.width = '500px'\n",
        "# if\n",
        "# display(button_apply)\n",
        "\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown  <font color=\"red\"> **Note that:** </font> If `use_model_from` is set to `Multi-models on SaprotHub`, each sample will be predicted using multiple models. For classification tasks, voting will be used to determine the final predicted category; for regression tasks, the predicted values from each model will be averaged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "85bPMEjUBI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.1.2: Get your Result\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# 0. MARKDOWN ##################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# @markdown Click the run button to make prediction.\n",
        "\n",
        "# @markdown <font color=\"red\">**Note that:**</font> When predicting a category, the index of categories starts from zero.\n",
        "\n",
        "################################################################################\n",
        "################################# 1. DATASET ##################################\n",
        "################################################################################\n",
        "\n",
        "# if mode == \"Multiple Sequences\":\n",
        "#   csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "# else:\n",
        "#   single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# 2. MODEL ##################################\n",
        "################################################################################\n",
        "def get_base_model(adapter_path):\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    adapter_config_dict = json.load(f)\n",
        "    base_model = adapter_config_dict['base_model_name_or_path']\n",
        "    if 'SaProt_650M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "    elif 'SaProt_35M_AF2' in base_model:\n",
        "      base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "    else:\n",
        "      raise RuntimeError(\"Please ensure the base model is \\\"SaProt_650M_AF2\\\" or \\\"SaProt_35M_AF2\\\"\")\n",
        "  return base_model\n",
        "\n",
        "def check_training_data_type(adapter_path, data_type):\n",
        "  metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "  if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      required_training_data_type = metadata['training_data_type']\n",
        "  else:\n",
        "    required_training_data_type = \"SA\"\n",
        "  assert required_training_data_type == training_data_type_dict[data_type], f\"This model ({base_model}) is trained on {required_training_data_type} sequences. Please ensure your data type is also {required_training_data_type} sequences for accurate predictions.\"\n",
        "\n",
        "\n",
        "# base_model = \"westlake-repl/SaProt_35M_AF2\"\n",
        "if multi_lora:\n",
        "  if use_model_from == \"Multi-models on ColabSaprot\":\n",
        "    config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / lora_config_path}) for lora_config_path in list(adapter_input.value)]\n",
        "  elif use_model_from == \"Multi-models on SaprotHub\":\n",
        "    #1. get adapter_list\n",
        "    repo_id_list = adapter_input.value.replace(\" \", \"\").split(',')\n",
        "    #2. download adapters\n",
        "    for repo_id in repo_id_list:\n",
        "      snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / repo_id)\n",
        "    config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "  assert len(config_list) > 0, \"Please select your models from the dropdown menu on the output of 3.1.1!\"\n",
        "  base_model = get_base_model(ADAPTER_HOME / task_type / config_list[0].lora_config_path)\n",
        "\n",
        "  for lora_config in config_list:\n",
        "    check_training_data_type(lora_config.lora_config_path, data_type)\n",
        "\n",
        "  lora_kwargs = EasyDict({\n",
        "    \"num_lora\": len(config_list),\n",
        "    \"config_list\": config_list\n",
        "  })\n",
        "\n",
        "\n",
        "else:\n",
        "  if use_model_from == \"Shared by peers on SaprotHub\":\n",
        "    snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / adapter_input.value)\n",
        "\n",
        "  adapter_path = ADAPTER_HOME / task_type / adapter_input.value\n",
        "  base_model = get_base_model(adapter_path)\n",
        "  check_training_data_type(adapter_path, data_type)\n",
        "  lora_kwargs = {\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "# if use_existing_model:\n",
        "#   if adapter_combobox.value =='':\n",
        "#     print(\"Please select a model!\")\n",
        "#     sys.exit()\n",
        "\n",
        "#   if \". \" in adapter_combobox.value:\n",
        "#     adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "#   else:\n",
        "#     adapter_path = adapter_combobox.value\n",
        "\n",
        "################################################################################\n",
        "##################################### config ###################################\n",
        "################################################################################\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "# task\n",
        "if task_type in [ \"classification\", \"token_classification\"]:\n",
        "  # config.model.kwargs.num_labels = num_of_categories.value\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "# base model\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "# config.model.save_path = model_save_path\n",
        "config.model.kwargs.config_path = base_model\n",
        "\n",
        "# lora\n",
        "# config.model.kwargs.lora_config_path = adapter_path\n",
        "# config.model.kwargs.use_lora = True\n",
        "# config.model.kwargs.lora_inference = True\n",
        "config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "################################################################################\n",
        "################################### inference ##################################\n",
        "################################################################################\n",
        "from peft import PeftModelForSequenceClassification\n",
        "\n",
        "model = my_load_model(config.model)\n",
        "tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "# print(\"#\"*100)\n",
        "print(Fore.BLUE+f\"Inference task type: {task_type}\"+Style.RESET_ALL)\n",
        "if mode == \"Multiple Sequences\":\n",
        "  print(Fore.BLUE+f\"Dataset: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "else:\n",
        "  if data_type == \"Single PDB/CIF Structure\":\n",
        "    print(Fore.BLUE+f'Dataset ({data_type}): {raw_data[0]}' +Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"Dataset: {raw_data.value}\"+Style.RESET_ALL)\n",
        "\n",
        "if multi_lora:\n",
        "  print(Fore.BLUE+f\"Model: {base_model} - {[str(lora_config.lora_config_path) for lora_config in lora_kwargs.config_list]}\"+Style.RESET_ALL)\n",
        "else:\n",
        "  print(Fore.BLUE+f\"Model: {base_model} - {adapter_path}\"+Style.RESET_ALL)\n",
        "# if use_existing_model:\n",
        "#   print(Fore.BLUE+f\"Adapter: {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "outputs_list=[]\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "  df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    # for index in tqdm(range(len(df))):\n",
        "      # seq = df['Sequence'].iloc[index]\n",
        "\n",
        "      input_1 = tokenizer(row[\"seq_1\"], return_tensors=\"pt\")\n",
        "      input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "\n",
        "      input_2 = tokenizer(row[\"seq_2\"], return_tensors=\"pt\")\n",
        "      input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "      with torch.no_grad(): outputs = model(input_1, input_2)\n",
        "      outputs_list.append(outputs)\n",
        "\n",
        "    df['score'] = [output.cpu().tolist() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "    # files.download(output_file)\n",
        "    file_download(output_file)\n",
        "\n",
        "    print(Fore.BLUE+f\"\\nThe prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "\n",
        "  else:\n",
        "    for index in tqdm(range(len(df))):\n",
        "      seq = df['Sequence'].iloc[index]\n",
        "      inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      with torch.no_grad(): outputs = model(inputs)\n",
        "      outputs_list.append(outputs)\n",
        "\n",
        "    df['score'] = [output.cpu().tolist() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "    # files.download(output_file)\n",
        "    file_download(output_file)\n",
        "\n",
        "    print(Fore.BLUE+f\"\\nThe prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "\n",
        "else:\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "      # print(\"You are making inference based on a sequence that you entered\")\n",
        "    input_1 = tokenizer(single_sa_seq[0], return_tensors=\"pt\")\n",
        "    input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "\n",
        "    input_2 = tokenizer(single_sa_seq[1], return_tensors=\"pt\")\n",
        "    input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "    outputs = model(input_1, input_2)\n",
        "    outputs_list.append(outputs)\n",
        "  else:\n",
        "    # print(\"You are making inference based on a sequence that you entered\")\n",
        "    inputs = tokenizer(single_sa_seq, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad(): outputs = model(inputs)\n",
        "    outputs_list.append(outputs)\n",
        "\n",
        "################################################################################\n",
        "##################################### output ###################################\n",
        "################################################################################\n",
        "\n",
        "print()\n",
        "print('='*100)\n",
        "print(Fore.BLUE+\"outputs:\"+Style.RESET_ALL)\n",
        "\n",
        "if task_type == \"classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(softmax_output_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Category {output.index(max(output))}, Probability: {output}\")\n",
        "elif task_type == \"regression\":\n",
        "  output_list = [output.squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(outputs_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Value {output.item()}\")\n",
        "elif task_type == \"token_classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "  # print(softmax_output_list)\n",
        "  print(\"The probability of each category:\")\n",
        "  for seq_index, seq in enumerate(softmax_output_list):\n",
        "    seq_prob_df = pd.DataFrame(seq)\n",
        "    print('='*100)\n",
        "    print(f'Sequence {seq_index + 1}:')\n",
        "    print(seq_prob_df[1:-1])\n",
        "elif task_type == \"pair_classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "  # print(softmax_output_list)\n",
        "  print(\"The probability of each category:\")\n",
        "  for seq_index, seq in enumerate(softmax_output_list):\n",
        "    seq_prob_df = pd.DataFrame(seq)\n",
        "    print('='*100)\n",
        "    print(f'Sequence {seq_index + 1}:')\n",
        "    print(seq_prob_df[1:-1])\n",
        "elif task_type == \"pair_regression\":\n",
        "  output_list = [output.squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(outputs_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Value {output.item()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdMIr07vBI7n"
      },
      "source": [
        "## 3.2: Mutational Effect Prediction <a name=\"mutational_effect\"></a>\n",
        "\n",
        "\n",
        "<a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Mutation Task\n",
        "- Single-site or Multi-site mutagenesis\n",
        "- Saturation mutagenesis\n",
        "\n",
        "<br>\n",
        "\n",
        "### Mutation Dataset\n",
        "\n",
        "For `Single-site or Multi-site mutagenesis`, **one additional column** are required in the CSV file: `mutation`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_SA_Sequences_data_format_mutation.png\n",
        "?raw=true\" height=\"200\" width=\"500px\" align=\"center\">\n",
        "\n",
        "- `mutation` column contains the **mutation information**.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Mutation Information\n",
        "\n",
        "Here is the detail about the representation of **mutation information**: <a name=\"mutation info\"></a>\n",
        "\n",
        "| mode | mutation information|\n",
        "| --- | --- |\n",
        "| Single-site mutagenesis | H87Y |\n",
        "| Multi-site mutagenesis | H87Y:V162M:P179L:P179R |\n",
        "\n",
        "- For `Single-site mutagenesis`, we use a term like \"H87Y\" to denote the mutation, where the first letter represents the **original amino acid**, the number in the middle represents the **mutation site** (indexed starting from 1), and the last letter represents the **mutated amino acid**,\n",
        "- For `Multi-site mutagenesis`, we use a colon \":\" to connect each single-site mutations, such as \"H87Y:V162M:P179L:P179R\".\n",
        "\n",
        "<!-- ### Prediction Result -->\n",
        "\n",
        "\n",
        "<!-- ### How to use your model for Mutational Effect Prediction -->\n",
        "\n",
        "<!--## 1. Input and Output\n",
        "\n",
        " You have four different combinations of **mutation task** and **mode** to choose from: -->\n",
        "\n",
        "<!--\n",
        " |Combination| Input | Output |\n",
        " | --- | --- | --- |\n",
        " |`Single-site or Multi-site mutagenesis` + `Single Sequence`| Enter **a SA sequence** and **a mutation information**| a score of the mutation |\n",
        " |`Single-site or Multi-site mutagenesis` + `Multiple Sequences`| Select **a dataset** and upload **a .csv file containing mutation information**| a .csv file containing the scores of mutations |\n",
        " |`Saturation mutagenesis` + `Single Sequence`| Enter **a SA sequence**| a .csv file containing the scores of all mutation on every position of the sequence |\n",
        " |`Saturation mutagenesis` + `Multiple Sequences`| Select **a dataset**| a .zip file containing the .csv files of the Saturation mutagenesis on every sequence |\n",
        "  -->\n",
        "\n",
        "\n",
        "<!-- ### 2. Format of the uploaded .csv file containing mutation information -->\n",
        "\n",
        "For Multiple Sequences, you are required to **upload an additional .csv file** as your mutation information.\n",
        "<font color=red>Please ensure that each mutation in the mutation CSV file corresponds to each Sequence in the dataset CSV file.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uxD_KOF1BI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.2.1: Task Config\n",
        "\n",
        "mutation_task = \"Single-site or Multi-site mutagenesis\" #@param [\"Single-site or Multi-site mutagenesis\", \"Saturation mutagenesis\"]\n",
        "\n",
        "# data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\", \"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "data_type = \"Multiple SA Sequences\" # @param [\"Single SA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple SA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if data_type in data_type_list[4:8] else \"Single Sequence\"\n",
        "\n",
        "if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "  if mode == \"Single Sequence\":\n",
        "    input_mut = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder='Enter Single Mutation Information here',\n",
        "      # description='SA Sequence:',\n",
        "      disabled=False)\n",
        "    print(Fore.BLUE+\"Mutation:\"+Style.RESET_ALL)\n",
        "    input_mut.layout.width = '500px'\n",
        "    display(input_mut)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3JSk91jmBI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.2.2: Get your Result\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ###################################\n",
        "################################################################################\n",
        "if mode == \"Single Sequence\":\n",
        "  seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "else:\n",
        "  dataset_csv_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "################################################################################\n",
        "################################# Task Info ####################################\n",
        "################################################################################\n",
        "base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(Fore.BLUE)\n",
        "print(f\"Mutation task: {mutation_task}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(f\"Model: {base_model}\")\n",
        "if mode == \"Multiple Sequences\":\n",
        "  print(Fore.BLUE+f\"Dataset: {dataset_csv_path}\"+Style.RESET_ALL)\n",
        "else:\n",
        "  print(Fore.BLUE+f\"Dataset: {seq}\"+Style.RESET_ALL)\n",
        "\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "print(f\"Predicting...\")\n",
        "timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "\n",
        "################################################################################\n",
        "################################# load model ###################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.model.saprot.saprot_foldseek_mutation_model import SaprotFoldseekMutationModel\n",
        "\n",
        "config = {\n",
        "    \"foldseek_path\": None,\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "\n",
        "try:\n",
        "  zero_shot_model\n",
        "except Exception:\n",
        "  zero_shot_model = SaprotFoldseekMutationModel(**config)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  zero_shot_model.to(device)\n",
        "\n",
        "################################################################################\n",
        "########################### Single Sequence ####################################\n",
        "################################################################################\n",
        "if mode == \"Single Sequence\":\n",
        "\n",
        "  if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "    mut = input_mut.value\n",
        "    # validate mut\n",
        "    aa_seq = seq[0::2]\n",
        "    for m in mut.split(':'):\n",
        "      ori_aa = m[0]\n",
        "      pos = int(m[1:-1])\n",
        "      mut_aa = m[-1]\n",
        "      assert aa_seq[pos-1] == ori_aa, f\"The provided mutation information contains an error ({m}): the original amino acid at position {pos} ({ori_aa}) does not match your sequence ({aa_seq[pos-1]}).\"\n",
        "\n",
        "    score = zero_shot_model.predict_mut(seq, mut)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "    print(f\"The score of mutation {mut} is {Fore.BLUE}{score}{Style.RESET_ALL}\")\n",
        "\n",
        "  if mutation_task==\"Saturation mutagenesis\":\n",
        "    timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "    output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "    mut_dicts = []\n",
        "    for pos in range(1, int(len(seq) / 2)+1):\n",
        "      mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "      mut_dicts.append(mut_dict)\n",
        "\n",
        "    mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "    df = pd.DataFrame(mut_list)\n",
        "    df.to_csv(output_path, index=None)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "    # files.download(output_path)\n",
        "    file_download(output_path)\n",
        "    print(f\"\\n{Fore.BLUE}The result has been saved to {output_path} and your local computer.{Style.RESET_ALL}\")\n",
        "\n",
        "################################################################################\n",
        "########################### Multiple Sequences #################################\n",
        "################################################################################\n",
        "if mode == \"Multiple Sequences\":\n",
        "\n",
        "  dataset_df = pd.read_csv(dataset_csv_path)\n",
        "  results = []\n",
        "\n",
        "  if mutation_task==\"Single-site or Multi-site mutagenesis\":\n",
        "    for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "     seq = row['Sequence']\n",
        "     mut_info = row['mutation']\n",
        "     results.append(zero_shot_model.predict_mut(seq, mut_info).cpu().item())\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "    # result_df = pd.DataFrame()\n",
        "    # result_df['Sequence'] = dataset_df['Sequence']\n",
        "    # result_df['mutation'] = dataset_df['mutation']\n",
        "    dataset_df['score'] = results\n",
        "\n",
        "    output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}.csv\"\n",
        "    dataset_df.to_csv(output_path, index=None)\n",
        "    # files.download(output_path)\n",
        "    file_download(output_path)\n",
        "    print(f\"{Fore.BLUE}The result has been saved to {output_path} and your local computer {Style.RESET_ALL}\")\n",
        "\n",
        "  else:\n",
        "    for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "      seq = row['Sequence']\n",
        "      mut_dicts = []\n",
        "      for pos in range(1, int(len(seq) / 2)+1):\n",
        "        mut_dict = zero_shot_model.predict_pos_mut(seq, pos)\n",
        "        mut_dicts.append(mut_dict)\n",
        "      mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "      result_df = pd.DataFrame(mut_list)\n",
        "      results.append(result_df)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(Fore.BLUE+\"Output:\"+Style.RESET_ALL)\n",
        "\n",
        "    zip_files = []\n",
        "    for i in range(len(results)):\n",
        "      output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_csv_path).stem}_Sequence{i+1}.csv\"\n",
        "      results[i].to_csv(output_path, index=None)\n",
        "      zip_files.append(output_path)\n",
        "\n",
        "    # zip and download zip to local computer\n",
        "    zip_path = OUTPUT_HOME / f\"{timestamp}_{Path(dataset_csv_path).stem}.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for file in zip_files:\n",
        "            zipf.write(file, os.path.basename(file))\n",
        "    # files.download(zip_path)\n",
        "    file_download(zip_path)\n",
        "    print(f\"{Fore.BLUE}The result has been saved to {zip_path} and your local computer{Style.RESET_ALL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlQdqTcBI7n"
      },
      "source": [
        "## 3.3: Inverse Folding Prediction <a name=\"inverse_folding\"></a>\n",
        "\n",
        "Predict the amino acid sequence from protein backbone structure.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The protein backbone structure should be provided in .pdb/.cif file format.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!-- Predict the residue sequence of a structure-aware sequence with masked amino acids (which could be all masked or partially masked).\n",
        "\n",
        "<br>\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Enter a **SA sequence with masked amino acids** into the `sa_seq` input box.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example,\n",
        "**input** is a SA Sequence with masked amino acids:\n",
        "\n",
        "`#d#v#v#v#p#p#p#p#a#p#a#q#k#k#k#k#w`\n",
        "\n",
        "and the **output** predicted by model is an AA Sequence:\n",
        "\n",
        "`MEELGLPDLPPGGVVVV`.\n",
        "\n",
        "<br> -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DT7M_DU2BI7n"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.1: Upload .pdb/.cif structure file\n",
        "\n",
        "#@markdown After clicking the run button, an upload button will appear for you to upload your .pdb/.cif structure file.\n",
        "\n",
        "#@markdown <font face=\"Consolas\" size=2 color='gray'>Note：since you may not know the AA type, you can simply populate your .pdb/.cif file with any random AA. If you want to predit partial positions given some accurate AA information in other positions, just input the accurate AA in these positions and any random AA in unknown positions.</fonte>\n",
        "\n",
        "#@markdown After uploading is finished, the .pdb/.cif structure will be transformed into the corresponding AA Sequence and Structure (3Di) Sequence.\n",
        "\n",
        "#@markdown You can **mask partial or all amino acids** in the AA sequence with '#' at certain positions, allowing the model to make predictions for those masked amino acids.\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"\n",
        "# raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "def get_structure_file():\n",
        "  print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "  dropdown_type = ipywidgets.Dropdown(\n",
        "    value=\"PDB\",\n",
        "    options=[\"PDB\", \"AF2\"],\n",
        "    disabled=False)\n",
        "  dropdown_type.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "  display(dropdown_type)\n",
        "\n",
        "  input_chain = ipywidgets.Text(\n",
        "    value=\"A\",\n",
        "    placeholder=f'Enter the name of chain here',\n",
        "    disabled=False)\n",
        "  input_chain.layout.width = '500px'\n",
        "  print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "  display(input_chain)\n",
        "\n",
        "  print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "  pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "  return pdb_file_path, pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "\n",
        "backbone_path, stem, dropdown_type, input_chain = get_structure_file()\n",
        "raw_data = (stem, dropdown_type, input_chain)\n",
        "\n",
        "sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "aa_seq = sa_seq[0::2]\n",
        "struc_seq = sa_seq[1::2]\n",
        "\n",
        "# masked_sa_seq = ''\n",
        "# for s in sa_seq[1::2]:\n",
        "#   masked_sa_seq += '#' + s\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "input_aa_seq = ipywidgets.Text(\n",
        "      value=aa_seq,\n",
        "      placeholder='Enter Amino Acid Sequence here',\n",
        "      disabled=False)\n",
        "print(Fore.BLUE+\"Amino Acid Sequence:\"+Style.RESET_ALL)\n",
        "input_aa_seq.layout.width = '500px'\n",
        "display(input_aa_seq)\n",
        "\n",
        "input_struc_seq = ipywidgets.Text(\n",
        "  value=struc_seq,\n",
        "  placeholder='Enter Structure Sequence here',\n",
        "  disabled=False)\n",
        "print(Fore.BLUE+\"Structure Sequence:\"+Style.RESET_ALL)\n",
        "input_struc_seq.layout.width = '500px'\n",
        "display(input_struc_seq)\n",
        "\n",
        "print(Fore.RED+\"If you want to mask all amino acids and make prediction, simply clear the 'Amino Acid Sequence' box.\")\n",
        "\n",
        "backbone_name = os.path.basename(backbone_path)\n",
        "show_pdb(backbone_path, color=\"rainbow\").show()\n",
        "print(f\"Backbone visualization of {backbone_name} ({len(struc_seq)} amino acids)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "baiH-BrBl2Ge"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.2: Predict Amino Acid Sequence\n",
        "\n",
        "#@markdown Click the run button to get the predicted Amino Acid Sequence\n",
        "\n",
        "method = \"multinomial\" # @param [\"argmax\", \"multinomial\"]\n",
        "num_samples = 1 # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown - `method` refers to the prediction method. It could be either \"argmax\" or \"multinomial\".\n",
        "#@markdown   - `argmax` selects the amino acid with the highest probability.\n",
        "#@markdown   - `multinomial` samples an amino acid from the multinomial distribution.\n",
        "\n",
        "\n",
        "#@markdown - `num_samples` refers to the number of output amino acid sequences.\n",
        "\n",
        "save_name = \"predicted_seq\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### Dataset ########################################\n",
        "################################################################################\n",
        "\n",
        "masked_aa_seq = input_aa_seq.value\n",
        "if masked_aa_seq.strip() == \"\":\n",
        "  masked_aa_seq = \"#\" * len(input_struc_seq.value)\n",
        "\n",
        "masked_struc_seq = input_struc_seq.value\n",
        "\n",
        "# assert len(masked_aa_seq) == len(masked_struc_seq), f\"Please make sure that the amino acid sequence ({len(masked_aa_seq)}) and the structure sequence ({len(masked_struc_seq)}) have the same length.\"\n",
        "# masked_sa_seq = ''.join(a + b for a, b in zip(masked_aa_seq, masked_struc_seq))\n",
        "\n",
        "\n",
        "# if num_samples == 1:\n",
        "#   method = \"argmax\"\n",
        "# elif num_samples > 1:\n",
        "#   method = \"multinomial\"\n",
        "# else:\n",
        "#   raise BaseException(\"\\\"num_samples\\\" should be an integer greater than or equal to 1.\")\n",
        "\n",
        "################################################################################\n",
        "############################### Model ##########################################\n",
        "################################################################################\n",
        "# base_model = \"westlake-repl/SaProt_650M_AF2\"\n",
        "base_model = \"westlake-repl/SaProt_650M_AF2_inverse_folding\"\n",
        "\n",
        "config = {\n",
        "    \"config_path\": base_model,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "from saprot.model.saprot.saprot_if_model import SaProtIFModel\n",
        "try:\n",
        "  saprot_if_model\n",
        "except Exception:\n",
        "  saprot_if_model = SaProtIFModel(**config)\n",
        "  tokenizer = saprot_if_model.tokenizer\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  saprot_if_model.to(device)\n",
        "\n",
        "################################################################################\n",
        "############################### Predict ########################################\n",
        "################################################################################\n",
        "\n",
        "pred_aa_seqs = saprot_if_model.predict(masked_aa_seq, masked_struc_seq, method=method, num_samples=num_samples)\n",
        "\n",
        "print(\"#\"*100)\n",
        "print(Fore.BLUE+\"outputs:\"+Style.RESET_ALL)\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.fasta\"\n",
        "with open(save_path, \"w\") as w:\n",
        "  for i, aa_seq in enumerate(pred_aa_seqs):\n",
        "    print(aa_seq)\n",
        "    w.write(f\">predicted_seq_{i}\\n{aa_seq}\")\n",
        "\n",
        "file_download(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idDDKW2pl2Gf"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.3: Predict the structure of generated sequence\n",
        "\n",
        "#@markdown  <font color=\"red\"> **Warning: Please do not run this cell if you only have 12GB RAM!!! This will cause\n",
        "#@markdown the out of memory error and you will have to restart the notebook. We recommend you connect to a runtime\n",
        "#@markdown with more RAM to run the cell properly.** </font>\n",
        "\n",
        "#@markdown Click the run button to predict the structure of generated sequence using ESMFold\n",
        "\n",
        "protein_sequence = \"Input yout sequence\" # @param {type:\"string\"}\n",
        "save_name = \"predicted_structure\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Visualization settings\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### LOAD ESMFOLD ################################\n",
        "################################################################################\n",
        "try:\n",
        "  esmfold\n",
        "except Exception:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
        "  esmfold.esm = esmfold.esm.half()\n",
        "  esmfold.trunk.set_chunk_size(64)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  esmfold.to(device)\n",
        "\n",
        "################################################################################\n",
        "################################## PREDICT ###################################\n",
        "################################################################################\n",
        "tokenized_input = tokenizer(\n",
        "    [protein_sequence],\n",
        "    return_tensors=\"pt\",\n",
        "    add_special_tokens=False,\n",
        "    max_length=1024,\n",
        "    truncation=True,\n",
        "    )['input_ids']\n",
        "\n",
        "tokenized_input = tokenized_input.to(esmfold.device)\n",
        "with torch.no_grad():\n",
        "  output = esmfold(tokenized_input)\n",
        "\n",
        "################################################################################\n",
        "#################################### SAVE ####################################\n",
        "################################################################################\n",
        "save_path = f\"{root_dir}/SaprotHub/output/{save_name}.pdb\"\n",
        "pdb = convert_outputs_to_pdb(output)\n",
        "with open(save_path, \"w\") as f:\n",
        "  f.write(\"\".join(pdb))\n",
        "\n",
        "################################################################################\n",
        "################################# VISUALIZE ##################################\n",
        "################################################################################\n",
        "show_pdb(save_path, show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\":\n",
        "  plot_plddt_legend().show()\n",
        "\n",
        "print(\"Predicted structure\")\n",
        "file_download(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PecOgaEEZw4C"
      },
      "outputs": [],
      "source": [
        "#@title 3.3.4: Align proteins using TMalign\n",
        "\n",
        "#@markdown You can find the **uploaded proteins** from /content/SaprotHub/structures (if you connect to your local server, then the path is /SaprotHub/structures).\n",
        "\n",
        "#@markdown You can find the **predicted proteins** from /content/SaprotHub/output (if you connect to your local server, then the path is /SaprotHub/output).\n",
        "\n",
        "#@markdown Right click the pdb file to copy the path and then paste it into the box:\n",
        "pdb_path_1 = \"\" # @param {type:\"string\"}\n",
        "pdb_path_2 = \"\" # @param {type:\"string\"}\n",
        "\n",
        "pdb_path_1 = f\"{root_dir}{pdb_path_1}\"\n",
        "pdb_path_2 = f\"{root_dir}{pdb_path_2}\"\n",
        "\n",
        "assert os.path.exists(pdb_path_1) and os.path.exists(pdb_path_2), \"Input proteins do not exist!\"\n",
        "\n",
        "cmd = f\"{root_dir}/SaprotHub/bin/TMalign {pdb_path_1} {pdb_path_2}\"\n",
        "print(os.popen(cmd).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIIoTQgJBI7o"
      },
      "source": [
        "# **4: (Optional) Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjzMSqaY6SD"
      },
      "source": [
        "## 4.1: Get Structure-Aware Sequence <a name=\"get_sa\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BgsBSLcmBI7o"
      },
      "outputs": [],
      "source": [
        "# @title 4.1.1: AA Sequence, UniProt ID, PDB/CIF file -> SA Sequence\n",
        "\n",
        "################################################################################\n",
        "################################ input #########################################\n",
        "################################################################################\n",
        "\n",
        "data_type = \"Single PDB/CIF Structure\"  # @param [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\", \"Multiple AA Sequences\", \"Multiple UniProt IDs\", \"Multiple PDB/CIF Structures\"]\n",
        "\n",
        "if data_type == data_type_list[7]:\n",
        "    # upload and unzip PDB files\n",
        "    print(Fore.BLUE + f\"Please upload your .zip file which contains {data_type} files\" + Style.RESET_ALL)\n",
        "    pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "    print(Fore.BLUE + \"Successfully upload your .zip file!\" + Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        file_names = zip_ref.namelist()\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    uploaded_csv_path = UPLOAD_FILE_HOME / f\"{pdb_zip_path.stem}.csv\"\n",
        "    df = pd.DataFrame(file_names, columns=['Sequence'])\n",
        "    df.to_csv(uploaded_csv_path, index=False)\n",
        "    raw_data = uploaded_csv_path\n",
        "\n",
        "else:\n",
        "    raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "################################################################################\n",
        "############################### output #########################################\n",
        "################################################################################\n",
        "\n",
        "if data_type in [\"Single AA Sequence\", \"Single UniProt ID\", \"Single PDB/CIF Structure\"]:\n",
        "    def apply(button):\n",
        "        button.disabled = True\n",
        "        button.description = 'Clicked'\n",
        "        button.button_style = ''\n",
        "        sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "\n",
        "        print(\"=\"*100)\n",
        "        print(f\"Amino Acid Sequence: {sa_seq[0::2]}\")\n",
        "        print(f\"Structure Sequence: {sa_seq[1::2]}\")\n",
        "        print(\"=\"*100)\n",
        "        print(Fore.BLUE + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + Style.RESET_ALL)\n",
        "        print(sa_seq)\n",
        "\n",
        "    button_apply = ipywidgets.Button(\n",
        "        description='Apply',\n",
        "        disabled=False,\n",
        "        button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Apply',\n",
        "        icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "    button_apply.on_click(apply)\n",
        "    button_apply.layout.width = '500px'\n",
        "    display(button_apply)\n",
        "else:\n",
        "    csv_dataset = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    print(Fore.BLUE + \"The Structure-Aware Sequences are saved in a .csv file here:\" + Style.RESET_ALL)\n",
        "    print(csv_dataset)\n",
        "    file_download(csv_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAE5-b_iZEyq"
      },
      "source": [
        "## 4.2: Convert `.fa/.fasta` file to `.csv` file in the data format of \"Multiple AA Sequences\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDkm_OeABI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.2.1: `.fa/.fasta` -> Multiple AA Sequences `.csv` <a name=\"fa2csv\"></a>\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "aa_seq_dict = { \"Sequence\": [],\n",
        "                # \"label\": [],\n",
        "                # \"stage\":[]\n",
        "                }\n",
        "\n",
        "fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "assert Path(fa_file_path).name.split('.')[1] in ['fa', 'fasta'], \"Please upload a .fa or .fasta file.\"\n",
        "with fa_file_path.open(\"r\") as fa:\n",
        "  for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "      aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "\n",
        "fa_df = pd.DataFrame(aa_seq_dict)\n",
        "print(fa_df[5:])\n",
        "\n",
        "csv_file_path = UPLOAD_FILE_HOME / f'{fa_file_path.stem}.csv'\n",
        "fa_df.to_csv(csv_file_path, index=None)\n",
        "# files.download(csv_file_path)\n",
        "file_download(csv_file_path)\n",
        "\n",
        "################################################################################\n",
        "############################ .fa 2 .csv and split ##############################\n",
        "################################################################################\n",
        "\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# split = ['train', 'valid', 'test']\n",
        "\n",
        "# aa_seq_dict = { \"Sequence\": [],\n",
        "#                 \"label\": [],\n",
        "#                 \"stage\":[]}\n",
        "\n",
        "\n",
        "\n",
        "# if automatically_split_dataset:\n",
        "\n",
        "#   fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#   label = fa_file_path.stem\n",
        "\n",
        "#   with fa_file_path.open(\"r\") as fa:\n",
        "#       for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "#           aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#           aa_seq_dict[\"label\"].append(label)\n",
        "#   weights = [0.8, 0.1, 0.1]\n",
        "#   aa_seq_dict[\"stage\"] = np.random.choice(split, size=len(aa_seq_dict[\"Sequence\"]), p=weights).tolist()\n",
        "\n",
        "# else:\n",
        "#   for i in range(3):\n",
        "#     print(Fore.BLUE+f\"Please upload a .fa file as your {split[i]} dataset\")\n",
        "#     fa_file_path = upload_file(UPLOAD_FILE_HOME)\n",
        "#     label = fa_file_path.stem\n",
        "\n",
        "#     with fa_file_path.open(\"r\") as fa:\n",
        "#         for record in tqdm(SeqIO.parse(fa, 'fasta')):\n",
        "#             aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#             aa_seq_dict[\"label\"].append(label)\n",
        "#             aa_seq_dict[\"stage\"].append(split[i])\n",
        "\n",
        "#     print()\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "# fa_df = pd.DataFrame(aa_seq_dict)\n",
        "# timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "# fa_df.to_csv(f'/content/SaprotHub/upload_files/{timestamp}.csv', index=None)\n",
        "# files.download(f'/content/SaprotHub/upload_files/{timestamp}.csv')\n",
        "# print(fa_df[5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5fOAAfpZGkm"
      },
      "source": [
        "## 4.3: Dataset Split\n",
        "\n",
        "Please click the run button to upload your .csv dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xtadHW9vBI7o"
      },
      "outputs": [],
      "source": [
        "#@title 4.3.1: Randomly split your .csv dataset <a name=\"split_dataset\"></a>\n",
        "\n",
        "csv_dataset_path = upload_file(UPLOAD_FILE_HOME)\n",
        "dataset_df = pd.read_csv(csv_dataset_path)\n",
        "\n",
        "split = ['train', 'valid', 'test']\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "\n",
        "if ('stage' not in dataset_df.columns) or (dataset_df[\"stage\"].nunique()<3):\n",
        "  dataset_df[\"stage\"] = np.random.choice(split, size=len(dataset_df), p=split_ratio).tolist()\n",
        "\n",
        "dataset_df.to_csv(csv_dataset_path, index=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFuAgRlv9EiZ"
      },
      "source": [
        "# Manual <a name=\"manual\"></a>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQCdY6Pm9_FK"
      },
      "source": [
        "## How to train and share your model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbQQjnvW-WVZ"
      },
      "source": [
        "### Train your Model\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the input and selection of Task Configs\n",
        "\n",
        "- `task_name` is the name of the training task you're working on.\n",
        "- `task_objective` describes the goal of your task, like sorting protein sequences into categories or predicting the values of some protein properties.\n",
        "- `base_model` is the base model you use for training. By default, it's set to the officially pretrained SaProt, but you can use models either retrained (by yourself) by ColabSaprot or shared on [SaprotHub](https://huggingface.co/SaProtHub). For example, you can choose `Trained-by-peers` with your own data if you want to retrain on SaProt models shared by others.  There are a wide range of retrained models available on [SaprotHub](https://huggingface.co/SaProtHub).\n",
        "- `data_type` indicates the kind of data you're using, which is determined by the dataset file you upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "Click the run button to apply the configs.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-1.png?raw=true\" height=\"300\" width=\"600px\" align=\"center\">\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes will appear.\n",
        "\n",
        "Complete the input of additional information and upload files.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-2.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "\n",
        "<!-- If you want to train on an existing model, choose \"Existing Models with your data\" as `base_model` at step 1, and then \"Existing model\" input box will appear. Enter a huggingface model id or select a local model. -->\n",
        "\n",
        "(Note: Do not click the \"Run\" button of the next cell before completing the input and upload.)\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-3.png?raw=true\" height=\"300\" width=\"300px\" align=\"center\">\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "Complete the input of training configs\n",
        "\n",
        "- `batch_size` depends on the number of training samples. If your training data set is large enough, we recommend using 32, 64,128,256, ..., others can be set to 8, 4, 2. (Note that you can not use a larger batch size if you the Colab default T4 GPU. Strongly suggest you subscribe to Colab Pro for an A100 GPU.)\n",
        "- `max_epochs` refers to the maximum number of training iterations. A larger value needs more training time. The best model will be saved after each iteration. You can adjust `max_epochs` to control training duration. (Note that the max running time of Colab is 12hrs for unsubscribed user or 24hrs for Colab Pro+ user)\n",
        "- `learning_rate` affects the convergence speed of the model. Through experimentation, we have found that `5.0e-4` is a good default value for base model `Official pretrained SaProt (650M)` and `1.0e-3` for `Official pretrained SaProt (35M)`.\n",
        "\n",
        "### Step 5\n",
        "\n",
        "Click the \"Run\" button to start training.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-4.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "You can monitor the training process by these plots. After training, check the training results and the saved model.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-5.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "\n",
        "### Advanced usage: Continual learning\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/train-6.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "939lKY7e-iQi"
      },
      "source": [
        "### (Optional) Upload your Model to Huggingface:\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Click the \"Run\" button and the Hugging Face login interface will appear.\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "Find your token by clicking the link.\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "Enter the token and click the \"Login\" button.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/upload-1.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">\n",
        "\n",
        "<!-- ![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/upload-1-2.png?raw=true) -->\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "Enter the model name, model description and other information.\n",
        "\n",
        "#### Step 5\n",
        "\n",
        "Click the button to upload the model. You can check your model by the link.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/upload-2.png?raw=true\" height=\"300\" width=\"700px\" align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/upload-3.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gjYyyB-O_s"
      },
      "source": [
        "## How to use your model for prediction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUbfz_i9-pkk"
      },
      "source": [
        "### Classification&Regression prediction task\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the input and selection of Task Configs, and then\n",
        "\n",
        "- `task_objective` describes the goal of your task, like sorting protein sequences into categories or predicting the values of some protein properties.\n",
        "- `use_model_from` depends on whether you want to use a local model or a Huggingface model. If you choose `Shared by peers on SaprotHub`, please enter the Hugging Face model ID in the input box. If you choose `Local Model`, simply select your local model from the options. Additionally, there's a wide range of models available on SaprotHub.\n",
        "- `data_type` indicates the kind of data you're using, which determines the dataset file you should upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "<!-- ![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/cls_regr-1-1.png?raw=true) -->\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "Click the run button to apply the configs.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/cls_regr-1.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes and upload button will appear.\n",
        "\n",
        "Complete the input of additional information and upload files.\n",
        "\n",
        "(Note: Do not click the \"Run\" button of the next cell before completing the input and upload.)\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/cls_regr-2.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/cls_regr-3.png?raw=true\" height=\"300\" width=\"400px\" align=\"center\">\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "Click the run button to start predicting. Check your results after finishing prediction.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/cls_regr-4.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwc_ZYR0-s3M"
      },
      "source": [
        "### Mutational effect prediction task\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Complete the selection of Task Configs.\n",
        "\n",
        "- `mutation_task` indicates the type of mutation task. You can choose from `Single-site or Multi-site mutagenesis` and `Saturation mutagenesis`.\n",
        "- `data_type` indicates the kind of data you're using, which determines the dataset file you should upload. You can find more details about the formats for different types of data in the provided <a href=\"#data_format\">instruction</a>.\n",
        "\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "Click the run button to apply the configs.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/mep-1.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "\n",
        "\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes and upload button will appear.\n",
        "\n",
        "For a single sequence, enter the sequence and the mutation information into the corresponding input fields. (Note that for Saturation mutagenesis, you won't see the Mutation input box.)\n",
        "\n",
        "For multiple sequences, click the upload button to upload your dataset. (Note that for Saturation mutagenesis, you don’t need to provide mutation information in your dataset, which means only `sequence` column is required in the .csv dataset.)\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/mep-2.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "\n",
        "\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "Click the run button to start predicting. Check your results after finishing prediction.\n",
        "\n",
        "- For a single sequence, the predicted score will be show in the output.\n",
        "\n",
        "- For multiple sequences, the predicted score will be saved in a .csv file.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/mep-3.png?raw=true\" height=\"300\" width=\"600px\" align=\"center\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- ![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-2.png?raw=true) -->\n",
        "\n",
        "\n",
        "<!-- ![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-3.png?raw=true)\n",
        "\n",
        "![Untitled](https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/mutation-3-4.png?raw=true) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfwHGTa6-HUP"
      },
      "source": [
        "### Inverse folding task\n",
        "\n",
        "#### Step 1\n",
        "\n",
        "Click the run button to upload the structure file, which could be in the format of .pdb or .cif file.\n",
        "\n",
        "\n",
        "\n",
        "#### Step 2\n",
        "\n",
        "After clicking the \"Run\" button, additional input boxes and upload button will appear.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/if-1.png?raw=true\" height=\"300\" width=\"500px\" align=\"center\">\n",
        "\n",
        "\n",
        "\n",
        "#### Step 3\n",
        "\n",
        "After uploading the structure file, it will be transformed into AA sequence and structure sequence.\n",
        "\n",
        "Use '#' to mask some amino acids for prediction.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/if-2.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "\n",
        "#### Step 4\n",
        "\n",
        "Choose the prediction method.\n",
        "\n",
        "#### Step 5\n",
        "\n",
        "Click the run button to make prediction.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/if-3.png?raw=true\" height=\"300\" width=\"1000px\" align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Instruction/v1/if-4.png?raw=true\" height=\"300\" width=\"600px\" align=\"center\">\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}